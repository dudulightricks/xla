


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>PyTorch on XLA Devices &mdash; PyTorch/XLA master documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','');</script>
    <!-- End Google Tag Manager -->
  

  
  <script src="_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2023">
                  <span class="dropdown-title">Contributor Awards - 2023</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News 
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                  <p></p>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="https://pytorch.org/join" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  master (2.5.0+git0df5c29 )
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <!-- Local TOC -->
              <div class="local-toc"><ul>
<li><a class="reference internal" href="#">PyTorch on XLA Devices</a><ul>
<li><a class="reference internal" href="#creating-an-xla-tensor">Creating an XLA Tensor</a></li>
<li><a class="reference internal" href="#xla-tensors-are-pytorch-tensors">XLA Tensors are PyTorch Tensors</a></li>
<li><a class="reference internal" href="#running-models-on-xla-devices">Running Models on XLA Devices</a><ul>
<li><a class="reference internal" href="#running-on-a-single-xla-device">Running on a Single XLA Device</a></li>
<li><a class="reference internal" href="#running-on-multiple-xla-devices-with-multi-processing">Running on Multiple XLA Devices with Multi-processing</a></li>
<li><a class="reference internal" href="#running-on-tpu-pods">Running on TPU Pods</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id3">XLA Tensor Deep Dive</a><ul>
<li><a class="reference internal" href="#xla-tensors-are-lazy">XLA Tensors are Lazy</a></li>
<li><a class="reference internal" href="#xla-tensors-and-bfloat16">XLA Tensors and bFloat16</a></li>
<li><a class="reference internal" href="#memory-layout">Memory Layout</a></li>
<li><a class="reference internal" href="#moving-xla-tensors-to-and-from-the-cpu">Moving XLA Tensors to and from the CPU</a></li>
<li><a class="reference internal" href="#saving-and-loading-xla-tensors">Saving and Loading XLA Tensors</a></li>
</ul>
</li>
<li><a class="reference internal" href="#compilation-caching">Compilation Caching</a></li>
<li><a class="reference internal" href="#further-reading">Further Reading</a></li>
</ul>
</li>
<li><a class="reference internal" href="#pytorch-xla-api">PyTorch/XLA API</a><ul>
<li><a class="reference internal" href="#module-torch_xla">torch_xla</a><ul>
<li><a class="reference internal" href="#torch_xla.device"><code class="docutils literal notranslate"><span class="pre">device()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.devices"><code class="docutils literal notranslate"><span class="pre">devices()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.device_count"><code class="docutils literal notranslate"><span class="pre">device_count()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.sync"><code class="docutils literal notranslate"><span class="pre">sync()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.step"><code class="docutils literal notranslate"><span class="pre">step()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.manual_seed"><code class="docutils literal notranslate"><span class="pre">manual_seed()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#module-torch_xla.runtime">runtime</a><ul>
<li><a class="reference internal" href="#torch_xla.runtime.device_type"><code class="docutils literal notranslate"><span class="pre">device_type()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.runtime.local_process_count"><code class="docutils literal notranslate"><span class="pre">local_process_count()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.runtime.local_device_count"><code class="docutils literal notranslate"><span class="pre">local_device_count()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.runtime.addressable_device_count"><code class="docutils literal notranslate"><span class="pre">addressable_device_count()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.runtime.global_device_count"><code class="docutils literal notranslate"><span class="pre">global_device_count()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.runtime.global_runtime_device_count"><code class="docutils literal notranslate"><span class="pre">global_runtime_device_count()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.runtime.world_size"><code class="docutils literal notranslate"><span class="pre">world_size()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.runtime.global_ordinal"><code class="docutils literal notranslate"><span class="pre">global_ordinal()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.runtime.local_ordinal"><code class="docutils literal notranslate"><span class="pre">local_ordinal()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.runtime.get_master_ip"><code class="docutils literal notranslate"><span class="pre">get_master_ip()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.runtime.use_spmd"><code class="docutils literal notranslate"><span class="pre">use_spmd()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.runtime.is_spmd"><code class="docutils literal notranslate"><span class="pre">is_spmd()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.runtime.initialize_cache"><code class="docutils literal notranslate"><span class="pre">initialize_cache()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#module-torch_xla.core.xla_model">xla_model</a><ul>
<li><a class="reference internal" href="#torch_xla.core.xla_model.xla_device"><code class="docutils literal notranslate"><span class="pre">xla_device()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.core.xla_model.xla_device_hw"><code class="docutils literal notranslate"><span class="pre">xla_device_hw()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.core.xla_model.is_master_ordinal"><code class="docutils literal notranslate"><span class="pre">is_master_ordinal()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.core.xla_model.all_reduce"><code class="docutils literal notranslate"><span class="pre">all_reduce()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.core.xla_model.all_gather"><code class="docutils literal notranslate"><span class="pre">all_gather()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.core.xla_model.all_to_all"><code class="docutils literal notranslate"><span class="pre">all_to_all()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.core.xla_model.add_step_closure"><code class="docutils literal notranslate"><span class="pre">add_step_closure()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.core.xla_model.wait_device_ops"><code class="docutils literal notranslate"><span class="pre">wait_device_ops()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.core.xla_model.optimizer_step"><code class="docutils literal notranslate"><span class="pre">optimizer_step()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.core.xla_model.save"><code class="docutils literal notranslate"><span class="pre">save()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.core.xla_model.rendezvous"><code class="docutils literal notranslate"><span class="pre">rendezvous()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.core.xla_model.mesh_reduce"><code class="docutils literal notranslate"><span class="pre">mesh_reduce()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.core.xla_model.set_rng_state"><code class="docutils literal notranslate"><span class="pre">set_rng_state()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.core.xla_model.get_rng_state"><code class="docutils literal notranslate"><span class="pre">get_rng_state()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.core.xla_model.get_memory_info"><code class="docutils literal notranslate"><span class="pre">get_memory_info()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.core.xla_model.get_stablehlo"><code class="docutils literal notranslate"><span class="pre">get_stablehlo()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.core.xla_model.get_stablehlo_bytecode"><code class="docutils literal notranslate"><span class="pre">get_stablehlo_bytecode()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#module-torch_xla.distributed.parallel_loader">distributed</a><ul>
<li><a class="reference internal" href="#torch_xla.distributed.parallel_loader.ParallelLoader"><code class="docutils literal notranslate"><span class="pre">ParallelLoader</span></code></a><ul>
<li><a class="reference internal" href="#torch_xla.distributed.parallel_loader.ParallelLoader.per_device_loader"><code class="docutils literal notranslate"><span class="pre">ParallelLoader.per_device_loader()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torch_xla.distributed.xla_multiprocessing.spawn"><code class="docutils literal notranslate"><span class="pre">spawn()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#module-torch_xla.distributed.spmd">spmd</a><ul>
<li><a class="reference internal" href="#torch_xla.distributed.spmd.mark_sharding"><code class="docutils literal notranslate"><span class="pre">mark_sharding()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.distributed.spmd.clear_sharding"><code class="docutils literal notranslate"><span class="pre">clear_sharding()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.distributed.spmd.set_global_mesh"><code class="docutils literal notranslate"><span class="pre">set_global_mesh()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.distributed.spmd.get_global_mesh"><code class="docutils literal notranslate"><span class="pre">get_global_mesh()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.distributed.spmd.Mesh"><code class="docutils literal notranslate"><span class="pre">Mesh</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.distributed.spmd.HybridMesh"><code class="docutils literal notranslate"><span class="pre">HybridMesh</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.distributed.spmd.ShardingSpec"><code class="docutils literal notranslate"><span class="pre">ShardingSpec</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#module-torch_xla.experimental">experimental</a><ul>
<li><a class="reference internal" href="#torch_xla.experimental.eager_mode"><code class="docutils literal notranslate"><span class="pre">eager_mode()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.experimental.compile"><code class="docutils literal notranslate"><span class="pre">compile()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#module-torch_xla.debug.metrics">debug</a><ul>
<li><a class="reference internal" href="#torch_xla.debug.metrics.metrics_report"><code class="docutils literal notranslate"><span class="pre">metrics_report()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.debug.metrics.short_metrics_report"><code class="docutils literal notranslate"><span class="pre">short_metrics_report()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.debug.metrics.counter_names"><code class="docutils literal notranslate"><span class="pre">counter_names()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.debug.metrics.counter_value"><code class="docutils literal notranslate"><span class="pre">counter_value()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.debug.metrics.metric_names"><code class="docutils literal notranslate"><span class="pre">metric_names()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.debug.metrics.metric_data"><code class="docutils literal notranslate"><span class="pre">metric_data()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#beginner-s-guide-to-pytorch-xla">Beginner’s Guide to PyTorch/XLA</a><ul>
<li><a class="reference internal" href="#basic-high-level-understanding-of-some-xla-details">Basic high-level understanding of some XLA details</a></li>
<li><a class="reference internal" href="#tpu-setup">TPU Setup</a></li>
<li><a class="reference internal" href="#converting-code-to-pytorch-xla">Converting code to PyTorch XLA</a></li>
<li><a class="reference internal" href="#example-1-stable-diffusion-inference-in-pytorch-lightning-on-a-single-tpu-device">Example 1. Stable Diffusion inference in PyTorch Lightning on a Single TPU Device</a></li>
<li><a class="reference internal" href="#example-2-hf-stable-diffusion-inference">Example 2. HF Stable Diffusion Inference</a></li>
<li><a class="reference internal" href="#running-on-a-single-tpu-device">Running on a Single TPU device</a></li>
<li><a class="reference internal" href="#profiling-and-performance-analysis">Profiling and performance analysis</a></li>
<li><a class="reference internal" href="#running-on-multiple-tpu-devices">Running on Multiple TPU Devices</a></li>
<li><a class="reference internal" href="#running-on-pods">Running on Pods</a></li>
</ul>
</li>
<li><a class="reference internal" href="#troubleshooting">Troubleshooting</a><ul>
<li><a class="reference internal" href="#sanity-check">Sanity Check</a><ul>
<li><a class="reference internal" href="#check-pytorch-xla-version">Check PyTorch/XLA Version</a></li>
<li><a class="reference internal" href="#perform-a-simple-calculation">Perform A Simple Calculation</a></li>
<li><a class="reference internal" href="#run-resnet-with-fake-data">Run Resnet With Fake Data</a></li>
</ul>
</li>
<li><a class="reference internal" href="#performance-debugging">Performance Debugging</a></li>
<li><a class="reference internal" href="#pytorch-xla-debugging-tool">PyTorch/XLA Debugging Tool</a><ul>
<li><a class="reference internal" href="#perform-a-auto-metrics-analysis">Perform A Auto-Metrics Analysis</a></li>
<li><a class="reference internal" href="#compilation-execution-analysis">Compilation &amp; Execution Analysis</a></li>
</ul>
</li>
<li><a class="reference internal" href="#get-a-metrics-report">Get A Metrics Report</a></li>
<li><a class="reference internal" href="#understand-the-metrics-report">Understand The Metrics Report</a></li>
<li><a class="reference internal" href="#clear-the-metrics-report">Clear The Metrics Report</a></li>
<li><a class="reference internal" href="#pytorch-xla-dynamo-debugging-tool">PyTorch/XLA + Dynamo Debugging Tool</a></li>
<li><a class="reference internal" href="#performance-profiling">Performance Profiling</a></li>
<li><a class="reference internal" href="#simple-benchmarking">Simple Benchmarking</a></li>
<li><a class="reference internal" href="#known-performance-caveats">Known Performance Caveats</a></li>
<li><a class="reference internal" href="#xla-tensor-quirks">XLA Tensor Quirks</a></li>
<li><a class="reference internal" href="#more-debugging-tools">More Debugging Tools</a><ul>
<li><a class="reference internal" href="#environment-variables">Environment Variables</a></li>
<li><a class="reference internal" href="#common-debugging-environment-variables-combinations">Common Debugging Environment Variables Combinations</a></li>
<li><a class="reference internal" href="#reproducing-pytorch-xla-ci-cd-unit-test-failures">Reproducing PyTorch/XLA CI/CD unit test failures.</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#pjrt-runtime">PJRT Runtime</a><ul>
<li><a class="reference internal" href="#tl-dr">TL;DR</a></li>
<li><a class="reference internal" href="#benefits">Benefits</a></li>
<li><a class="reference internal" href="#quickstart">Quickstart</a><ul>
<li><a class="reference internal" href="#cpu">CPU</a></li>
<li><a class="reference internal" href="#tpu">TPU</a><ul>
<li><a class="reference internal" href="#pods">Pods</a></li>
<li><a class="reference internal" href="#docker">Docker</a></li>
</ul>
</li>
<li><a class="reference internal" href="#gpu">GPU</a></li>
<li><a class="reference internal" href="#single-node-gpu-training">Single-node GPU training</a></li>
<li><a class="reference internal" href="#multi-node-gpu-training">Multi-node GPU training</a></li>
</ul>
</li>
<li><a class="reference internal" href="#differences-from-xrt">Differences from XRT</a><ul>
<li><a class="reference internal" href="#id20">Multithreading on TPU v2/v3</a></li>
<li><a class="reference internal" href="#changes-to-xm-rendezvous">Changes to xm.rendezvous</a></li>
<li><a class="reference internal" href="#pjrt-and-torch-distributed">PJRT and torch.distributed</a></li>
</ul>
</li>
<li><a class="reference internal" href="#performance">Performance</a><ul>
<li><a class="reference internal" href="#new-tpu-runtime">New TPU runtime</a></li>
</ul>
</li>
<li><a class="reference internal" href="#torchdynamo-torch-compile-integration-in-pytorch-xla">TorchDynamo(torch.compile) integration in PyTorch XLA</a><ul>
<li><a class="reference internal" href="#integration">Integration</a></li>
<li><a class="reference internal" href="#inference">Inference</a></li>
<li><a class="reference internal" href="#training">Training</a></li>
<li><a class="reference internal" href="#feature-gaps">Feature gaps</a></li>
<li><a class="reference internal" href="#take-away">Take away</a></li>
</ul>
</li>
<li><a class="reference internal" href="#fully-sharded-data-parallel-fsdp-in-pytorch-xla">Fully Sharded Data Parallel (FSDP) in PyTorch XLA</a><ul>
<li><a class="reference internal" href="#example-training-scripts-on-mnist-and-imagenet">Example training scripts on MNIST and ImageNet</a><ul>
<li><a class="reference internal" href="#installation">Installation</a></li>
<li><a class="reference internal" href="#clone-pytorch-xla-repo">Clone PyTorch/XLA repo</a></li>
<li><a class="reference internal" href="#train-mnist-on-v3-8-tpu">Train MNIST on v3-8 TPU</a></li>
<li><a class="reference internal" href="#train-imagenet-with-resnet-50-on-v3-8-tpu">Train ImageNet with ResNet-50 on v3-8 TPU</a></li>
</ul>
</li>
<li><a class="reference internal" href="#example-training-scripts-on-tpu-pod-with-10-billion-parameters">Example training scripts on TPU pod (with 10 billion parameters)</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#how-to-do-distributeddataparallel">How to do <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code></a><ul>
<li><a class="reference internal" href="#background-motivation">Background / Motivation</a></li>
<li><a class="reference internal" href="#how-to-use-distributeddataparallel">How to use DistributedDataParallel</a></li>
<li><a class="reference internal" href="#benchmarking">Benchmarking</a><ul>
<li><a class="reference internal" href="#resnet50-with-fake-data">Resnet50 with fake data</a></li>
<li><a class="reference internal" href="#mnist-with-fake-data">MNIST with fake data</a></li>
<li><a class="reference internal" href="#mnist-with-real-data">MNIST with real data</a></li>
</ul>
</li>
<li><a class="reference internal" href="#disclaimer">Disclaimer</a></li>
</ul>
</li>
<li><a class="reference internal" href="#how-to-run-with-pytorch-xla-gpu">How to run with PyTorch/XLA:GPU</a><ul>
<li><a class="reference internal" href="#create-a-gpu-instance">Create a GPU instance</a></li>
<li><a class="reference internal" href="#environment-setup">Environment Setup</a><ul>
<li><a class="reference internal" href="#id23">Docker</a></li>
<li><a class="reference internal" href="#check-environment-variable">Check environment variable</a></li>
<li><a class="reference internal" href="#wheel">Wheel</a></li>
</ul>
</li>
<li><a class="reference internal" href="#run-some-simple-models">Run some simple models</a><ul>
<li><a class="reference internal" href="#mp-imagenet-example">MP_ImageNet Example</a></li>
<li><a class="reference internal" href="#resnet-example">ResNet Example</a></li>
</ul>
</li>
<li><a class="reference internal" href="#amp-automatic-mixed-precision">AMP (AUTOMATIC MIXED PRECISION)</a></li>
<li><a class="reference internal" href="#develop-pytorch-xla-on-a-gpu-instance-build-pytorch-xla-from-source-with-gpu-support">Develop PyTorch/XLA on a GPU instance (build PyTorch/XLA from source with GPU support)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#pytorch-xla-spmd-user-guide">PyTorch/XLA SPMD User Guide</a><ul>
<li><a class="reference internal" href="#what-is-pytorch-xla-spmd">What is PyTorch/XLA SPMD?</a></li>
<li><a class="reference internal" href="#how-to-use-pytorch-xla-spmd">How to use PyTorch/XLA SPMD?</a><ul>
<li><a class="reference internal" href="#spmd-mode">SPMD Mode</a></li>
<li><a class="reference internal" href="#mesh">Mesh</a></li>
<li><a class="reference internal" href="#partition-spec">Partition Spec</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id31">Further Reading</a></li>
</ul>
</li>
<li><a class="reference internal" href="#fully-sharded-data-parallel-via-spmd">Fully Sharded Data Parallel via SPMD</a><ul>
<li><a class="reference internal" href="#sharding-output">Sharding output</a></li>
<li><a class="reference internal" href="#id35">Gradient checkpointing</a></li>
<li><a class="reference internal" href="#huggingface-llama-2-example">HuggingFace Llama 2 Example</a></li>
</ul>
</li>
</ul>
</div>
            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="#">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>PyTorch on XLA Devices</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="_sources/index.rst.txt" rel="nofollow"><img src="_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id="
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="section" id="pytorch-on-xla-devices">
<h1>PyTorch on XLA Devices<a class="headerlink" href="#pytorch-on-xla-devices" title="Permalink to this heading">¶</a></h1>
<p>PyTorch runs on XLA devices, like TPUs, with the
<a class="reference external" href="https://github.com/pytorch/xla/">torch_xla package</a>. This document describes
how to run your models on these devices.</p>
<div class="section" id="creating-an-xla-tensor">
<h2>Creating an XLA Tensor<a class="headerlink" href="#creating-an-xla-tensor" title="Permalink to this heading">¶</a></h2>
<p>PyTorch/XLA adds a new <code class="docutils literal notranslate"><span class="pre">xla</span></code> device type to PyTorch. This device type works just
like other PyTorch device types. For example, here’s how to create and
print an XLA tensor:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch_xla</span>
<span class="kn">import</span> <span class="nn">torch_xla.core.xla_model</span> <span class="k">as</span> <span class="nn">xm</span>

<span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
</pre></div>
</div>
<p>This code should look familiar. PyTorch/XLA uses the same interface as regular
PyTorch with a few additions. Importing <code class="docutils literal notranslate"><span class="pre">torch_xla</span></code> initializes PyTorch/XLA, and
<code class="docutils literal notranslate"><span class="pre">xm.xla_device()</span></code> returns the current XLA device. This may be a CPU or TPU
depending on your environment.</p>
</div>
<div class="section" id="xla-tensors-are-pytorch-tensors">
<h2>XLA Tensors are PyTorch Tensors<a class="headerlink" href="#xla-tensors-are-pytorch-tensors" title="Permalink to this heading">¶</a></h2>
<p>PyTorch operations can be performed on XLA tensors just like CPU or CUDA tensors.</p>
<p>For example, XLA tensors can be added together:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">t0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>
<span class="n">t1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">t0</span> <span class="o">+</span> <span class="n">t1</span><span class="p">)</span>
</pre></div>
</div>
<p>Or matrix multiplied:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">t0</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">t1</span><span class="p">))</span>
</pre></div>
</div>
<p>Or used with neural network modules:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">l_in</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>
<span class="n">linear</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>
<span class="n">l_out</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">l_in</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">l_out</span><span class="p">)</span>
</pre></div>
</div>
<p>Like other device types, XLA tensors only work with other XLA tensors on the
same device. So code like</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">l_in</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>
<span class="n">linear</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">l_out</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">l_in</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">l_out</span><span class="p">)</span>
<span class="c1"># Input tensor is not an XLA tensor: torch.FloatTensor</span>
</pre></div>
</div>
<p>will throw an error since the <code class="docutils literal notranslate"><span class="pre">torch.nn.Linear</span></code> module is on the CPU.</p>
</div>
<div class="section" id="running-models-on-xla-devices">
<h2>Running Models on XLA Devices<a class="headerlink" href="#running-models-on-xla-devices" title="Permalink to this heading">¶</a></h2>
<p>Building a new PyTorch network or converting an existing one to run on XLA
devices requires only a few lines of XLA-specific code. The following snippets
highlight these lines when running on a single device and multiple devices with XLA
multi-processing.</p>
<div class="section" id="running-on-a-single-xla-device">
<h3>Running on a Single XLA Device<a class="headerlink" href="#running-on-a-single-xla-device" title="Permalink to this heading">¶</a></h3>
<p>The following snippet shows a network training on a single XLA device:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch_xla.core.xla_model</span> <span class="k">as</span> <span class="nn">xm</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">)</span>

<span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
  <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
  <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
  <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
  <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
  <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

  <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
  <span class="n">xm</span><span class="o">.</span><span class="n">mark_step</span><span class="p">()</span>
</pre></div>
</div>
<p>This snippet highlights how easy it is to switch your model to run on XLA. The
model definition, dataloader, optimizer and training loop can work on any device.
The only XLA-specific code is a couple lines that acquire the XLA device and
mark the step. Calling
<code class="docutils literal notranslate"><span class="pre">xm.mark_step()</span></code> at the end of each training
iteration causes XLA to execute its current graph and update the model’s
parameters. See <a class="reference external" href="#xla-tensor-deep-dive">XLA Tensor Deep Dive</a> for more on
how XLA creates graphs and runs operations.</p>
</div>
<div class="section" id="running-on-multiple-xla-devices-with-multi-processing">
<h3>Running on Multiple XLA Devices with Multi-processing<a class="headerlink" href="#running-on-multiple-xla-devices-with-multi-processing" title="Permalink to this heading">¶</a></h3>
<p>PyTorch/XLA makes it easy to accelerate training by running on multiple XLA
devices. The following snippet shows how:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch_xla.core.xla_model</span> <span class="k">as</span> <span class="nn">xm</span>
<span class="kn">import</span> <span class="nn">torch_xla.distributed.parallel_loader</span> <span class="k">as</span> <span class="nn">pl</span>
<span class="kn">import</span> <span class="nn">torch_xla.distributed.xla_multiprocessing</span> <span class="k">as</span> <span class="nn">xmp</span>

<span class="k">def</span> <span class="nf">_mp_fn</span><span class="p">(</span><span class="n">index</span><span class="p">):</span>
  <span class="n">device</span> <span class="o">=</span> <span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">()</span>
  <span class="n">mp_device_loader</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">MpDeviceLoader</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>

  <span class="n">model</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
  <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>
  <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">)</span>

  <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">mp_device_loader</span><span class="p">:</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">xm</span><span class="o">.</span><span class="n">optimizer_step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
  <span class="n">xmp</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span><span class="n">_mp_fn</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">())</span>
</pre></div>
</div>
<p>There are three differences between this multi-device snippet and the previous
single device snippet. Let’s go over then one by one.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">xmp.spawn()</span></code></p>
<ul>
<li><p>Creates the processes that each run an XLA device.</p></li>
<li><p>Each process will only be able to access the device assigned to the current process. For example on a TPU v4-8, there will be 4 processes being spawn up and each process will own a TPU device.</p></li>
<li><p>Note that if you print the <code class="docutils literal notranslate"><span class="pre">xm.xla_device()</span></code> on each process you will see <code class="docutils literal notranslate"><span class="pre">xla:0</span></code> on all devices. This is because each process can only see one device. This does not mean multi-process is not functioning. The only execution is with PJRT runtime on TPU v2 and TPU v3 since there will be <code class="docutils literal notranslate"><span class="pre">#devices/2</span></code> processes and each process will have 2 threads(check this <a class="reference external" href="https://github.com/pytorch/xla/blob/master/docs/pjrt.md#tpus-v2v3-vs-v4">doc</a> for more details).</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">MpDeviceLoader</span></code></p>
<ul>
<li><p>Loads the training data onto each device.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MpDeviceLoader</span></code> can wrap on a torch dataloader. It can preload the data to the device and overlap the dataloading with device execution to improve the performance.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MpDeviceLoader</span></code> also call <code class="docutils literal notranslate"><span class="pre">xm.mark_step</span></code> for you every <code class="docutils literal notranslate"><span class="pre">batches_per_execution</span></code>(default to 1) batch being yield.</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">xm.optimizer_step(optimizer)</span></code></p>
<ul>
<li><p>Consolidates the gradients between devices and issues the XLA device step computation.</p></li>
<li><p>It is pretty much a <code class="docutils literal notranslate"><span class="pre">all_reduce_gradients</span></code> + <code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code> + <code class="docutils literal notranslate"><span class="pre">mark_step</span></code> and returns the loss being reduced.</p></li>
</ul>
</li>
</ul>
<p>The model definition, optimizer definition and training loop remain the same.</p>
<blockquote>
<div><p><strong>NOTE:</strong> It is important to note that, when using multi-processing, the user can start
retrieving and accessing XLA devices only from within the target function of
<code class="docutils literal notranslate"><span class="pre">xmp.spawn()</span></code> (or any function which has <code class="docutils literal notranslate"><span class="pre">xmp.spawn()</span></code> as parent in the call
stack).</p>
</div></blockquote>
<p>See the
<a class="reference external" href="https://github.com/pytorch/xla/blob/master/test/test_train_mp_mnist.py">full multiprocessing example</a>
for more on training a network on multiple XLA devices with multi-processing.</p>
</div>
<div class="section" id="running-on-tpu-pods">
<h3>Running on TPU Pods<a class="headerlink" href="#running-on-tpu-pods" title="Permalink to this heading">¶</a></h3>
<p>Multi-host setup for different accelerators can be very different. This doc will talk about the device independent bits of multi-host training and will use the TPU + PJRT runtime(currently available on 1.13 and 2.x releases) as an example.</p>
<p>Before you being, please take a look at our user guide at <a class="reference external" href="https://cloud.google.com/tpu/docs/run-calculation-pytorch">here</a> which will explain some Google Cloud basis like how to use <code class="docutils literal notranslate"><span class="pre">gcloud</span></code> command and how to setup your project. You can also check <a class="reference external" href="https://cloud.google.com/tpu/docs/how-to">here</a> for all Cloud TPU Howto. This doc will focus on the PyTorch/XLA perspective of the Setup.</p>
<p>Let’s assume you have the above mnist example from above section in a <code class="docutils literal notranslate"><span class="pre">train_mnist_xla.py</span></code>. If it is a single host multi device training, you would ssh to the TPUVM and run command like</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">PJRT_DEVICE</span><span class="o">=</span><span class="n">TPU</span> <span class="n">python3</span> <span class="n">train_mnist_xla</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
<p>Now in order to run the same models on a TPU v4-16 (which has 2 host, each with 4 TPU devices), you will need to</p>
<ul class="simple">
<li><p>Make sure each host can access the training script and training data. This is usually done by using the <code class="docutils literal notranslate"><span class="pre">gcloud</span> <span class="pre">scp</span></code> command or <code class="docutils literal notranslate"><span class="pre">gcloud</span> <span class="pre">ssh</span></code> command to copy the training scripts to all hosts.</p></li>
<li><p>Run the same training command on all hosts at the same time.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>gcloud alpha compute tpus tpu-vm ssh $USER-pjrt --zone=$ZONE --project=$PROJECT --worker=all --command=&quot;PJRT_DEVICE=TPU python3 train_mnist_xla.py&quot;
</pre></div>
</div>
<p>Above <code class="docutils literal notranslate"><span class="pre">gcloud</span> <span class="pre">ssh</span></code> command will ssh to all hosts in TPUVM Pod and run the same command at the same time..</p>
<blockquote>
<div><p><strong>NOTE:</strong> You need to run run above <code class="docutils literal notranslate"><span class="pre">gcloud</span></code> command outside of the TPUVM vm.</p>
</div></blockquote>
<p>The model code and training script is the same for the multi-process training and the multi-host training. PyTorch/XLA and the underlying infrastructure will make sure each device is aware of the global topology and each device’s local and global ordinal. Cross-device communication will happen across all devices instead of local devices.</p>
<p>For more details regarding PJRT runtime and how to run it on pod, please refer to this <a class="reference external" href="https://github.com/pytorch/xla/blob/master/docs/pjrt.md#tpu">doc</a>. For more information about PyTorch/XLA and TPU pod and a complete guide to run a resnet50 with fakedata on TPU pod, please refer to this <a class="reference external" href="https://cloud.google.com/tpu/docs/pytorch-pods">guide</a>.</p>
</div>
</div>
<div class="section" id="id3">
<h2>XLA Tensor Deep Dive<a class="headerlink" href="#id3" title="Permalink to this heading">¶</a></h2>
<p>Using XLA tensors and devices requires changing only a few lines of code. But
even though XLA tensors act a lot like CPU and CUDA tensors, their internals are
different. This section describes what makes XLA tensors unique.</p>
<div class="section" id="xla-tensors-are-lazy">
<h3>XLA Tensors are Lazy<a class="headerlink" href="#xla-tensors-are-lazy" title="Permalink to this heading">¶</a></h3>
<p>CPU and CUDA tensors launch operations immediately or <span class="raw-html-m2r"><b>eagerly</b></span>. XLA tensors,
on the other hand, are <span class="raw-html-m2r"><b>lazy</b></span>. They record operations in a graph until the
results are needed. Deferring execution like this lets XLA optimize it. A graph
of multiple separate operations might be fused into a single optimized
operation, for example.</p>
<p>Lazy execution is generally invisible to the caller. PyTorch/XLA automatically
constructs the graphs, sends them to XLA devices, and synchronizes when
copying data between an XLA device and the CPU. Inserting a barrier when
taking an optimizer step explicitly synchronizes the CPU and the XLA device. For
more information about our lazy tensor design, you can read <a class="reference external" href="https://arxiv.org/pdf/2102.13267.pdf">this paper</a>.</p>
</div>
<div class="section" id="xla-tensors-and-bfloat16">
<h3>XLA Tensors and bFloat16<a class="headerlink" href="#xla-tensors-and-bfloat16" title="Permalink to this heading">¶</a></h3>
<p>PyTorch/XLA can use the
<a class="reference external" href="https://en.wikipedia.org/wiki/Bfloat16_floating-point_format">bfloat16</a>
datatype when running on TPUs. In fact, PyTorch/XLA handles float types
(<code class="docutils literal notranslate"><span class="pre">torch.float</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.double</span></code>) differently on TPUs. This behavior is
controlled by the <code class="docutils literal notranslate"><span class="pre">XLA_USE_BF16</span></code> and <code class="docutils literal notranslate"><span class="pre">XLA_DOWNCAST_BF16</span></code> environment variable:</p>
<ul class="simple">
<li><p>By default both <code class="docutils literal notranslate"><span class="pre">torch.float</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.double</span></code> are
<code class="docutils literal notranslate"><span class="pre">torch.float</span></code> on TPUs.</p></li>
<li><p>If <code class="docutils literal notranslate"><span class="pre">XLA_USE_BF16</span></code> is set, then <code class="docutils literal notranslate"><span class="pre">torch.float</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.double</span></code> are both
<code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> on TPUs.</p></li>
<li><p>If <code class="docutils literal notranslate"><span class="pre">XLA_DOWNCAST_BF16</span></code> is set, then <code class="docutils literal notranslate"><span class="pre">torch.float</span></code> is <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> on TPUs and <code class="docutils literal notranslate"><span class="pre">torch.double</span></code> is <code class="docutils literal notranslate"><span class="pre">float32</span></code> on TPUs.</p></li>
<li><p>If a PyTorch tensor has <code class="docutils literal notranslate"><span class="pre">torch.bfloat16</span></code> data type, this will be directly
mapped to the TPU <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> (XLA <code class="docutils literal notranslate"><span class="pre">BF16</span></code> primitive type).</p></li>
</ul>
<p>Developers should note that <em>XLA tensors on TPUs will always report their PyTorch datatype</em> regardless of
the actual datatype they’re using. This conversion is automatic and opaque.
If an XLA tensor on a TPU is moved back to the CPU it will be converted
from its actual datatype to its PyTorch datatype. Depending on how your code operates, this conversion triggered by
the type of processing unit can be important.</p>
</div>
<div class="section" id="memory-layout">
<h3>Memory Layout<a class="headerlink" href="#memory-layout" title="Permalink to this heading">¶</a></h3>
<p>The internal data representation of XLA tensors is opaque to the user. They
do not expose their storage and they always appear to be contiguous, unlike
CPU and CUDA tensors. This allows XLA to adjust a tensor’s memory layout for
better performance.</p>
</div>
<div class="section" id="moving-xla-tensors-to-and-from-the-cpu">
<h3>Moving XLA Tensors to and from the CPU<a class="headerlink" href="#moving-xla-tensors-to-and-from-the-cpu" title="Permalink to this heading">¶</a></h3>
<p>XLA tensors can be moved from the CPU to an XLA device and from an XLA device
to the CPU. If a view is moved then the data its viewing is also copied to the
other device and the view relationship is not preserved. Put another way,
once data is copied to another device it has no relationship with its
previous device or any tensors on it. Again, depending on how your code operates,
appreciating and accommodating this transition can be important.</p>
</div>
<div class="section" id="saving-and-loading-xla-tensors">
<h3>Saving and Loading XLA Tensors<a class="headerlink" href="#saving-and-loading-xla-tensors" title="Permalink to this heading">¶</a></h3>
<p>XLA tensors should be moved to the CPU before saving, as in the following
snippet:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch_xla</span>
<span class="kn">import</span> <span class="nn">torch_xla.core.xla_model</span> <span class="k">as</span> <span class="nn">xm</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">()</span>

<span class="n">t0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">t1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="n">tensors</span> <span class="o">=</span> <span class="p">(</span><span class="n">t0</span><span class="o">.</span><span class="n">cpu</span><span class="p">(),</span> <span class="n">t1</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span>

<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="s1">&#39;tensors.pt&#39;</span><span class="p">)</span>

<span class="n">tensors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;tensors.pt&#39;</span><span class="p">)</span>

<span class="n">t0</span> <span class="o">=</span> <span class="n">tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">t1</span> <span class="o">=</span> <span class="n">tensors</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<p>This lets you put the loaded tensors on any available device, not just the one on which they were initialized.</p>
<p>Per the above note on moving XLA tensors to the CPU, care must be taken when
working with views. Instead of saving views it is recommended that you recreate
them after the tensors have been loaded and moved to their destination device(s).</p>
<p>A utility API is provided to save data by taking care of previously moving it
to CPU:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch_xla</span>
<span class="kn">import</span> <span class="nn">torch_xla.core.xla_model</span> <span class="k">as</span> <span class="nn">xm</span>

<span class="n">xm</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">path</span><span class="p">)</span>
</pre></div>
</div>
<p>In case of multiple devices, the above API will only save the data for the master
device ordinal (0).</p>
<p>In case where memory is limited compared to the size of the model parameters, an
API is provided that reduces the memory footprint on the host:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch_xla.utils.serialization</span> <span class="k">as</span> <span class="nn">xser</span>

<span class="n">xser</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">path</span><span class="p">)</span>
</pre></div>
</div>
<p>This API streams XLA tensors to CPU one at a time, reducing the amount of host
memory used, but it requires a matching load API to restore:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch_xla.utils.serialization</span> <span class="k">as</span> <span class="nn">xser</span>

<span class="n">state_dict</span> <span class="o">=</span> <span class="n">xser</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
</pre></div>
</div>
<p>Directly saving XLA tensors is possible but not recommended. XLA
tensors are always loaded back to the device they were saved from, and if
that device is unavailable the load will fail. PyTorch/XLA, like all of PyTorch,
is under active development and this behavior may change in the future.</p>
</div>
</div>
<div class="section" id="compilation-caching">
<h2>Compilation Caching<a class="headerlink" href="#compilation-caching" title="Permalink to this heading">¶</a></h2>
<p>The XLA compiler converts the traced HLO into an executable which runs on
the devices. Compilation can be time consuming, and in cases where the HLO
doesn’t change across executions, the compilation result can be persisted to
disk for reuse, significantly reducing development iteration time.</p>
<p>Note that if the HLO changes between executions, a recompilation will still
occur.</p>
<p>This is currently an experimental opt-in API, which must be activated before
any computations are executed. Initialization is done through the
<code class="docutils literal notranslate"><span class="pre">initialize_cache</span></code> API:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch_xla.runtime</span> <span class="k">as</span> <span class="nn">xr</span>
<span class="n">xr</span><span class="o">.</span><span class="n">initialize_cache</span><span class="p">(</span><span class="s1">&#39;YOUR_CACHE_PATH&#39;</span><span class="p">,</span> <span class="n">readonly</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>This will initialize a persistent compilation cache at the specified path. The
<code class="docutils literal notranslate"><span class="pre">readonly</span></code> parameter can be used to control whether the worker will be able to
write to the cache, which can be useful when a shared cache mount is used for
an SPMD workload.</p>
</div>
<div class="section" id="further-reading">
<h2>Further Reading<a class="headerlink" href="#further-reading" title="Permalink to this heading">¶</a></h2>
<p>Additional documentation is available at the
<a class="reference external" href="https://github.com/pytorch/xla/">PyTorch/XLA repo</a>. More examples of running
networks on TPUs are available
<a class="reference external" href="https://github.com/pytorch-tpu/examples">here</a>.</p>
</div>
</div>
<div class="section" id="pytorch-xla-api">
<h1>PyTorch/XLA API<a class="headerlink" href="#pytorch-xla-api" title="Permalink to this heading">¶</a></h1>
<div class="section" id="module-torch_xla">
<span id="torch-xla"></span><h2>torch_xla<a class="headerlink" href="#module-torch_xla" title="Permalink to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.device">
<span class="sig-prename descclassname"><span class="pre">torch_xla.</span></span><span class="sig-name descname"><span class="pre">device</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">device</span></span></span><a class="reference internal" href="_modules/torch_xla/torch_xla.html#device"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.device" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a given instance of an XLA device.</p>
<p>If SPMD enables, returns a virtual device that wraps all devices available
to this process.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>index</strong> – index of the XLA device to be returned. Corresponds to index in
<cite>torch_xla.devices()</cite>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>An XLA <cite>torch.device</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.devices">
<span class="sig-prename descclassname"><span class="pre">torch_xla.</span></span><span class="sig-name descname"><span class="pre">devices</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">device</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/torch_xla/torch_xla.html#devices"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.devices" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns all devices available in the current process.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A list of XLA <cite>torch.devices</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.device_count">
<span class="sig-prename descclassname"><span class="pre">torch_xla.</span></span><span class="sig-name descname"><span class="pre">device_count</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="reference internal" href="_modules/torch_xla/torch_xla.html#device_count"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.device_count" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns number of addressable devices in the current process.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.sync">
<span class="sig-prename descclassname"><span class="pre">torch_xla.</span></span><span class="sig-name descname"><span class="pre">sync</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/torch_xla.html#sync"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.sync" title="Permalink to this definition">¶</a></dt>
<dd><p>Launches all pending graph operations.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.step">
<span class="sig-prename descclassname"><span class="pre">torch_xla.</span></span><span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/torch_xla.html#step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Wraps code that should be dispatched to the runtime.</p>
<p>Experimental: <cite>xla.step</cite> is still a work in progress. Some code that currently
works with <cite>xla.step</cite> but does not follow best practices will become errors in
future releases. See <a class="reference external" href="https://github.com/pytorch/xla/issues/6751">https://github.com/pytorch/xla/issues/6751</a> for context.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.manual_seed">
<span class="sig-prename descclassname"><span class="pre">torch_xla.</span></span><span class="sig-name descname"><span class="pre">manual_seed</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">seed</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/torch_xla.html#manual_seed"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.manual_seed" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the seed for generating random numbers for the current XLA device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>seed</strong> (<em>python:integer</em>) – The state to be set.</p></li>
<li><p><strong>device</strong> (<em>torch.device</em><em>, </em><em>optional</em>) – The device where the RNG state needs to be set.
If missing the default device seed will be set.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-torch_xla.runtime">
<span id="runtime"></span><h2>runtime<a class="headerlink" href="#module-torch_xla.runtime" title="Permalink to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.runtime.device_type">
<span class="sig-prename descclassname"><span class="pre">torch_xla.runtime.</span></span><span class="sig-name descname"><span class="pre">device_type</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/torch_xla/runtime.html#device_type"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.runtime.device_type" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the current PjRt device type.</p>
<p>Selects a default device if none has been configured</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.runtime.local_process_count">
<span class="sig-prename descclassname"><span class="pre">torch_xla.runtime.</span></span><span class="sig-name descname"><span class="pre">local_process_count</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="reference internal" href="_modules/torch_xla/runtime.html#local_process_count"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.runtime.local_process_count" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the number of processes running on this host.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.runtime.local_device_count">
<span class="sig-prename descclassname"><span class="pre">torch_xla.runtime.</span></span><span class="sig-name descname"><span class="pre">local_device_count</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="reference internal" href="_modules/torch_xla/runtime.html#local_device_count"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.runtime.local_device_count" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the total number of devices on this host.</p>
<p>Assumes each process has the same number of addressable devices.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.runtime.addressable_device_count">
<span class="sig-prename descclassname"><span class="pre">torch_xla.runtime.</span></span><span class="sig-name descname"><span class="pre">addressable_device_count</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="reference internal" href="_modules/torch_xla/runtime.html#addressable_device_count"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.runtime.addressable_device_count" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the number of devices visible to this process.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.runtime.global_device_count">
<span class="sig-prename descclassname"><span class="pre">torch_xla.runtime.</span></span><span class="sig-name descname"><span class="pre">global_device_count</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="reference internal" href="_modules/torch_xla/runtime.html#global_device_count"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.runtime.global_device_count" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the total number of devices across all processes/hosts.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.runtime.global_runtime_device_count">
<span class="sig-prename descclassname"><span class="pre">torch_xla.runtime.</span></span><span class="sig-name descname"><span class="pre">global_runtime_device_count</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="reference internal" href="_modules/torch_xla/runtime.html#global_runtime_device_count"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.runtime.global_runtime_device_count" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the total number of runtime devices across all processes/hosts, especially useful for SPMD.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.runtime.world_size">
<span class="sig-prename descclassname"><span class="pre">torch_xla.runtime.</span></span><span class="sig-name descname"><span class="pre">world_size</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="reference internal" href="_modules/torch_xla/runtime.html#world_size"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.runtime.world_size" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the total number of processes participating in the job.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.runtime.global_ordinal">
<span class="sig-prename descclassname"><span class="pre">torch_xla.runtime.</span></span><span class="sig-name descname"><span class="pre">global_ordinal</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="reference internal" href="_modules/torch_xla/runtime.html#global_ordinal"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.runtime.global_ordinal" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns global ordinal of this thread within all processes.</p>
<p>Global ordinal is in range [0, global_device_count). Global ordinals are not
guaranteed to have any predictable relationship to the TPU worker ID nor are
they guaranteed to be contiguous on each host.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.runtime.local_ordinal">
<span class="sig-prename descclassname"><span class="pre">torch_xla.runtime.</span></span><span class="sig-name descname"><span class="pre">local_ordinal</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="reference internal" href="_modules/torch_xla/runtime.html#local_ordinal"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.runtime.local_ordinal" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns local ordinal of this thread within this host.</p>
<p>Local ordinal is in range [0, local_device_count).</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.runtime.get_master_ip">
<span class="sig-prename descclassname"><span class="pre">torch_xla.runtime.</span></span><span class="sig-name descname"><span class="pre">get_master_ip</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="reference internal" href="_modules/torch_xla/runtime.html#get_master_ip"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.runtime.get_master_ip" title="Permalink to this definition">¶</a></dt>
<dd><p>Retrieve the master worker IP for the runtime. This calls into
backend-specific discovery APIs.</p>
<p>Returns master worker’s IP address as a string.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.runtime.use_spmd">
<span class="sig-prename descclassname"><span class="pre">torch_xla.runtime.</span></span><span class="sig-name descname"><span class="pre">use_spmd</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">auto</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/runtime.html#use_spmd"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.runtime.use_spmd" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.runtime.is_spmd">
<span class="sig-prename descclassname"><span class="pre">torch_xla.runtime.</span></span><span class="sig-name descname"><span class="pre">is_spmd</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/runtime.html#is_spmd"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.runtime.is_spmd" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns if SPMD is set for execution.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.runtime.initialize_cache">
<span class="sig-prename descclassname"><span class="pre">torch_xla.runtime.</span></span><span class="sig-name descname"><span class="pre">initialize_cache</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">readonly</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/runtime.html#initialize_cache"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.runtime.initialize_cache" title="Permalink to this definition">¶</a></dt>
<dd><p>Initializes the persistent compilation cache. This API must be called
before any computations have been performed.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>path</strong> – The path at which to store the persistent cache.</p></li>
<li><p><strong>readonly</strong> – Whether or not this worker should have write access to the cache.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-torch_xla.core.xla_model">
<span id="xla-model"></span><h2>xla_model<a class="headerlink" href="#module-torch_xla.core.xla_model" title="Permalink to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.core.xla_model.xla_device">
<span class="sig-prename descclassname"><span class="pre">torch_xla.core.xla_model.</span></span><span class="sig-name descname"><span class="pre">xla_device</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">devkind</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#xla_device"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.core.xla_model.xla_device" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a given instance of an XLA device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n</strong> (<em>python:int</em><em>, </em><em>optional</em>) – The specific instance (ordinal) to be returned. If
specified, the specific XLA device instance will be returned. Otherwise
the first device of <cite>devkind</cite> will be returned.</p></li>
<li><p><strong>devkind</strong> (<em>string</em><em>...</em><em>, </em><em>optional</em>) – If specified, device type such as <cite>TPU</cite>,
<cite>CUDA</cite>, <cite>CPU</cite>, or custom PJRT device. Deprecated.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>torch.device</cite> with the requested instance.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.core.xla_model.xla_device_hw">
<span class="sig-prename descclassname"><span class="pre">torch_xla.core.xla_model.</span></span><span class="sig-name descname"><span class="pre">xla_device_hw</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#xla_device_hw"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.core.xla_model.xla_device_hw" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the hardware type of the given device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<em>string</em><em> or </em><em>torch.device</em>) – The xla device that will be mapped to the
real device.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A string representation of the hardware type of the given device.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.core.xla_model.is_master_ordinal">
<span class="sig-prename descclassname"><span class="pre">torch_xla.core.xla_model.</span></span><span class="sig-name descname"><span class="pre">is_master_ordinal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">local</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#is_master_ordinal"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.core.xla_model.is_master_ordinal" title="Permalink to this definition">¶</a></dt>
<dd><p>Checks whether the current process is the master ordinal (0).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>local</strong> (<em>bool</em>) – Whether the local or global master ordinal should be checked.
In case of multi-host replication, there is only one global master ordinal
(host 0, device 0), while there are NUM_HOSTS local master ordinals.
Default: True</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A boolean indicating whether the current process is the master ordinal.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.core.xla_model.all_reduce">
<span class="sig-prename descclassname"><span class="pre">torch_xla.core.xla_model.</span></span><span class="sig-name descname"><span class="pre">all_reduce</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">reduce_type</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pin_layout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#all_reduce"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.core.xla_model.all_reduce" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs an inplace reduce operation on the input tensor(s).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>reduce_type</strong> (<em>string</em>) – One of <code class="docutils literal notranslate"><span class="pre">xm.REDUCE_SUM</span></code>, <code class="docutils literal notranslate"><span class="pre">xm.REDUCE_MUL</span></code>,
<code class="docutils literal notranslate"><span class="pre">xm.REDUCE_AND</span></code>, <code class="docutils literal notranslate"><span class="pre">xm.REDUCE_OR</span></code>, <code class="docutils literal notranslate"><span class="pre">xm.REDUCE_MIN</span></code> and
<code class="docutils literal notranslate"><span class="pre">xm.REDUCE_MAX</span></code>.</p></li>
<li><p><strong>inputs</strong> – Either a single <cite>torch.Tensor</cite> or a list of <cite>torch.Tensor</cite> to
perform the all reduce op to.</p></li>
<li><p><strong>scale</strong> (<em>python:float</em>) – A default scaling value to be applied after the reduce.
Default: 1.0</p></li>
<li><p><strong>groups</strong> (<em>list</em><em>, </em><em>optional</em>) – <p>A list of list, representing the replica groups for
the <cite>all_reduce()</cite> operation. Example: <cite>[[0, 1, 2, 3], [4, 5, 6, 7]]</cite></p>
<blockquote>
<div><p>defines two groups, one with the <cite>[0, 1, 2, 3]</cite> replicas and one with
the <cite>[4, 5, 6, 7]</cite> replicas. If <cite>None</cite> there will be only one group with
all the replicas in it.</p>
</div></blockquote>
</p></li>
<li><p><strong>pin_layout</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether to pin the layout for this communication op.
Layout pining can prevent potential data corruption when each process that
participate in the communication has slightly different program, but it might
cause some xla compilation to fail. Unpin the layout when you see error message
like “HloModule has a mix of layout constrained”.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>If a single <cite>torch.Tensor</cite> is passed, the return value is a <cite>torch.Tensor</cite>
holding the reduced value (across the replicas). If a list/tuple is passed,
this function performs an inplace all-reduce op on the input tensors, and
returns the list/tuple itself.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.core.xla_model.all_gather">
<span class="sig-prename descclassname"><span class="pre">torch_xla.core.xla_model.</span></span><span class="sig-name descname"><span class="pre">all_gather</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pin_layout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#all_gather"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.core.xla_model.all_gather" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs an all-gather operation along a given dimension.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>value</strong> (<em>torch.Tensor</em>) – The input tensor.</p></li>
<li><p><strong>dim</strong> (<em>python:int</em>) – The gather dimension.
Default: 0</p></li>
<li><p><strong>groups</strong> (<em>list</em><em>, </em><em>optional</em>) – <p>A list of list, representing the replica groups for
the <cite>all_gather()</cite> operation. Example: <cite>[[0, 1, 2, 3], [4, 5, 6, 7]]</cite></p>
<blockquote>
<div><p>defines two groups, one with the <cite>[0, 1, 2, 3]</cite> replicas and one with
the <cite>[4, 5, 6, 7]</cite> replicas. If <cite>None</cite> there will be only one group with
all the replicas in it.</p>
</div></blockquote>
</p></li>
<li><p><strong>output</strong> (<em>torch.Tensor</em>) – Optional output tensor.</p></li>
<li><p><strong>pin_layout</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether to pin the layout for this communication op.
Layout pining can prevent potential data corruption when each process that
participate in the communication has slightly different program, but it might
cause some xla compilation to fail. Unpin the layout when you see error message
like “HloModule has a mix of layout constrained”.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor which has, in the <code class="docutils literal notranslate"><span class="pre">dim</span></code> dimension, all the values from the
participating replicas.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.core.xla_model.all_to_all">
<span class="sig-prename descclassname"><span class="pre">torch_xla.core.xla_model.</span></span><span class="sig-name descname"><span class="pre">all_to_all</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">split_dimension</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">concat_dimension</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">split_count</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pin_layout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#all_to_all"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.core.xla_model.all_to_all" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs an XLA <cite>AllToAll()</cite> operation on the input tensor.</p>
<p>See: <a class="reference external" href="https://www.tensorflow.org/xla/operation_semantics#alltoall">https://www.tensorflow.org/xla/operation_semantics#alltoall</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>value</strong> (<em>torch.Tensor</em>) – The input tensor.</p></li>
<li><p><strong>split_dimension</strong> (<em>python:int</em>) – The dimension upon which the split should happen.</p></li>
<li><p><strong>concat_dimension</strong> (<em>python:int</em>) – The dimension upon which the concat should happen.</p></li>
<li><p><strong>split_count</strong> (<em>python:int</em>) – The split count.</p></li>
<li><p><strong>groups</strong> (<em>list</em><em>, </em><em>optional</em>) – <p>A list of list, representing the replica groups for
the <cite>all_reduce()</cite> operation. Example: <cite>[[0, 1, 2, 3], [4, 5, 6, 7]]</cite></p>
<blockquote>
<div><p>defines two groups, one with the <cite>[0, 1, 2, 3]</cite> replicas and one with
the <cite>[4, 5, 6, 7]</cite> replicas. If <cite>None</cite> there will be only one group with
all the replicas in it.</p>
</div></blockquote>
</p></li>
<li><p><strong>pin_layout</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether to pin the layout for this communication op.
Layout pining can prevent potential data corruption when each process that
participate in the communication has slightly different program, but it might
cause some xla compilation to fail. Unpin the layout when you see error message
like “HloModule has a mix of layout constrained”.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The result <cite>torch.Tensor</cite> of the <cite>all_to_all()</cite> operation.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.core.xla_model.add_step_closure">
<span class="sig-prename descclassname"><span class="pre">torch_xla.core.xla_model.</span></span><span class="sig-name descname"><span class="pre">add_step_closure</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">closure</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">run_async</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#add_step_closure"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.core.xla_model.add_step_closure" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a closure to the list of the ones to be run at the end of the step.</p>
<p>Many times during model training there is the need to print/report (print to
console, post to tensorboard, etc…) information which require the content of
intermediary tensors to be inspected.
Inspecting different tensors content in different points of the model code
requires many executions and typically causes performance issues.
Adding a step closure will ensure that it will be run after the barrier, when
all the live tensors will be already materialized to device data.
Live tensors which will include the ones captured by the closure arguments.
So using <cite>add_step_closure()</cite> will ensure a single execution will be
performed, even when multiple closures are queued, requiring multiple tensors
to be inspected.
Step closures will be run sequentially in the order they have been queued.
Note that even though using this API the execution will be optimized, it is
advised to throttle the printing/reporting events once every N steps.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>closure</strong> (<em>callable</em>) – The function to be called.</p></li>
<li><p><strong>args</strong> (<em>tuple</em>) – The arguments to be passed to the closure.</p></li>
<li><p><strong>run_async</strong> – If True, run the closure asynchronously.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.core.xla_model.wait_device_ops">
<span class="sig-prename descclassname"><span class="pre">torch_xla.core.xla_model.</span></span><span class="sig-name descname"><span class="pre">wait_device_ops</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">devices</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[]</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#wait_device_ops"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.core.xla_model.wait_device_ops" title="Permalink to this definition">¶</a></dt>
<dd><p>Waits for all the async operations on the given devices to complete.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>devices</strong> (<em>string</em><em>...</em><em>, </em><em>optional</em>) – The devices whose async ops need to be waited
for. If empty, all the local devices will be waited for.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.core.xla_model.optimizer_step">
<span class="sig-prename descclassname"><span class="pre">torch_xla.core.xla_model.</span></span><span class="sig-name descname"><span class="pre">optimizer_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">barrier</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">{}</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pin_layout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#optimizer_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.core.xla_model.optimizer_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Run the provided optimizer step and issue the XLA device step computation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Optimizer</span></code>) – The <cite>torch.Optimizer</cite> instance whose
<cite>step()</cite> function needs to be called. The <cite>step()</cite> function will be called
with the <cite>optimizer_args</cite> named arguments.</p></li>
<li><p><strong>barrier</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether the XLA tensor barrier should be issued in
this API. If using the PyTorch XLA <cite>ParallelLoader</cite> or <cite>DataParallel</cite>
support, this is not necessary as the barrier will be issued by the XLA
data loader iterator <cite>next()</cite> call.
Default: False</p></li>
<li><p><strong>optimizer_args</strong> (<em>dict</em><em>, </em><em>optional</em>) – Named arguments dictionary for the
<cite>optimizer.step()</cite> call.</p></li>
<li><p><strong>groups</strong> (<em>list</em><em>, </em><em>optional</em>) – <p>A list of list, representing the replica groups for
the <cite>all_reduce()</cite> operation. Example: <cite>[[0, 1, 2, 3], [4, 5, 6, 7]]</cite></p>
<blockquote>
<div><p>defines two groups, one with the <cite>[0, 1, 2, 3]</cite> replicas and one with
the <cite>[4, 5, 6, 7]</cite> replicas. If <cite>None</cite> there will be only one group with
all the replicas in it.</p>
</div></blockquote>
</p></li>
<li><p><strong>pin_layout</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether to pin the layout when reducing gradients.
See <cite>xm.all_reduce</cite> for details.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The same value returned by the <cite>optimizer.step()</cite> call.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.core.xla_model.save">
<span class="sig-prename descclassname"><span class="pre">torch_xla.core.xla_model.</span></span><span class="sig-name descname"><span class="pre">save</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">file_or_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">master_only</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">global_master</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#save"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.core.xla_model.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the input data into a file.</p>
<p>The saved data is transferred to PyTorch CPU device before being saved, so a
following <cite>torch.load()</cite> will load CPU data.
Care must be taken when working with views. Instead of saving views it’s
recommended that you recreate them after the tensors have been loaded and
moved to their destination device(s).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – The input data to be saved. Any nested combination of Python objects
(list, tuples, sets, dicts, …).</p></li>
<li><p><strong>file_or_path</strong> – The destination for the data saving operation. Either a file
path or a Python file object. If <cite>master_only</cite> is <code class="docutils literal notranslate"><span class="pre">False</span></code> the path or
file objects must point to different destinations as otherwise all the
writes from the same host will override each other.</p></li>
<li><p><strong>master_only</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether only the master device should save the
data. If False, the <cite>file_or_path</cite> argument should be a different file or
path for each of the ordinals taking part to the replication, otherwise
all the replicas on the same host will be writing to the same location.
Default: True</p></li>
<li><p><strong>global_master</strong> (<em>bool</em><em>, </em><em>optional</em>) – When <code class="docutils literal notranslate"><span class="pre">master_only</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code> this flag
controls whether every host’s master (if <code class="docutils literal notranslate"><span class="pre">global_master</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>)
saves the content, or only the global master (ordinal 0).
Default: False</p></li>
<li><p><strong>sync</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to synchronize all replicas after saving
tensors. If True, all replicas must call <cite>xm.save</cite> or the main process
will hang.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.core.xla_model.rendezvous">
<span class="sig-prename descclassname"><span class="pre">torch_xla.core.xla_model.</span></span><span class="sig-name descname"><span class="pre">rendezvous</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tag</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">payload</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">b''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">replicas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[]</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#rendezvous"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.core.xla_model.rendezvous" title="Permalink to this definition">¶</a></dt>
<dd><p>Waits for all the mesh clients to reach the named rendezvous.</p>
<p>Note: PJRT does not support the XRT mesh server, so this is effectively an
alias to <cite>xla_rendezvous</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tag</strong> (<em>string</em>) – The name of the rendezvous to join.</p></li>
<li><p><strong>payload</strong> (<em>bytes</em><em>, </em><em>optional</em>) – The payload to be sent to the rendezvous.</p></li>
<li><p><strong>replicas</strong> (<em>list</em><em>, </em><em>python:int</em>) – The replica ordinals taking part of the rendezvous.
Empty means all replicas in the mesh.
Default: []</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The payloads exchanged by all the other cores, with the payload of core
ordinal <cite>i</cite> at position <cite>i</cite> in the returned tuple.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.core.xla_model.mesh_reduce">
<span class="sig-prename descclassname"><span class="pre">torch_xla.core.xla_model.</span></span><span class="sig-name descname"><span class="pre">mesh_reduce</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tag</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce_fn</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#mesh_reduce"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.core.xla_model.mesh_reduce" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs an out-of-graph client mesh reduction.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tag</strong> (<em>string</em>) – The name of the rendezvous to join.</p></li>
<li><p><strong>data</strong> – The data to be reduced. The <cite>reduce_fn</cite> callable will receive a list
with the copies of the same data coming from all the mesh client processes
(one per core).</p></li>
<li><p><strong>reduce_fn</strong> (<em>callable</em>) – A function which receives a list of <cite>data</cite>-like
objects and returns the reduced result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The reduced value.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.core.xla_model.set_rng_state">
<span class="sig-prename descclassname"><span class="pre">torch_xla.core.xla_model.</span></span><span class="sig-name descname"><span class="pre">set_rng_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">seed</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#set_rng_state"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.core.xla_model.set_rng_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the random number generator state.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>seed</strong> (<em>python:integer</em>) – The state to be set.</p></li>
<li><p><strong>device</strong> (<em>string</em><em>, </em><em>optional</em>) – The device where the RNG state needs to be set.
If missing the default device seed will be set.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.core.xla_model.get_rng_state">
<span class="sig-prename descclassname"><span class="pre">torch_xla.core.xla_model.</span></span><span class="sig-name descname"><span class="pre">get_rng_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#get_rng_state"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.core.xla_model.get_rng_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Gets the current running random number generator state.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<em>string</em><em>, </em><em>optional</em>) – The device whose RNG state needs to be retrieved.
If missing the default device seed will be set.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The RNG state, as integer.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.core.xla_model.get_memory_info">
<span class="sig-prename descclassname"><span class="pre">torch_xla.core.xla_model.</span></span><span class="sig-name descname"><span class="pre">get_memory_info</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">MemoryInfo</span></span></span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#get_memory_info"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.core.xla_model.get_memory_info" title="Permalink to this definition">¶</a></dt>
<dd><p>Retrieves the device memory usage.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> – The device whose memory information are requested.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>MemoryInfo dict with memory usage for the given device.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.core.xla_model.get_stablehlo">
<span class="sig-prename descclassname"><span class="pre">torch_xla.core.xla_model.</span></span><span class="sig-name descname"><span class="pre">get_stablehlo</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensors</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#get_stablehlo"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.core.xla_model.get_stablehlo" title="Permalink to this definition">¶</a></dt>
<dd><p>Get StableHLO for the computation graph in string format.</p>
<p>If <cite>tensors</cite> is not empty, the graph with <cite>tensors</cite> as outputs will be dump.
If <cite>tensors</cite> is empty, the whole computation graph will be dump.
TODO(lsy323): When <cite>tensors</cite> is empty, the some intermediate tensors will also be
dump as outputs. Need further investigation.</p>
<p>For inference graph, it is recommended to pass the model outputs to <cite>tensors</cite>.
For training graph, it is not straightforward to identify the “outputs”. Using empty <cite>tensors</cite> is recommended.</p>
<p>To enable source line info in StableHLO, please set env var XLA_HLO_DEBUG=1.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tensors</strong> (<em>list</em><em>[</em><em>torch.Tensor</em><em>]</em><em>, </em><em>optional</em>) – Tensors that represent the output/root of the StableHLO graph.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>StableHLO Module in string format.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.core.xla_model.get_stablehlo_bytecode">
<span class="sig-prename descclassname"><span class="pre">torch_xla.core.xla_model.</span></span><span class="sig-name descname"><span class="pre">get_stablehlo_bytecode</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensors</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bytes</span></span></span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#get_stablehlo_bytecode"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.core.xla_model.get_stablehlo_bytecode" title="Permalink to this definition">¶</a></dt>
<dd><p>Get StableHLO for the computation graph in bytecode format.</p>
<p>If <cite>tensors</cite> is not empty, the graph with <cite>tensors</cite> as outputs will be dump.
If <cite>tensors</cite> is empty, the whole computation graph will be dump.
TODO(lsy323): When <cite>tensors</cite> is empty, the some intermediate tensors will also be
dump as outputs. Need further investigation.</p>
<p>For inference graph, it is recommended to pass the model outputs to <cite>tensors</cite>.
For training graph, it is not straightforward to identify the “outputs”. Using empty <cite>tensors</cite> is recommended.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tensors</strong> (<em>list</em><em>[</em><em>torch.Tensor</em><em>]</em><em>, </em><em>optional</em>) – Tensors that represent the output/root of the StableHLO graph.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>StableHLO Module in bytecode format.</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-torch_xla.distributed.parallel_loader">
<span id="distributed"></span><h2>distributed<a class="headerlink" href="#module-torch_xla.distributed.parallel_loader" title="Permalink to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch_xla.distributed.parallel_loader.ParallelLoader">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch_xla.distributed.parallel_loader.</span></span><span class="sig-name descname"><span class="pre">ParallelLoader</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loader</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">devices</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batchdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batches_per_execution</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loader_prefetch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_prefetch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">host_to_device_transfer_threads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_sharding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/distributed/parallel_loader.html#ParallelLoader"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.distributed.parallel_loader.ParallelLoader" title="Permalink to this definition">¶</a></dt>
<dd><p>Wraps an existing PyTorch DataLoader with background data upload.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loader</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code>) – The PyTorch DataLoader to be
wrapped.</p></li>
<li><p><strong>devices</strong> (<cite>torch.device</cite>…) – The list of devices where the data has to be
sent. The i-th sample returned by the <cite>loader</cite> will be sent to <cite>devices[i
% len(devices)]</cite>.</p></li>
<li><p><strong>batchdim</strong> (<em>python:int</em><em>, </em><em>optional</em>) – The dimension which is holding the batch size.
Default: 0</p></li>
<li><p><strong>loader_prefetch_size</strong> (<em>python:int</em><em>, </em><em>optional</em>) – The max capacity of the queue used by
the thread which is reading samples from the <cite>loader</cite>, to be processed by
the worker threads which upload data to the devices.
Default: 8</p></li>
<li><p><strong>device_prefetch_size</strong> (<em>python:int</em><em>, </em><em>optional</em>) – The max size of the per-device queues,
where the worker threads deposit tensors which have already been sent to
devices.
Default: 4</p></li>
<li><p><strong>host_to_device_transfer_threads</strong> (<em>python:int</em><em>, </em><em>optional</em>) – The number of threads that
work in parallel to transfer data from loader queue to device queue.
Default: 1</p></li>
<li><p><strong>input_sharding</strong> (<a class="reference internal" href="#torch_xla.distributed.spmd.ShardingSpec" title="torch_xla.distributed.spmd.ShardingSpec"><em>ShardingSpec</em></a><em>, </em><em>optional</em>) – Sharding spec to apply to
compatible input tensors after loading.
Default: None</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="torch_xla.distributed.parallel_loader.ParallelLoader.per_device_loader">
<span class="sig-name descname"><span class="pre">per_device_loader</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/distributed/parallel_loader.html#ParallelLoader.per_device_loader"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.distributed.parallel_loader.ParallelLoader.per_device_loader" title="Permalink to this definition">¶</a></dt>
<dd><p>Retrieves the loader iterator object for the given device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<cite>torch.device</cite>) – The device whole loader is being requested.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The loader iterator object for the <cite>device</cite>. This is not a
<cite>torch.utils.data.DataLoader</cite> interface, but a Python iterator which
returns the same tensor data structure as returned by the wrapped
<cite>torch.utils.data.DataLoader</cite>, but residing on XLA devices.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<span class="target" id="module-torch_xla.distributed.xla_multiprocessing"></span><dl class="py function">
<dt class="sig sig-object py" id="torch_xla.distributed.xla_multiprocessing.spawn">
<span class="sig-prename descclassname"><span class="pre">torch_xla.distributed.xla_multiprocessing.</span></span><span class="sig-name descname"><span class="pre">spawn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nprocs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">join</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">daemon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">start_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'spawn'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/distributed/xla_multiprocessing.html#spawn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.distributed.xla_multiprocessing.spawn" title="Permalink to this definition">¶</a></dt>
<dd><p>Enables multi processing based replication.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fn</strong> (<em>callable</em>) – The function to be called for each device which takes part of
the replication. The function will be called with a first argument being
the global index of the process within the replication, followed by the
arguments passed in <cite>args</cite>.</p></li>
<li><p><strong>args</strong> (<em>tuple</em>) – The arguments for <cite>fn</cite>.
Default: Empty tuple</p></li>
<li><p><strong>nprocs</strong> (<em>python:int</em>) – The number of processes/devices for the replication. At the
moment, if specified, can be either 1 or the maximum number of devices.</p></li>
<li><p><strong>join</strong> (<em>bool</em>) – Whether the call should block waiting for the completion of the
processes which have being spawned.
Default: True</p></li>
<li><p><strong>daemon</strong> (<em>bool</em>) – Whether the processes being spawned should have the <cite>daemon</cite>
flag set (see Python multi-processing API).
Default: False</p></li>
<li><p><strong>start_method</strong> (<em>string</em>) – The Python <cite>multiprocessing</cite> process creation method.
Default: <cite>spawn</cite></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The same object returned by the <cite>torch.multiprocessing.spawn</cite> API. If
<cite>nprocs</cite> is 1 the <cite>fn</cite> function will be called directly, and the API will
return None.</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-torch_xla.distributed.spmd">
<span id="spmd"></span><h2>spmd<a class="headerlink" href="#module-torch_xla.distributed.spmd" title="Permalink to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.distributed.spmd.mark_sharding">
<span class="sig-prename descclassname"><span class="pre">torch_xla.distributed.spmd.</span></span><span class="sig-name descname"><span class="pre">mark_sharding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">t</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">XLAShardedTensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mesh</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torch_xla.distributed.spmd.Mesh" title="torch_xla.distributed.spmd.xla_sharding.Mesh"><span class="pre">Mesh</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">partition_spec</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">XLAShardedTensor</span></span></span><a class="reference internal" href="_modules/torch_xla/distributed/spmd/xla_sharding.html#mark_sharding"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.distributed.spmd.mark_sharding" title="Permalink to this definition">¶</a></dt>
<dd><p>Annotates the tensor provided with XLA partition spec. Internally,
it annotates the corresponding XLATensor as sharded for the XLA SpmdPartitioner pass.
:param t: input tensor to be annotated with partition_spec.
:type t: Union[torch.Tensor, XLAShardedTensor]
:param mesh: describes the logical XLA device topology and the underlying device IDs.
:type mesh: Mesh
:param partition_spec: A tuple of device_mesh dimension index or</p>
<blockquote>
<div><p><cite>None</cite>. Each index is an int, str if the mesh axis is named, or tuple of int or str.
This specifies how each input rank is sharded (index to mesh_shape) or replicated (None).
When a tuple is specified, the corresponding input tensor axis will be sharded along all
logical axes in the tuple. Note that the order the mesh axes are specified in the tuple
will impact the resulting sharding.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>example</strong> (<em>For</em>) – </p></li>
<li><p><strong>row-wise</strong> (<em>we can shard an 8x10 tensor 4-way</em>) – </p></li>
<li><p><strong>column-wise.</strong> (<em>and replicate</em>) – </p></li>
<li><p><strong>torch.randn</strong> (<em>&gt;&gt; input =</em>) – </p></li>
<li><p><strong>=</strong> (<em>&gt;&gt; partition_spec</em>) – </p></li>
<li><p><strong>=</strong> – </p></li>
<li><p><strong>dynamo_custom_op</strong> (<em>bool</em>) – if set to True, it calls the dynamo custom op variant of mark_sharding
to make itself recognizeable and traceable by dynamo.</p></li>
</ul>
</dd>
</dl>
<p>Examples
———————————
mesh_shape = (4, 2)
num_devices = xr.global_runtime_device_count()
device_ids = np.array(range(num_devices))
mesh = Mesh(device_ids, mesh_shape, (‘x’, ‘y’))</p>
<p># 4-way data parallel
input = torch.randn(8, 32).to(xm.xla_device())
xs.mark_sharding(input, mesh, (0, None))</p>
<p># 2-way model parallel
linear = nn.Linear(32, 10).to(xm.xla_device())
xs.mark_sharding(linear.weight, mesh, (None, 1))</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.distributed.spmd.clear_sharding">
<span class="sig-prename descclassname"><span class="pre">torch_xla.distributed.spmd.</span></span><span class="sig-name descname"><span class="pre">clear_sharding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">t</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">XLAShardedTensor</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="_modules/torch_xla/distributed/spmd/xla_sharding.html#clear_sharding"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.distributed.spmd.clear_sharding" title="Permalink to this definition">¶</a></dt>
<dd><p>Clear sharding annotation from the input tensor and return a <cite>cpu</cite> casted tensor.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.distributed.spmd.set_global_mesh">
<span class="sig-prename descclassname"><span class="pre">torch_xla.distributed.spmd.</span></span><span class="sig-name descname"><span class="pre">set_global_mesh</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mesh</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torch_xla.distributed.spmd.Mesh" title="torch_xla.distributed.spmd.xla_sharding.Mesh"><span class="pre">Mesh</span></a></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/distributed/spmd/xla_sharding.html#set_global_mesh"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.distributed.spmd.set_global_mesh" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.distributed.spmd.get_global_mesh">
<span class="sig-prename descclassname"><span class="pre">torch_xla.distributed.spmd.</span></span><span class="sig-name descname"><span class="pre">get_global_mesh</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/distributed/spmd/xla_sharding.html#get_global_mesh"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.distributed.spmd.get_global_mesh" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch_xla.distributed.spmd.Mesh">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch_xla.distributed.spmd.</span></span><span class="sig-name descname"><span class="pre">Mesh</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mesh_shape</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis_names</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/distributed/spmd/xla_sharding.html#Mesh"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.distributed.spmd.Mesh" title="Permalink to this definition">¶</a></dt>
<dd><p>Describe the logical XLA device topology mesh and the underlying resources.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>device_ids</strong> (<em>Union</em><em>[</em><em>np.ndarray</em><em>, </em><em>List</em><em>]</em>) – A raveled list of devices (IDs) in a custom order. The list is reshaped
to an <cite>mesh_shape</cite> array, filling the elements using C-like index order.</p></li>
<li><p><strong>mesh_shape</strong> (<em>Tuple</em><em>[</em><em>python:int</em><em>, </em><em>...</em><em>]</em>) – A int tuple describing the logical topology shape
of the device mesh, and each element describes the number of devices in
the corresponding axis.</p></li>
<li><p><strong>axis_names</strong> (<em>Tuple</em><em>[</em><em>str</em><em>, </em><em>...</em><em>]</em>) – A sequence of resource axis names to be assigned to the dimensions
of the <cite>devices</cite> argument. Its length should match the rank of <cite>devices</cite>.</p></li>
</ul>
</dd>
</dl>
<p>Example:
———————————
mesh_shape = (4, 2)
num_devices = len(xm.get_xla_supported_devices())
device_ids = np.array(range(num_devices))
mesh = Mesh(device_ids, mesh_shape, (‘x’, ‘y’))
mesh.get_logical_mesh()
&gt;&gt; array([[0, 1],</p>
<blockquote>
<div><p>[2, 3],
[4, 5],
[6, 7]])</p>
</div></blockquote>
<p>mesh.shape()
&gt;&gt; OrderedDict([(‘x’, 4), (‘y’, 2)])</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch_xla.distributed.spmd.HybridMesh">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch_xla.distributed.spmd.</span></span><span class="sig-name descname"><span class="pre">HybridMesh</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ici_mesh_shape</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dcn_mesh_shape</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis_names</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/distributed/spmd/xla_sharding.html#HybridMesh"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.distributed.spmd.HybridMesh" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Creates a hybrid device mesh of devices connected with ICI and DCN networks.</dt><dd><p>The shape of logical mesh should be ordered by increasing network-intensity
e.g. [replica, data, model] where mdl has the most network communication
requirements.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ici_mesh_shape</strong> – shape of the logical mesh for inner connected devices.</p></li>
<li><p><strong>dcn_mesh_shape</strong> – shape of logical mesh for outer connected devices.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<p># This example is assuming 2 slices of v4-8.
ici_mesh_shape = (1, 4, 1) # (data, fsdp, tensor)
dcn_mesh_shape = (2, 1, 1)</p>
<p>mesh = HybridMesh(ici_mesh_shape, dcn_mesh_shape, (‘data’,’fsdp’,’tensor’))
print(mesh.shape())
&gt;&gt; OrderedDict([(‘data’, 2), (‘fsdp’, 4), (‘tensor’, 1)])</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch_xla.distributed.spmd.ShardingSpec">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch_xla.distributed.spmd.</span></span><span class="sig-name descname"><span class="pre">ShardingSpec</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mesh</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torch_xla.distributed.spmd.Mesh" title="torch_xla.distributed.spmd.xla_sharding.Mesh"><span class="pre">torch_xla.distributed.spmd.xla_sharding.Mesh</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">partition_spec</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">minibatch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/distributed/spmd/xla_sharding.html#ShardingSpec"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.distributed.spmd.ShardingSpec" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-torch_xla.experimental">
<span id="experimental"></span><h2>experimental<a class="headerlink" href="#module-torch_xla.experimental" title="Permalink to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.experimental.eager_mode">
<span class="sig-prename descclassname"><span class="pre">torch_xla.experimental.</span></span><span class="sig-name descname"><span class="pre">eager_mode</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">enable</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/experimental/eager.html#eager_mode"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.experimental.eager_mode" title="Permalink to this definition">¶</a></dt>
<dd><p>Configure torch_xla’s default executation mode.</p>
<p>Under eager mode only functions that was <a href="#id5"><span class="problematic" id="id6">`</span></a>torch_xla.compile`d will be
traced and compiled. Other torch ops will be executed eagerly.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.experimental.compile">
<span class="sig-prename descclassname"><span class="pre">torch_xla.experimental.</span></span><span class="sig-name descname"><span class="pre">compile</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">func</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/experimental/eager.html#compile"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.experimental.compile" title="Permalink to this definition">¶</a></dt>
<dd><p>Compile the func with Lazy Tensor.</p>
<p>Return the optimized function that takes exact same input. Compile will
run the target func under the tracing mode using Lazy tensor.</p>
</dd></dl>

</div>
<div class="section" id="module-torch_xla.debug.metrics">
<span id="debug"></span><h2>debug<a class="headerlink" href="#module-torch_xla.debug.metrics" title="Permalink to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.debug.metrics.metrics_report">
<span class="sig-prename descclassname"><span class="pre">torch_xla.debug.metrics.</span></span><span class="sig-name descname"><span class="pre">metrics_report</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/debug/metrics.html#metrics_report"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.debug.metrics.metrics_report" title="Permalink to this definition">¶</a></dt>
<dd><p>Retrieves a string containing the full metrics and counters report.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.debug.metrics.short_metrics_report">
<span class="sig-prename descclassname"><span class="pre">torch_xla.debug.metrics.</span></span><span class="sig-name descname"><span class="pre">short_metrics_report</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">counter_names</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">list</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric_names</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">list</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/debug/metrics.html#short_metrics_report"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.debug.metrics.short_metrics_report" title="Permalink to this definition">¶</a></dt>
<dd><p>Retrieves a string containing the full metrics and counters report.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>counter_names</strong> (<em>list</em>) – The list of counter names whose data needs to be printed.</p></li>
<li><p><strong>metric_names</strong> (<em>list</em>) – The list of metric names whose data needs to be printed.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.debug.metrics.counter_names">
<span class="sig-prename descclassname"><span class="pre">torch_xla.debug.metrics.</span></span><span class="sig-name descname"><span class="pre">counter_names</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/debug/metrics.html#counter_names"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.debug.metrics.counter_names" title="Permalink to this definition">¶</a></dt>
<dd><p>Retrieves all the currently active counter names.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.debug.metrics.counter_value">
<span class="sig-prename descclassname"><span class="pre">torch_xla.debug.metrics.</span></span><span class="sig-name descname"><span class="pre">counter_value</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/debug/metrics.html#counter_value"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.debug.metrics.counter_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the value of an active counter.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>name</strong> (<em>string</em>) – The name of the counter whose value needs to be retrieved.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The counter value as integer.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.debug.metrics.metric_names">
<span class="sig-prename descclassname"><span class="pre">torch_xla.debug.metrics.</span></span><span class="sig-name descname"><span class="pre">metric_names</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/debug/metrics.html#metric_names"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.debug.metrics.metric_names" title="Permalink to this definition">¶</a></dt>
<dd><p>Retrieves all the currently active metric names.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch_xla.debug.metrics.metric_data">
<span class="sig-prename descclassname"><span class="pre">torch_xla.debug.metrics.</span></span><span class="sig-name descname"><span class="pre">metric_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/debug/metrics.html#metric_data"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_xla.debug.metrics.metric_data" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the data of an active metric.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>name</strong> (<em>string</em>) – The name of the metric whose data needs to be retrieved.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The metric data, which is a tuple of (TOTAL_SAMPLES, ACCUMULATOR, SAMPLES).
The <cite>TOTAL_SAMPLES</cite> is the total number of samples which have been posted to
the metric. A metric retains only a given number of samples (in a circular
buffer).
The <cite>ACCUMULATOR</cite> is the sum of the samples over <cite>TOTAL_SAMPLES</cite>.
The <cite>SAMPLES</cite> is a list of (TIME, VALUE) tuples.</p>
</dd>
</dl>
</dd></dl>

</div>
</div>
<div class="section" id="beginner-s-guide-to-pytorch-xla">
<h1>Beginner’s Guide to PyTorch/XLA<a class="headerlink" href="#beginner-s-guide-to-pytorch-xla" title="Permalink to this heading">¶</a></h1>
<p>This document provides a high-level overview of PyTorch XLA and illustrates a
few examples how PyTorch code is converted to run on XLA devices (e.g. TPUs).
This is not a complete solution, and additional changes may be required
depending on the specific code. However, this document should serve as a
starting point for the conversion process.</p>
<div class="section" id="basic-high-level-understanding-of-some-xla-details">
<h2>Basic high-level understanding of some XLA details<a class="headerlink" href="#basic-high-level-understanding-of-some-xla-details" title="Permalink to this heading">¶</a></h2>
<dl class="simple">
<dt>This section provides a brief overview of the basic details of PyTorch XLA,</dt><dd><p>which should help readers better understand the required modifications and
optimizations of code. It is supplement to the API guide described <a class="reference external" href="https://github.com/pytorch/xla/blob/master/API_GUIDE.md">here</a>.</p>
</dd>
</dl>
<p>Unlike regular PyTorch, which executes code line by line and does not block execution until the value of a <span class="raw-html-m2r"><ins> PyTorch tensor </ins></span> is fetched, PyTorch XLA works differently. It iterates through the python code and records the operations on <span class="raw-html-m2r"><ins> (PyTorch) XLA tensors </ins></span> in an intermediate representation (IR) graph until it encounters a barrier (discussed below). This process of generating the IR graph is referred to as tracing (LazyTensor tracing or code tracing). PyTorch XLA then converts the IR graph to a lower-level machine-readable format called HLO (High-Level Opcodes). HLO is a representation of a computation that is specific to the XLA compiler and allows it to generate efficient code for the hardware that it is running on. HLO is fed to the XLA compiler for compilation and optimization. Compilation is then cached by PyTorch XLA to be reused later if/when needed. The compilation of the graph is done on the host (CPU), which is the machine that runs the Python code. If there are multiple XLA devices, the host compiles the code for each of the devices separately except when using SPMD (single-program, multiple-data). For example, v4-8 has one host machine and <a class="reference external" href="https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#tpu_v4">four devices</a>. In this case the host compiles the code for each of the four devices separately. In case of pod slices, when there are multiple hosts, each host does the compilation for XLA devices it is attached to. If SPMD is used, then the code is compiled only once (for given shapes and computations) on each host for all the devices.</p>
<a class="reference external image-reference" href="assets/pytorchXLA_flow.svg"><img alt="img" src="assets/pytorchXLA_flow.svg" /></a>
<p>For more details and examples, please refer to the <a class="reference external" href="https://pytorch.org/blog/understanding-lazytensor-system-performance-with-pytorch-xla-on-cloud-tpu/">LazyTensor guide</a>.</p>
<p>The operations in the IR graph are executed only when values of tensors are needed. This is referred to as evaluation or materialization of tensors. Sometimes this is also called lazy evaluation and it can lead to significant <a class="reference external" href="https://arxiv.org/pdf/2102.13267.pdf">performance improvements</a>.</p>
<p>The <em>synchronous</em> operations in Pytorch XLA, like printing, logging, checkpointing or callbacks  block tracing and result in slower execution. In the case when an operation requires a specific value of an XLA tensor, e.g. <code class="docutils literal notranslate"><span class="pre">print(xla_tensor_z)</span></code>, tracing is blocked until the value of that tensor is available to the host. Note that only the part of the graph responsible for computing that tensor value is executed. These operations do not cut the IR graph, but they trigger host-device communication through <code class="docutils literal notranslate"><span class="pre">TransferFromDevice</span></code>, which results in slower performance.</p>
<p>A <em>barrier</em> is a special instruction that tells XLA to execute the IR graph and materialize the tensors. This means that the PyTorch XLA tensors will be evaluated, and the results will be available to the host. The user-exposed barrier in Pytorch XLA is <a class="reference external" href="https://github.com/pytorch/xla/blob/bdceee54eca1269ee954f6cdd1868c584d0e88a4/torch_xla/core/xla_model.py#L808">xm.mark_step()</a>, which  breaks the IR graph and results in code execution on the XLA devices. One of the key properties of <code class="docutils literal notranslate"><span class="pre">xm.mark_step</span></code> is that unlike synchronous operations it does not block the further tracing while the device is executing the graph. However, it does block access to the values of the tensors that are being materialized.</p>
<p>The example in the LazyTensor guide illustrates what happens in a simple case of adding two tensors. Now, suppose we have a for loop that adds XLA tensors and uses the value later:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">tensors_on_device</span><span class="p">:</span>
    <span class="n">z</span> <span class="o">+=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
</pre></div>
</div>
<p>Without a barrier, the Python tracing will result in a single graph that wraps the addition of tensors <code class="docutils literal notranslate"><span class="pre">len(tensors_on_device)</span></code> times. This is because the <code class="docutils literal notranslate"><span class="pre">for</span></code> loop is not captured by the tracing, so each iteration of the loop will create a new  subgraph corresponding to the computation of <code class="docutils literal notranslate"><span class="pre">z</span> <span class="pre">+=</span> <span class="pre">x+y</span></code> and add it to the graph. Here is an example when <code class="docutils literal notranslate"><span class="pre">len(tensors_on_device)=3</span></code>.</p>
<a class="reference external image-reference" href="assets/IRgraph_no_markstep.png"><img alt="img" src="assets/IRgraph_no_markstep.png" /></a>
<p>However, introducing a barrier at the end of the loop will result in a smaller graph that will be compiled once during the first pass inside the <code class="docutils literal notranslate"><span class="pre">for</span></code> loop and will be reused for the next  <code class="docutils literal notranslate"><span class="pre">len(tensors_on_device)-1</span></code> iterations. The barrier will signal to the tracing that the graph traced so far can be submitted for execution, and if that graph has been seen before, a cached compiled program will be reused.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">tensors_on_device</span><span class="p">:</span>
    <span class="n">z</span> <span class="o">+=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
    <span class="n">xm</span><span class="o">.</span><span class="n">mark_step</span><span class="p">()</span>
</pre></div>
</div>
<p>In this case there will be a small graph that is used <code class="docutils literal notranslate"><span class="pre">len(tensors_on_device)=3</span></code> times.</p>
<a class="reference external image-reference" href="assets/IRgraph_markstep.png"><img alt="img" src="assets/IRgraph_markstep.png" /></a>
<p>It is important to highlight that in PyTorch XLA Python code inside for loops is traced and a new graph is constructed for each iteration if there is a barrier at the end. This can be a significant performance bottleneck.</p>
<p>The XLA graphs can be reused when the same computation happens on the same shapes of tensors. If the shapes of the inputs or intermediate tensors change, then the XLA compiler will recompile a new graph with the new tensor shapes. This means that if you have dynamic shapes or if your code does not reuse tensor graphs, running your model on XLA will not be suitable for that use case. Padding the input into a fixed shape can be an option to help avoid dynamic shapes. Otherwise, a significant amount of time will be spent by the compiler on optimizing and fusing operations which will not be used again.</p>
<p>The trade-off between graph size and compilation time is also important to consider. If there is one large IR graph, the XLA compiler can spend a lot of time on optimization and fusion of the ops. This can result in a very long compilation time. However, the later execution may be much faster, due to the optimizations that were performed during compilation.</p>
<p>Sometimes it is worth breaking the IR graph with <code class="docutils literal notranslate"><span class="pre">xm.mark_step()</span></code>. As explained above, this will result in a smaller graph that can be reused later. However making graphs smaller can reduce optimizations that otherwise could be done by the XLA compiler.</p>
<p>Another important point to consider is <a class="reference external" href="https://github.com/pytorch/xla/blob/a1f822e2627a5639464273241821852677401026/torch_xla/distributed/parallel_loader.py#L186">MPDeviceLoader</a>. Once your code is running on an XLA device, consider wrapping the torch dataloader with XLA <code class="docutils literal notranslate"><span class="pre">MPDeviceLoader</span></code> which preloads data to the device to improve performance and includes <code class="docutils literal notranslate"><span class="pre">xm.mark_step()</span></code> in it. The latter automatically breaks the iterations over batches of data and sends them for execution. Note, if you are not using MPDeviceLoader, you might need to set <code class="docutils literal notranslate"><span class="pre">barrier=True</span></code> in the <code class="docutils literal notranslate"><span class="pre">optimizer_step()</span></code> to enable <code class="docutils literal notranslate"><span class="pre">xm.mark_step()</span></code> if running a training job or explicitly adding <code class="docutils literal notranslate"><span class="pre">xm.mark_step()</span></code>.</p>
</div>
<div class="section" id="tpu-setup">
<h2>TPU Setup<a class="headerlink" href="#tpu-setup" title="Permalink to this heading">¶</a></h2>
<p>Create TPU with base image to use nightly wheels or from the stable release by specifying the <code class="docutils literal notranslate"><span class="pre">RUNTIME_VERSION</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>export ZONE=us-central2-b
export PROJECT_ID=your-project-id
export ACCELERATOR_TYPE=v4-8 # v4-16, v4-32, …
export RUNTIME_VERSION=tpu-vm-v4-pt-2.0 # or tpu-vm-v4-base
export TPU_NAME=your_tpu_name

gcloud compute tpus tpu-vm create ${TPU_NAME} \
--zone=${ZONE} \
--accelerator-type=${ACCELERATOR_TYPE} \
--version=${RUNTIME_VERSION} \
--subnetwork=tpusubnet
</pre></div>
</div>
<p>If you have a single host VM (e.g. v4-8), you can ssh to your vm and run the following commands from the vm directly. Otherwise, in case of TPU pods, you can use <code class="docutils literal notranslate"><span class="pre">--worker=all</span> <span class="pre">--command=&quot;&quot;</span></code> similar to</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>gcloud compute tpus tpu-vm ssh ${TPU_NAME} \
--zone=us-central2-b \
--worker=all \
--command=&quot;pip3 install https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch-nightly-cp38-cp38-linux_x86_64.whl&quot;
</pre></div>
</div>
<p>Next, if you are using base image, install nightly packages  and required libraries</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>pip3 install https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch-nightly-cp38-cp38-linux_x86_64.whl
​​pip3 install https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch_xla-nightly-cp38-cp38-linux_x86_64.whl
sudo apt-get install libopenblas-dev -y

sudo apt-get update &amp;&amp; sudo apt-get install libgl1 -y # diffusion specific
</pre></div>
</div>
</div>
<div class="section" id="converting-code-to-pytorch-xla">
<h2>Converting code to PyTorch XLA<a class="headerlink" href="#converting-code-to-pytorch-xla" title="Permalink to this heading">¶</a></h2>
<p>General guidelines to modify your code:</p>
<ul class="simple">
<li><p>Replace <code class="docutils literal notranslate"><span class="pre">cuda</span></code> with <code class="docutils literal notranslate"><span class="pre">xm.xla_device()</span></code></p></li>
<li><p>Remove progress bar, printing that would access the XLA tensor values</p></li>
<li><p>Reduce logging and callbacks that would access the XLA tensor values</p></li>
<li><p>Wrap data loader with MPDeviceLoader</p></li>
<li><p>Profile to further optimize the code</p></li>
</ul>
<p>Remember: each case is unique so you might need to do something different for each case.</p>
</div>
<div class="section" id="example-1-stable-diffusion-inference-in-pytorch-lightning-on-a-single-tpu-device">
<h2>Example 1. Stable Diffusion inference in PyTorch Lightning on a Single TPU Device<a class="headerlink" href="#example-1-stable-diffusion-inference-in-pytorch-lightning-on-a-single-tpu-device" title="Permalink to this heading">¶</a></h2>
<p>As a first example consider the <a class="reference external" href="https://github.com/pytorch-tpu/stable-diffusion/blob/main/scripts/txt2img.py">inference code</a> of the stable diffusion model in PyTorch Lightning which can be run from command line as</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">scripts</span><span class="o">/</span><span class="n">txt2img</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">prompt</span> <span class="s2">&quot;a photograph of an astronaut riding a horse&quot;</span>
</pre></div>
</div>
<p>For your reference, the diff of modifications described below can be found <a class="reference external" href="https://github.com/pytorch-tpu/stable-diffusion/commit/57f398eb784387e244dc5fb78421aa5261abd1ef">here</a>. Let’s go over them step by step.
As in the general guideline above, start with changes related to <code class="docutils literal notranslate"><span class="pre">cuda</span></code> device. This inference code is written to run on GPUs and <code class="docutils literal notranslate"><span class="pre">cuda</span></code> can be found in multiple places. Start making changes by removing  <code class="docutils literal notranslate"><span class="pre">model.cuda()</span></code> from <a class="reference external" href="https://github.com/pytorch-tpu/stable-diffusion/blob/978da4c625a712a01ee066d019a0b0d2319cd8b3/scripts/txt2img.py#L64">this line</a>, and <code class="docutils literal notranslate"><span class="pre">precision_scope</span></code> from <a class="reference external" href="https://github.com/pytorch-tpu/stable-diffusion/blob/978da4c625a712a01ee066d019a0b0d2319cd8b3/scripts/txt2img.py#L290">here</a>. Additionally, replace the <code class="docutils literal notranslate"><span class="pre">cuda</span></code> device in <a class="reference external" href="https://github.com/pytorch-tpu/stable-diffusion/blob/978da4c625a712a01ee066d019a0b0d2319cd8b3/scripts/txt2img.py#L248">this line</a> with the <code class="docutils literal notranslate"><span class="pre">xla</span></code> device similar to the code below:</p>
<p>Next, this particular configuration of the model is using <code class="docutils literal notranslate"><span class="pre">FrozenCLIPEmbedder</span></code>, therefore we will modify this <a class="reference external" href="https://github.com/pytorch-tpu/stable-diffusion/blob/978da4c625a712a01ee066d019a0b0d2319cd8b3/ldm/modules/encoders/modules.py#L143">line</a> as well. For simplicity we will directly define the <code class="docutils literal notranslate"><span class="pre">device</span></code> in this tutorial, but you can pass the <code class="docutils literal notranslate"><span class="pre">device</span></code> value to the function as well.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch_xla.core.xla_model</span> <span class="k">as</span> <span class="nn">xm</span>
<span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">()</span>
</pre></div>
</div>
<p>Another place in the code that has cuda specific code is DDIM scheduler. Add <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">torch_xla.core.xla_model</span> <span class="pre">as</span> <span class="pre">xm</span></code> on top of the file then replace <a class="reference external" href="https://github.com/pytorch-tpu/stable-diffusion/blob/978da4c625a712a01ee066d019a0b0d2319cd8b3/ldm/models/diffusion/ddim.py#L21-L22">these</a> lines</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">attr</span><span class="o">.</span><span class="n">device</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">):</span>
   <span class="n">attr</span> <span class="o">=</span> <span class="n">attr</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">))</span>
</pre></div>
</div>
<p>with</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">device</span> <span class="o">=</span> <span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">()</span>
<span class="n">attr</span> <span class="o">=</span> <span class="n">attr</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
</pre></div>
</div>
<p>Next, you can reduce device (TPU) and host (CPU) communication by removing print statements, disabling progress bars, and reducing or removing callbacks and logging. These operations require the device to stop executing, falling back to the CPU, executing the logging/callbacks, and then returning to the device. This can be a significant performance bottleneck, especially on large models.</p>
<p>After making these changes, the code will run on TPUs. However, the performance will be very slow. This is because the XLA compiler tries to build a single (huge) graph that wraps the number of inference steps (in this case, 50) as there is no barrier inside the for loop. It is difficult for the compiler to optimize the graph, and this leads to significant performance degradation. As discussed above, breaking the for loop with the barrier (xm.mark_step()) will result in a smaller graph that is  easier for the compiler to optimize. This will also allow the compiler to reuse the graph from the previous step, which can improve performance.</p>
<p>Now the <a class="reference external" href="https://github.com/pytorch-tpu/stable-diffusion/blob/ss-inference/scripts/txt2img.py">code</a> is ready to run on TPUs in a reasonable time. More optimization and analysis can be done by <a class="reference external" href="https://cloud.google.com/tpu/docs/pytorch-xla-performance-profiling-tpu-vm">capturing a profile</a> and investigating further. However, this is not covered here.</p>
<p>Note: if you are running on v4-8 TPU, then you have 4 available XLA (TPU) devices. Running the code as above will only use one XLA device. In order to run on all 4 devices you need to use <code class="docutils literal notranslate"><span class="pre">xmp.spawn()</span></code> function to spawn the code on all the devices. We will discuss an <code class="docutils literal notranslate"><span class="pre">xmp.spawn</span></code> in the next example.</p>
</div>
<div class="section" id="example-2-hf-stable-diffusion-inference">
<h2>Example 2. HF Stable Diffusion Inference<a class="headerlink" href="#example-2-hf-stable-diffusion-inference" title="Permalink to this heading">¶</a></h2>
<p>Now, consider using <a class="reference external" href="https://github.com/huggingface/diffusers/tree/main/examples/text_to_image">Stable Diffusion Inference</a> in the HuggingFace diffusers library for both the SD-XL and 2.1 versions of the model. For your reference, the changes described below can be found in this <a class="reference external" href="https://github.com/pytorch-tpu/diffusers">repo</a>. You can clone the repo and run the inference using the following command on your TPU VM:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>(vm)$ git clone https://github.com/pytorch-tpu/diffusers.git
(vm)$ cd diffusers/examples/text_to_image/
(vm)$ python3 inference_tpu_single_device.py
</pre></div>
</div>
<p>Since there is no bf16 version of the SD-XL model available, you can use the <code class="docutils literal notranslate"><span class="pre">XLA_USE_BF16=1</span></code> flag to convert all values to bf16 and speed up training.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>(vm)$ XLA_USE_BF16=1 python3 inference_tpu_single_device.py # uses sd-xl version
</pre></div>
</div>
<p>or</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>(vm)$ python3 inference_tpu_multidevice.py # uses 2.1 version
</pre></div>
</div>
<p>(already includes <code class="docutils literal notranslate"><span class="pre">torch.bfloat16</span></code> in the 2.1 version of the model).</p>
<p>Warning: watch out for caveats highlighted <a class="reference external" href="https://github.com/huggingface/diffusers/pull/4254#issuecomment-1712289803">here</a>.</p>
</div>
<div class="section" id="running-on-a-single-tpu-device">
<h2>Running on a Single TPU device<a class="headerlink" href="#running-on-a-single-tpu-device" title="Permalink to this heading">¶</a></h2>
<p>This section describes the changes that need to be made to the <a class="reference external" href="https://github.com/huggingface/diffusers/tree/main/examples/text_to_image#inference">text_to_image inference example</a> code to run it on TPUs.</p>
<p>The original code uses Lora for inference, but this tutorial will not use it. Instead, we will set the <code class="docutils literal notranslate"><span class="pre">model_id</span></code> argument to <code class="docutils literal notranslate"><span class="pre">stabilityai/stable-diffusion-xl-base-0.9</span></code> when initializing the pipeline. We will also use the default scheduler (DPMSolverMultistepScheduler). However, similar changes can be made to the other schedulers as well.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">git</span> <span class="n">clone</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">huggingface</span><span class="o">/</span><span class="n">diffusers</span>
<span class="n">cd</span> <span class="n">diffusers</span>
<span class="n">pip</span> <span class="n">install</span> <span class="o">.</span> <span class="c1"># pip install -e .</span>

<span class="n">cd</span> <span class="n">examples</span><span class="o">/</span><span class="n">text_to_image</span><span class="o">/</span>
<span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">r</span> <span class="n">requirements</span><span class="o">.</span><span class="n">txt</span>
<span class="n">pip</span> <span class="n">install</span> <span class="n">invisible_watermark</span> <span class="n">transformers</span> <span class="n">accelerate</span> <span class="n">safetensors</span>
</pre></div>
</div>
<p>(If <code class="docutils literal notranslate"><span class="pre">accelerate</span></code> is not found, log out, log back in.)</p>
<p>Log in to HF and agree to the <a class="reference external" href="https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9">sd-xl 0.9 license</a> on the model card. Next, go to  <a class="reference external" href="https://huggingface.co/settings/tokens">account→settings→access</a> token and generate a new token. Copy the token and run the following command with that specific token value on your vm</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>(vm)$ huggingface-cli login --token _your_copied_token__
</pre></div>
</div>
<p>The HuggingFace readme provides PyTorch code that is written to run on GPUs. To run it on TPUs, the first step is to change the CUDA device to an XLA device. This can be done by replacing the line <code class="docutils literal notranslate"><span class="pre">pipe.to(&quot;cuda&quot;)</span></code> with the following lines:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch_xla.core.xla_model</span> <span class="k">as</span> <span class="nn">xm</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">()</span>
<span class="n">pipe</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<p>Additionally, it is important to note that the first time you run inference with XLA, it will take a long time to compile. For example, compilation time for stable diffusion XL model inference from HuggingFace can take about an hour to compile, whereas the actual inference may take only 5 seconds, depending on the batch size. Likewise, a GPT-2 model can take about 10-15 mins to compile, after which the training epoch time becomes much faster. This is because XLA builds a graph of the computation that will be performed, and then optimizes this graph for the specific hardware that it is running on. However, once the graph has been compiled, it can be reused for subsequent inferences, which will be much faster. Therefore, if you are only running inference once, you may not benefit from using XLA. However, if you are running inference multiple times, or if you are running inference on a list of prompts, you will start to see the advantages of XLA after the first few inferences. For example, if you run inference on a list of 10 prompts, the first inference (maybe two<a class="footnote-reference brackets" href="#fn-1" id="id12">1</a>) may take a long time to compile, but the remaining inference steps will be much faster. This is because XLA will reuse the graph that it compiled for the first inference.</p>
<p>If you try to run the code without making any additional changes, you will notice that the compilation time is very long (&gt;6 hours). This is because the XLA compiler tries to build a single graph for all of the scheduler steps at once similar to what we have discussed in the previous example. To make the code run faster, we need to break the graph up into smaller pieces with <code class="docutils literal notranslate"><span class="pre">xm.mark_step()</span></code> and reuse them in the next steps. This happens inside the <code class="docutils literal notranslate"><span class="pre">pipe.__call__</span></code> <a class="reference external" href="https://github.com/huggingface/diffusers/blob/2b1786735e27bc97f4d4699712292d5c463a7380/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py#L559">function</a> in <a class="reference external" href="https://github.com/huggingface/diffusers/blob/2b1786735e27bc97f4d4699712292d5c463a7380/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py#L805-L839">these lines</a>. Disabling the progress bar, removing callbacks and adding <code class="docutils literal notranslate"><span class="pre">xm.mark_step()</span></code> at the end of the for loop speeds up the code significantly. Changes are provided in this <a class="reference external" href="https://github.com/huggingface/diffusers/compare/main...pytorch-tpu:diffusers:main">commit</a>.</p>
<p>Additionally, the <code class="docutils literal notranslate"><span class="pre">self.scheduler.step()</span></code> function, which by default uses the DPMSolverMultistepScheduler scheduler, has a few issues that are described in the
<a class="reference external" href="https://pytorch.org/xla/release/2.0/index.html#known-performance-caveats">PyTorch XLA caveats</a>. The <code class="docutils literal notranslate"><span class="pre">.nonzero()</span></code> and <code class="docutils literal notranslate"><span class="pre">.item()</span></code> calls in this function send requests to the CPU for tensor evaluation, which trigger device-host communication. This is not desirable, as it can slow down the code. In this particular case, we can avoid these calls by passing the index to the function directly. This will prevent the function from sending requests to the CPU, and will improve the performance of the code. Changes are available in <a class="reference external" href="https://github.com/pytorch-tpu/diffusers/commit/0243d2ef9c2c7bc06956bb1bcc92c23038f6519d">this</a> commit. The code now is ready to be run on TPUs.</p>
</div>
<div class="section" id="profiling-and-performance-analysis">
<h2>Profiling and performance analysis<a class="headerlink" href="#profiling-and-performance-analysis" title="Permalink to this heading">¶</a></h2>
<p>To further investigate the performance of the model, we can profile it using the profiling <a class="reference external" href="https://cloud.google.com/tpu/docs/pytorch-xla-performance-profiling-tpu-vm">guide</a>. As a rule of thumb, the profiling script should be run with the maximum batch size that fits into the memory for <a class="reference external" href="https://cloud.google.com/tpu/docs/performance-guide">optimal memory usage</a>. It also helps to overlap tracing of the code with device execution which leads to more optimal device usage. The duration of profiling should be long enough to capture at least one step. Good performance of the model on TPUs means that device-host communication is minimized and the device is constantly running processes with no idle time.</p>
<p>Starting a server in the <code class="docutils literal notranslate"><span class="pre">inference_tpu_*.py</span></code> file and running <code class="docutils literal notranslate"><span class="pre">capture_profile.py</span></code> script as described in the guide will give us information on processes that run on the devices. Currently, only one XLA device is profiled. To better understand the TPU idle time (gaps in the profile), profiling traces (<code class="docutils literal notranslate"><span class="pre">xp.Trace()</span></code>) should be added to the code. The <code class="docutils literal notranslate"><span class="pre">xp.Trace()</span></code> measures the time it takes to trace the python code on the host machine wrapped with the trace. For this example, <code class="docutils literal notranslate"><span class="pre">xp.Trace()</span></code> traces were added inside the <a class="reference external" href="https://github.com/ssusie/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py">pipeline</a> and the <a class="reference external" href="https://github.com/ssusie/diffusers/blob/main/src/diffusers/models/unet_2d_condition.py">U-net model</a> to measure the time to run specific sections of the code on the host (CPU).</p>
<p>If the gaps in the profile are due to Python code tracing that happens on the host, then this might be a bottleneck and there is no further straightforward optimization that can be done. Otherwise, the code should be analyzed further to understand the caveats and improve the performance further. Note that you cannot <code class="docutils literal notranslate"><span class="pre">xp.Trace()</span></code> wrap portions of the code where <code class="docutils literal notranslate"><span class="pre">xm.mark_step()</span></code> is called.</p>
<p>To illustrate this we can look at already captured profiles that were uploaded to tensorboard following the profiling guide.</p>
<p>Starting from Stable Diffusion model version 2.1</p>
<p>If we capture a profile without inserting any traces, we will see the following:</p>
<a class="reference external image-reference" href="assets/image.png"><img alt="Alt text" src="assets/image.png" /></a>
<p>The single TPU device on v4-8, which has two cores, appears to be busy. There are no significant gaps in their usage, except for a small one in the middle. If we scroll up to try to find which process is occupying the host machine, we will not find any information. Therefore, we will add <code class="docutils literal notranslate"><span class="pre">xp.traces</span></code> to the pipeline <a class="reference external" href="https://github.com/pytorch-tpu/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py">file</a> as well as the U-net <a class="reference external" href="https://github.com/pytorch-tpu/diffusers/blob/main/src/diffusers/models/unet_2d_condition.py">function</a>. The latter may not be useful for this particular use case, but it does demonstrate how traces can be added in different places and how their information is displayed in TensorBoard.</p>
<p>If we add traces and re-capture the profile with the largest batch size that can fit on the device (32 in this case), we will see that the gap in the device is caused by a Python process that is running on the host machine.</p>
<a class="reference external image-reference" href="assets/image-1.png"><img alt="Alt text" src="assets/image-1.png" /></a>
<a class="reference external image-reference" href="assets/image-2.png"><img alt="Alt text" src="assets/image-2.png" /></a>
<p>We can use the appropriate tool to zoom in on the timeline and see which process is running during that period. This is when the Python code tracing happens on the host, and we cannot improve the tracing further at this point.</p>
<p>Now, let’s examine the XL version of the model and do the same thing. We will add traces to the pipeline <a class="reference external" href="https://github.com/pytorch-tpu/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py">file</a> in the same way that we did for the 2.1 version and capture a profile.</p>
<a class="reference external image-reference" href="assets/image-4.png"><img alt="Alt text" src="assets/image-4.png" /></a>
<p>This time, in addition to the large gap in the middle, which is caused by the <code class="docutils literal notranslate"><span class="pre">pipe_watermark</span></code> tracing, there are many small gaps between the inference steps within <a class="reference external" href="https://github.com/pytorch-tpu/diffusers/blob/0243d2ef9c2c7bc06956bb1bcc92c23038f6519d/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py#L814-L830">this loop</a>.</p>
<p>First look closer into the large gap that is caused by <code class="docutils literal notranslate"><span class="pre">pipe_watermark</span></code>. The gap is preceded with <code class="docutils literal notranslate"><span class="pre">TransferFromDevice</span></code> which indicates that something is happening on the host machine that is waiting for computation to finish before proceeding. Looking into watermark <a class="reference external" href="https://github.com/pytorch-tpu/diffusers/blob/0243d2ef9c2c7bc06956bb1bcc92c23038f6519d/src/diffusers/pipelines/stable_diffusion_xl/watermark.py#L29">code</a>, we can see that tensors are transferred to cpu and converted to numpy arrays in order to be processed with <code class="docutils literal notranslate"><span class="pre">cv2</span></code> and <code class="docutils literal notranslate"><span class="pre">pywt</span></code> libraries later. Since this part is not straightforward to optimize, we will leave this as is.</p>
<p>Now if we zoom in on the loop, we can see that the graph within the loop is broken into smaller parts because the <code class="docutils literal notranslate"><span class="pre">TransferFromDevice</span></code> operation happens.</p>
<a class="reference external image-reference" href="assets/image-3.png"><img alt="Alt text" src="assets/image-3.png" /></a>
<p>If we investigate the U-Net function and the scheduler, we can see that the U-Net code does not contain any optimization targets for PyTorch/XLA. However, there are <code class="docutils literal notranslate"><span class="pre">.item()</span></code> and <code class="docutils literal notranslate"><span class="pre">.nonzero()</span></code> calls inside the <a class="reference external" href="https://github.com/huggingface/diffusers/blob/15782fd506e8c4a7c2b288fc2e558bd77fdfa51a/src/diffusers/schedulers/scheduling_euler_discrete.py#L371">scheduler.step</a>. We can <a class="reference external" href="https://github.com/pytorch-tpu/diffusers/blob/0243d2ef9c2c7bc06956bb1bcc92c23038f6519d/src/diffusers/schedulers/scheduling_euler_discrete.py#L310">rewrite</a> the function to avoid those calls. If we fix this issue and rerun a profile, we will not see much difference. However, since we have reduced the device-host communication that was introducing smaller graphs, we allowed the compiler to optimize the code better. The function <a class="reference external" href="https://github.com/huggingface/diffusers/blob/15782fd506e8c4a7c2b288fc2e558bd77fdfa51a/src/diffusers/schedulers/scheduling_euler_discrete.py#L205">scale_model_input</a> has similar issues, and  we can fix these by making the changes we made above to the  <code class="docutils literal notranslate"><span class="pre">step</span></code> function. Overall, since many of the gaps are caused from python level code tracing and graph building, these gaps are not possible to optimize with the current version of PyTorch XLA, but we may see improvements in the future when dynamo is enabled in PyTorch XLA.</p>
</div>
<div class="section" id="running-on-multiple-tpu-devices">
<h2>Running on Multiple TPU Devices<a class="headerlink" href="#running-on-multiple-tpu-devices" title="Permalink to this heading">¶</a></h2>
<p>To use multiple TPU devices, you can use the <code class="docutils literal notranslate"><span class="pre">xmp.spawn</span></code> function to spawn the function you ran on a single device to multiple devices. The <code class="docutils literal notranslate"><span class="pre">xmp.spawn</span></code> function will start processes on multiple TPU devices and sync them when needed. This can be done by passing the <code class="docutils literal notranslate"><span class="pre">index</span></code> argument to the function that runs on a single device. For example,</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch_xla.distributed.xla_multiprocessing</span> <span class="k">as</span> <span class="nn">xmp</span>

<span class="k">def</span> <span class="nf">my_function</span><span class="p">(</span><span class="n">index</span><span class="p">):</span>
  <span class="c1"># function that runs on a single device</span>

<span class="n">xmp</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span><span class="n">my_function</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,),</span> <span class="n">nprocs</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
<p>In this example, the <code class="docutils literal notranslate"><span class="pre">my_function</span></code> function will be spawned on 4 TPU devices on v4-8, with each device being assigned an index from 0 to 3.</p>
<p><a class="reference external" href="https://github.com/ssusie/diffusers/blob/main/examples/text_to_image/inference_tpu_multidevice.py">This file</a> illustrates how xmp.spawn can be used to run stable diffusion 2.1 version on multiple TPU devices. For this version similar to the above changes were made to the <a class="reference external" href="https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py">pipeline</a> file.</p>
</div>
<div class="section" id="running-on-pods">
<h2>Running on Pods<a class="headerlink" href="#running-on-pods" title="Permalink to this heading">¶</a></h2>
<p>Once you have the code for running on a single host device, there is no further change needed. You can create the TPU pod, for example, by following these <a class="reference external" href="https://cloud.google.com/tpu/docs/pytorch-pods#create-tpu-vm">instructions</a>. Then run your script with</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>gcloud compute tpus tpu-vm ssh ${TPU_NAME} \
  --zone=${ZONE} \
  --worker=all \
  --command=&quot;python3 your_script.py&quot;
</pre></div>
</div>
<dl class="footnote brackets">
<dt class="label" id="fn-1"><span class="brackets"><a class="fn-backref" href="#id12">1</a></span></dt>
<dd><p>0 and 1 are magic numbers in XLA and treated as constants in the HLO. So if there is a random number generator in the code that can generate these values, the code will compile for each value separately. This can be disabled with <code class="docutils literal notranslate"><span class="pre">XLA_NO_SPECIAL_SCALARS=1</span></code> environment variable.</p>
</dd>
</dl>
</div>
</div>
<div class="section" id="troubleshooting">
<h1>Troubleshooting<a class="headerlink" href="#troubleshooting" title="Permalink to this heading">¶</a></h1>
<p>Note that the information in this section is subject to be removed in future releases of the <em>PyTorch/XLA</em> software,
since many of them are peculiar to a given internal implementation which might change.</p>
<div class="section" id="sanity-check">
<h2>Sanity Check<a class="headerlink" href="#sanity-check" title="Permalink to this heading">¶</a></h2>
<p>Before performing any in depth debugging, we want to do a sanity check on the installed PyTorch/XLA.</p>
<div class="section" id="check-pytorch-xla-version">
<h3>Check PyTorch/XLA Version<a class="headerlink" href="#check-pytorch-xla-version" title="Permalink to this heading">¶</a></h3>
<p>PyTorch and PyTorch/XLA version should match. Check out our <a class="reference external" href="https://github.com/pytorch/xla#getting-started">README</a> for more detials on versions available.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>vm:~$ python
&gt;&gt;&gt; import torch
&gt;&gt;&gt; import torch_xla
&gt;&gt;&gt; print(torch.__version__)
2.1.0+cu121
&gt;&gt;&gt; print(torch_xla.__version__)
2.1.0
</pre></div>
</div>
</div>
<div class="section" id="perform-a-simple-calculation">
<h3>Perform A Simple Calculation<a class="headerlink" href="#perform-a-simple-calculation" title="Permalink to this heading">¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>vm:~$ export PJRT_DEVICE=TPU
vm:~$ python3
&gt;&gt;&gt; import torch
&gt;&gt;&gt; import torch_xla.core.xla_model as xm
&gt;&gt;&gt; t1 = torch.tensor(100, device=xm.xla_device())
&gt;&gt;&gt; t2 = torch.tensor(200, device=xm.xla_device())
&gt;&gt;&gt; print(t1 + t2)
tensor(300, device=&#39;xla:0&#39;)
</pre></div>
</div>
</div>
<div class="section" id="run-resnet-with-fake-data">
<h3>Run Resnet With Fake Data<a class="headerlink" href="#run-resnet-with-fake-data" title="Permalink to this heading">¶</a></h3>
<p>For nightly</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>vm:~$ git clone https://github.com/pytorch/xla.git
vm:~$ python xla/test/test_train_mp_imagenet.py --fake_data
</pre></div>
</div>
<p>For release version <code class="docutils literal notranslate"><span class="pre">x.y</span></code>, you want to use the branch <code class="docutils literal notranslate"><span class="pre">rx.y</span></code>. For example if you installed 2.1 release, you should do</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>vm:~$ git clone --branch r2.1 https://github.com/pytorch/xla.git
vm:~$ python xla/test/test_train_mp_imagenet.py --fake_data
</pre></div>
</div>
<p>If you can get the resnet to run we can conclude that torch_xla is installed correctly.</p>
</div>
</div>
<div class="section" id="performance-debugging">
<h2>Performance Debugging<a class="headerlink" href="#performance-debugging" title="Permalink to this heading">¶</a></h2>
<p>To diagnose performance issues, we can use the execution metrics and counters provided by <em>PyTorch/XLA</em>
The <strong>first thing</strong> to check when model is slow is to generate a metrics report.</p>
<p>Metrics report is extremely helpful in diagnosing issues. Please try to include it in your bug
report sent to us if you have it.</p>
</div>
<div class="section" id="pytorch-xla-debugging-tool">
<h2>PyTorch/XLA Debugging Tool<a class="headerlink" href="#pytorch-xla-debugging-tool" title="Permalink to this heading">¶</a></h2>
<p>You can enable the PyTorch/XLA debugging tool by setting <code class="docutils literal notranslate"><span class="pre">PT_XLA_DEBUG_LEVEL=2</span></code>, which provides a couple useful debugging features. You can also lower the debug level to <code class="docutils literal notranslate"><span class="pre">1</span></code> to slip the execution analysis.</p>
<div class="section" id="perform-a-auto-metrics-analysis">
<h3>Perform A Auto-Metrics Analysis<a class="headerlink" href="#perform-a-auto-metrics-analysis" title="Permalink to this heading">¶</a></h3>
<p>The debugging tool will analyze the metrics report and provide a summary. Some example output would be</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pt</span><span class="o">-</span><span class="n">xla</span><span class="o">-</span><span class="n">profiler</span><span class="p">:</span> <span class="n">CompileTime</span> <span class="n">too</span> <span class="n">frequent</span><span class="p">:</span> <span class="mi">21</span> <span class="n">counts</span> <span class="n">during</span> <span class="mi">11</span> <span class="n">steps</span>
<span class="n">pt</span><span class="o">-</span><span class="n">xla</span><span class="o">-</span><span class="n">profiler</span><span class="p">:</span> <span class="n">TransferFromDeviceTime</span> <span class="n">too</span> <span class="n">frequent</span><span class="p">:</span> <span class="mi">11</span> <span class="n">counts</span> <span class="n">during</span> <span class="mi">11</span> <span class="n">steps</span>
<span class="n">pt</span><span class="o">-</span><span class="n">xla</span><span class="o">-</span><span class="n">profiler</span><span class="p">:</span> <span class="n">Op</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="ow">not</span> <span class="n">lowered</span><span class="p">:</span> <span class="n">aten</span><span class="p">::</span><span class="n">_ctc_loss</span><span class="p">,</span> <span class="n">aten</span><span class="p">::</span><span class="n">_ctc_loss_backward</span><span class="p">,</span>  <span class="n">Please</span> <span class="nb">open</span> <span class="n">a</span> <span class="n">GitHub</span> <span class="n">issue</span> <span class="k">with</span> <span class="n">the</span> <span class="n">above</span> <span class="n">op</span> <span class="n">lowering</span> <span class="n">requests</span><span class="o">.</span>
<span class="n">pt</span><span class="o">-</span><span class="n">xla</span><span class="o">-</span><span class="n">profiler</span><span class="p">:</span> <span class="n">CompileTime</span> <span class="n">too</span> <span class="n">frequent</span><span class="p">:</span> <span class="mi">23</span> <span class="n">counts</span> <span class="n">during</span> <span class="mi">12</span> <span class="n">steps</span>
<span class="n">pt</span><span class="o">-</span><span class="n">xla</span><span class="o">-</span><span class="n">profiler</span><span class="p">:</span> <span class="n">TransferFromDeviceTime</span> <span class="n">too</span> <span class="n">frequent</span><span class="p">:</span> <span class="mi">12</span> <span class="n">counts</span> <span class="n">during</span> <span class="mi">12</span> <span class="n">steps</span>
</pre></div>
</div>
</div>
<div class="section" id="compilation-execution-analysis">
<h3>Compilation &amp; Execution Analysis<a class="headerlink" href="#compilation-execution-analysis" title="Permalink to this heading">¶</a></h3>
<p>The debugging tool will analyze every compilation and execution for your model. Some example output would be</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Compilation</span> <span class="n">Analysis</span><span class="p">:</span> <span class="o">================================================================================</span>
<span class="n">Compilation</span> <span class="n">Analysis</span><span class="p">:</span> <span class="n">Compilation</span> <span class="n">Cause</span>
<span class="n">Compilation</span> <span class="n">Analysis</span><span class="p">:</span>   <span class="n">mark_step</span> <span class="ow">in</span> <span class="n">parallel</span> <span class="n">loader</span> <span class="n">at</span> <span class="n">step</span> <span class="n">end</span>
<span class="n">Compilation</span> <span class="n">Analysis</span><span class="p">:</span> <span class="n">Graph</span> <span class="n">Info</span><span class="p">:</span>
<span class="n">Compilation</span> <span class="n">Analysis</span><span class="p">:</span>   <span class="n">Graph</span> <span class="n">Hash</span><span class="p">:</span> <span class="n">c74c3b91b855b2b123f833b0d5f86943</span>
<span class="n">Compilation</span> <span class="n">Analysis</span><span class="p">:</span>   <span class="n">Number</span> <span class="n">of</span> <span class="n">Graph</span> <span class="n">Inputs</span><span class="p">:</span> <span class="mi">35</span>
<span class="n">Compilation</span> <span class="n">Analysis</span><span class="p">:</span>   <span class="n">Number</span> <span class="n">of</span> <span class="n">Graph</span> <span class="n">Outputs</span><span class="p">:</span> <span class="mi">107</span>
<span class="n">Compilation</span> <span class="n">Analysis</span><span class="p">:</span> <span class="n">Python</span> <span class="n">Frame</span> <span class="n">Triggered</span> <span class="n">Execution</span><span class="p">:</span>
<span class="n">Compilation</span> <span class="n">Analysis</span><span class="p">:</span>   <span class="n">mark_step</span> <span class="p">(</span><span class="o">/</span><span class="n">workspaces</span><span class="o">/</span><span class="n">dk3</span><span class="o">/</span><span class="n">pytorch</span><span class="o">/</span><span class="n">xla</span><span class="o">/</span><span class="n">torch_xla</span><span class="o">/</span><span class="n">core</span><span class="o">/</span><span class="n">xla_model</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">1055</span><span class="p">)</span>
<span class="n">Compilation</span> <span class="n">Analysis</span><span class="p">:</span>   <span class="nb">next</span> <span class="p">(</span><span class="o">/</span><span class="n">workspaces</span><span class="o">/</span><span class="n">dk3</span><span class="o">/</span><span class="n">pytorch</span><span class="o">/</span><span class="n">xla</span><span class="o">/</span><span class="n">torch_xla</span><span class="o">/</span><span class="n">distributed</span><span class="o">/</span><span class="n">parallel_loader</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">44</span><span class="p">)</span>
<span class="n">Compilation</span> <span class="n">Analysis</span><span class="p">:</span>   <span class="fm">__next__</span> <span class="p">(</span><span class="o">/</span><span class="n">workspaces</span><span class="o">/</span><span class="n">dk3</span><span class="o">/</span><span class="n">pytorch</span><span class="o">/</span><span class="n">xla</span><span class="o">/</span><span class="n">torch_xla</span><span class="o">/</span><span class="n">distributed</span><span class="o">/</span><span class="n">parallel_loader</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">32</span><span class="p">)</span>
<span class="n">Compilation</span> <span class="n">Analysis</span><span class="p">:</span>   <span class="n">train_loop_fn</span> <span class="p">(</span><span class="o">/</span><span class="n">workspaces</span><span class="o">/</span><span class="n">dk3</span><span class="o">/</span><span class="n">pytorch</span><span class="o">/</span><span class="n">xla</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">train_decoder_only_base</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">48</span><span class="p">)</span>
<span class="n">Compilation</span> <span class="n">Analysis</span><span class="p">:</span>   <span class="n">start_training</span> <span class="p">(</span><span class="o">/</span><span class="n">workspaces</span><span class="o">/</span><span class="n">dk3</span><span class="o">/</span><span class="n">pytorch</span><span class="o">/</span><span class="n">xla</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">train_decoder_only_base</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">65</span><span class="p">)</span>
<span class="n">Compilation</span> <span class="n">Analysis</span><span class="p">:</span>   <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span> <span class="p">(</span><span class="o">/</span><span class="n">workspaces</span><span class="o">/</span><span class="n">dk3</span><span class="o">/</span><span class="n">pytorch</span><span class="o">/</span><span class="n">xla</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">train_decoder_only_base</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">73</span><span class="p">)</span>
<span class="n">Compilation</span> <span class="n">Analysis</span><span class="p">:</span> <span class="o">--------------------------------------------------------------------------------</span>
<span class="n">Compilation</span> <span class="n">Analysis</span><span class="p">:</span> <span class="o">================================================================================</span>

<span class="n">Post</span> <span class="n">Compilation</span> <span class="n">Analysis</span><span class="p">:</span> <span class="o">================================================================================</span>
<span class="n">Post</span> <span class="n">Compilation</span> <span class="n">Analysis</span><span class="p">:</span> <span class="n">Graph</span> <span class="nb">input</span> <span class="n">size</span><span class="p">:</span> <span class="mf">1.548000</span> <span class="n">GB</span>
<span class="n">Post</span> <span class="n">Compilation</span> <span class="n">Analysis</span><span class="p">:</span> <span class="n">Graph</span> <span class="n">output</span> <span class="n">size</span><span class="p">:</span> <span class="mf">7.922460</span> <span class="n">GB</span>
<span class="n">Post</span> <span class="n">Compilation</span> <span class="n">Analysis</span><span class="p">:</span> <span class="n">Aliased</span> <span class="n">Input</span> <span class="n">size</span><span class="p">:</span> <span class="mf">1.547871</span> <span class="n">GB</span>
<span class="n">Post</span> <span class="n">Compilation</span> <span class="n">Analysis</span><span class="p">:</span> <span class="n">Intermediate</span> <span class="n">tensor</span> <span class="n">size</span><span class="p">:</span> <span class="mf">12.124478</span> <span class="n">GB</span>
<span class="n">Post</span> <span class="n">Compilation</span> <span class="n">Analysis</span><span class="p">:</span> <span class="n">Compiled</span> <span class="n">program</span> <span class="n">size</span><span class="p">:</span> <span class="mf">0.028210</span> <span class="n">GB</span>
<span class="n">Post</span> <span class="n">Compilation</span> <span class="n">Analysis</span><span class="p">:</span> <span class="o">--------------------------------------------------------------------------------</span>
<span class="n">Post</span> <span class="n">Compilation</span> <span class="n">Analysis</span><span class="p">:</span> <span class="o">================================================================================</span>

<span class="n">Execution</span> <span class="n">Analysis</span><span class="p">:</span> <span class="o">================================================================================</span>
<span class="n">Execution</span> <span class="n">Analysis</span><span class="p">:</span> <span class="n">Execution</span> <span class="n">Cause</span>
<span class="n">Execution</span> <span class="n">Analysis</span><span class="p">:</span>   <span class="n">mark_step</span> <span class="ow">in</span> <span class="n">parallel</span> <span class="n">loader</span> <span class="n">at</span> <span class="n">step</span> <span class="n">end</span>
<span class="n">Execution</span> <span class="n">Analysis</span><span class="p">:</span> <span class="n">Graph</span> <span class="n">Info</span><span class="p">:</span>
<span class="n">Execution</span> <span class="n">Analysis</span><span class="p">:</span>   <span class="n">Graph</span> <span class="n">Hash</span><span class="p">:</span> <span class="n">c74c3b91b855b2b123f833b0d5f86943</span>
<span class="n">Execution</span> <span class="n">Analysis</span><span class="p">:</span>   <span class="n">Number</span> <span class="n">of</span> <span class="n">Graph</span> <span class="n">Inputs</span><span class="p">:</span> <span class="mi">35</span>
<span class="n">Execution</span> <span class="n">Analysis</span><span class="p">:</span>   <span class="n">Number</span> <span class="n">of</span> <span class="n">Graph</span> <span class="n">Outputs</span><span class="p">:</span> <span class="mi">107</span>
<span class="n">Execution</span> <span class="n">Analysis</span><span class="p">:</span> <span class="n">Python</span> <span class="n">Frame</span> <span class="n">Triggered</span> <span class="n">Execution</span><span class="p">:</span>
<span class="n">Execution</span> <span class="n">Analysis</span><span class="p">:</span>   <span class="n">mark_step</span> <span class="p">(</span><span class="o">/</span><span class="n">workspaces</span><span class="o">/</span><span class="n">dk3</span><span class="o">/</span><span class="n">pytorch</span><span class="o">/</span><span class="n">xla</span><span class="o">/</span><span class="n">torch_xla</span><span class="o">/</span><span class="n">core</span><span class="o">/</span><span class="n">xla_model</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">1055</span><span class="p">)</span>
<span class="n">Execution</span> <span class="n">Analysis</span><span class="p">:</span>   <span class="nb">next</span> <span class="p">(</span><span class="o">/</span><span class="n">workspaces</span><span class="o">/</span><span class="n">dk3</span><span class="o">/</span><span class="n">pytorch</span><span class="o">/</span><span class="n">xla</span><span class="o">/</span><span class="n">torch_xla</span><span class="o">/</span><span class="n">distributed</span><span class="o">/</span><span class="n">parallel_loader</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">44</span><span class="p">)</span>
<span class="n">Execution</span> <span class="n">Analysis</span><span class="p">:</span>   <span class="fm">__next__</span> <span class="p">(</span><span class="o">/</span><span class="n">workspaces</span><span class="o">/</span><span class="n">dk3</span><span class="o">/</span><span class="n">pytorch</span><span class="o">/</span><span class="n">xla</span><span class="o">/</span><span class="n">torch_xla</span><span class="o">/</span><span class="n">distributed</span><span class="o">/</span><span class="n">parallel_loader</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">32</span><span class="p">)</span>
<span class="n">Execution</span> <span class="n">Analysis</span><span class="p">:</span>   <span class="n">train_loop_fn</span> <span class="p">(</span><span class="o">/</span><span class="n">workspaces</span><span class="o">/</span><span class="n">dk3</span><span class="o">/</span><span class="n">pytorch</span><span class="o">/</span><span class="n">xla</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">train_decoder_only_base</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">48</span><span class="p">)</span>
<span class="n">Execution</span> <span class="n">Analysis</span><span class="p">:</span>   <span class="n">start_training</span> <span class="p">(</span><span class="o">/</span><span class="n">workspaces</span><span class="o">/</span><span class="n">dk3</span><span class="o">/</span><span class="n">pytorch</span><span class="o">/</span><span class="n">xla</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">train_decoder_only_base</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">65</span><span class="p">)</span>
<span class="n">Execution</span> <span class="n">Analysis</span><span class="p">:</span>   <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span> <span class="p">(</span><span class="o">/</span><span class="n">workspaces</span><span class="o">/</span><span class="n">dk3</span><span class="o">/</span><span class="n">pytorch</span><span class="o">/</span><span class="n">xla</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">train_decoder_only_base</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">73</span><span class="p">)</span>
<span class="n">Execution</span> <span class="n">Analysis</span><span class="p">:</span> <span class="o">--------------------------------------------------------------------------------</span>
<span class="n">Execution</span> <span class="n">Analysis</span><span class="p">:</span> <span class="o">================================================================================</span>
</pre></div>
</div>
<p>Some common causes of Compilation/Executation are</p>
<ol class="arabic simple">
<li><p>User manually call <code class="docutils literal notranslate"><span class="pre">mark_step</span></code>.</p></li>
<li><p><a class="reference external" href="https://github.com/pytorch/xla/blob/fe4af0080af07f78ca2b614dd91b71885a3bbbb8/torch_xla/distributed/parallel_loader.py#L49-L51">Parallel loader</a> call <code class="docutils literal notranslate"><span class="pre">mark_step</span></code> for every x (configurable) batch.</p></li>
<li><p>Exiting a <a class="reference external" href="https://github.com/pytorch/xla/blob/fe4af0080af07f78ca2b614dd91b71885a3bbbb8/torch_xla/debug/profiler.py#L165-L171">profiler StepTrace region</a>.</p></li>
<li><p>Dynamo decide to compile/execute the graph.</p></li>
<li><p>User trying to access(often due to logging) the value of a tensor before the <code class="docutils literal notranslate"><span class="pre">mark_step</span></code>.</p></li>
</ol>
<p>The executation caused by 1-4 are expected, and we want to avoid 5 by either reduce the frequency of accessing tensor values or manually add a <code class="docutils literal notranslate"><span class="pre">mark_step</span></code> before accessing.</p>
<p>Users should expect to see this <code class="docutils literal notranslate"><span class="pre">Compilation</span> <span class="pre">Cause</span></code> + <code class="docutils literal notranslate"><span class="pre">Executation</span> <span class="pre">Cause</span></code> pairs for first couple steps. After the model stabilize users should expect to only see <code class="docutils literal notranslate"><span class="pre">Execution</span> <span class="pre">Cause</span></code>(you can disable execution analysis by <code class="docutils literal notranslate"><span class="pre">PT_XLA_DEBUG_LEVEL=1</span></code>). To use PyTorch/XLA efficiently, we expect the same models code to be run for every step and compilation only happen once for every graph. If you keep seeing <code class="docutils literal notranslate"><span class="pre">Compilation</span> <span class="pre">Cause</span></code>, you should try to dump the IR/HLO following <a class="reference external" href="#common-debugging-environment-variables-combinations">this section</a> and compare the graphs for each step and understand the source of the differences.</p>
<p>Following section will explain how to get and understand a more detail metrics report.</p>
</div>
</div>
<div class="section" id="get-a-metrics-report">
<h2>Get A Metrics Report<a class="headerlink" href="#get-a-metrics-report" title="Permalink to this heading">¶</a></h2>
<p>Put the following line in your program to generate a report:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch_xla.debug.metrics</span> <span class="k">as</span> <span class="nn">met</span>

<span class="c1"># For short report that only contains a few key metrics.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">met</span><span class="o">.</span><span class="n">short_metrics_report</span><span class="p">())</span>
<span class="c1"># For full report that includes all metrics.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">met</span><span class="o">.</span><span class="n">metrics_report</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="section" id="understand-the-metrics-report">
<h2>Understand The Metrics Report<a class="headerlink" href="#understand-the-metrics-report" title="Permalink to this heading">¶</a></h2>
<p>The report includes things like:</p>
<ul class="simple">
<li><p>how many time we issue <em>XLA</em> compilations and time spent on issuing.</p></li>
<li><p>how many times we execute and time spent on execution</p></li>
<li><p>how many device data handles we create/destroy etc.</p></li>
</ul>
<p>This information is reported in terms of percentiles of the samples. An example is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Metric</span><span class="p">:</span> <span class="n">CompileTime</span>
  <span class="n">TotalSamples</span><span class="p">:</span> <span class="mi">202</span>
  <span class="n">Counter</span><span class="p">:</span> <span class="mi">06</span><span class="n">m09s401ms746</span><span class="mf">.001</span><span class="n">us</span>
  <span class="n">ValueRate</span><span class="p">:</span> <span class="mi">778</span><span class="n">ms572</span><span class="mf">.062</span><span class="n">us</span> <span class="o">/</span> <span class="n">second</span>
  <span class="n">Rate</span><span class="p">:</span> <span class="mf">0.425201</span> <span class="o">/</span> <span class="n">second</span>
  <span class="n">Percentiles</span><span class="p">:</span> <span class="mi">1</span><span class="o">%=</span><span class="mi">001</span><span class="n">ms32</span><span class="mf">.778</span><span class="n">us</span><span class="p">;</span> <span class="mi">5</span><span class="o">%=</span><span class="mi">001</span><span class="n">ms61</span><span class="mf">.283</span><span class="n">us</span><span class="p">;</span> <span class="mi">10</span><span class="o">%=</span><span class="mi">001</span><span class="n">ms79</span><span class="mf">.236</span><span class="n">us</span><span class="p">;</span> <span class="mi">20</span><span class="o">%=</span><span class="mi">001</span><span class="n">ms110</span><span class="mf">.973</span><span class="n">us</span><span class="p">;</span> <span class="mi">50</span><span class="o">%=</span><span class="mi">001</span><span class="n">ms228</span><span class="mf">.773</span><span class="n">us</span><span class="p">;</span> <span class="mi">80</span><span class="o">%=</span><span class="mi">001</span><span class="n">ms339</span><span class="mf">.183</span><span class="n">us</span><span class="p">;</span> <span class="mi">90</span><span class="o">%=</span><span class="mi">001</span><span class="n">ms434</span><span class="mf">.305</span><span class="n">us</span><span class="p">;</span> <span class="mi">95</span><span class="o">%=</span><span class="mi">002</span><span class="n">ms921</span><span class="mf">.063</span><span class="n">us</span><span class="p">;</span> <span class="mi">99</span><span class="o">%=</span><span class="mi">21</span><span class="n">s102ms853</span><span class="mf">.173</span><span class="n">us</span>
</pre></div>
</div>
<p>We also provide counters, which are named integer variables which track internal software status. For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Counter</span><span class="p">:</span> <span class="n">CachedSyncTensors</span>
  <span class="n">Value</span><span class="p">:</span> <span class="mi">395</span>
</pre></div>
</div>
<p>In this report, any counter that starts with <code class="docutils literal notranslate"><span class="pre">aten::</span></code>
indicates a context switch between the XLA device and CPU, which can be a
potential performance optimization area in the model code.</p>
<p>Counters are useful to understand which operations are routed back to the CPU engine of <em>PyTorch</em>.
They are fully qualified with their C++ namespace:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Counter</span><span class="p">:</span> <span class="n">aten</span><span class="p">::</span><span class="n">nonzero</span>
  <span class="n">Value</span><span class="p">:</span> <span class="mi">33</span>
</pre></div>
</div>
<p>If you see <code class="docutils literal notranslate"><span class="pre">aten::</span></code> ops other than <code class="docutils literal notranslate"><span class="pre">nonzero</span></code> and <code class="docutils literal notranslate"><span class="pre">_local_scalar_dense</span></code>, that usually means a missing
lowering in PyTorch/XLA. Feel free to open a feature request for it on <a class="reference external" href="https://github.com/pytorch/xla/issues">GitHub issues</a>.</p>
</div>
<div class="section" id="clear-the-metrics-report">
<h2>Clear The Metrics Report<a class="headerlink" href="#clear-the-metrics-report" title="Permalink to this heading">¶</a></h2>
<p>If you want to clear the metrics between steps/epochs, you can use</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch_xla.debug.metrics</span> <span class="k">as</span> <span class="nn">met</span>

<span class="n">met</span><span class="o">.</span><span class="n">clear_all</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="pytorch-xla-dynamo-debugging-tool">
<h2>PyTorch/XLA + Dynamo Debugging Tool<a class="headerlink" href="#pytorch-xla-dynamo-debugging-tool" title="Permalink to this heading">¶</a></h2>
<p>You can enable the PyTorch/XLA + Dynamo debugging tool by setting <code class="docutils literal notranslate"><span class="pre">XLA_DYNAMO_DEBUG=1</span></code>.</p>
</div>
<div class="section" id="performance-profiling">
<h2>Performance Profiling<a class="headerlink" href="#performance-profiling" title="Permalink to this heading">¶</a></h2>
<p>To profile your workload in depth to understand bottlenecks please check the following resources:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://cloud.google.com/tpu/docs/pytorch-xla-performance-profiling-tpu-vm">Official tutorial</a></p></li>
<li><p><a class="reference external" href="https://colab.research.google.com/github/pytorch/xla/blob/master/contrib/colab/pytorch-xla-profiling-colab.ipynb">Colab notebook</a></p></li>
<li><p><a class="reference external" href="https://github.com/pytorch/xla/blob/master/test/test_profile_mp_mnist.py">Sample MNIST training script with profiling</a></p></li>
<li><p><a class="reference external" href="https://github.com/pytorch/xla/blob/master/scripts/capture_profile.py">Utility script for capturing performance profiles</a></p></li>
</ul>
</div>
<div class="section" id="simple-benchmarking">
<h2>Simple Benchmarking<a class="headerlink" href="#simple-benchmarking" title="Permalink to this heading">¶</a></h2>
<p>Take a look at <cite>``examples/train_resnet_benchmark.py`</cite> &lt;<a class="reference external" href="https://github.com/pytorch/xla/blob/master/examples/train_resnet_benchmark.py">https://github.com/pytorch/xla/blob/master/examples/train_resnet_benchmark.py</a>&gt;`_ for how to benchmark a PyTorch/XLA model.</p>
</div>
<div class="section" id="known-performance-caveats">
<h2>Known Performance Caveats<a class="headerlink" href="#known-performance-caveats" title="Permalink to this heading">¶</a></h2>
<p>PyTorch/XLA behaves semantically like regular PyTorch and XLA tensors share the full tensor interface with CPU &amp; GPU tensors.
However, constraints in XLA/hardware and the lazy evaluation model suggest certain patterns might result in bad performance.</p>
<p>If your model shows bad performance, keep in mind the following caveats:</p>
<ol class="arabic">
<li><p><strong>XLA/TPU yield degraded performance with too many recompilations.</strong></p>
<p>XLA compilation is expensive. PyTorch/XLA automatically recompiles the graph every time new shapes are encountered.
Usually models should stabilize within a few steps and you can see huge speedup for the rest of training.</p>
<p>In order to avoid recompilations, not only must shapes be constant, but computations across XLA devices in all hosts should also be constant.</p>
<p><em>Possible sources</em>:</p>
<ul class="simple">
<li><p>Direct or indirect uses of <code class="docutils literal notranslate"><span class="pre">nonzero</span></code> introduce dynamic shapes; for example, masked indexing <code class="docutils literal notranslate"><span class="pre">base[index]</span></code> where <code class="docutils literal notranslate"><span class="pre">index</span></code> is a mask tensor.</p></li>
<li><p>Loops with a different number of iterations between steps can result in different execution graphs, thus require recompilations.</p></li>
</ul>
<p><em>Solution</em>:</p>
<ul class="simple">
<li><p>Tensor shapes should be the same between iterations, or a low number of shape variations should be used.</p></li>
<li><p>Pad tensors to fixed sizes when possible.</p></li>
</ul>
</li>
<li><p><strong>Certain operations don’t have native translations to XLA.</strong></p>
<p>For these operations PyTorch/XLA automatically transfers to the CPU memory, evaluates on CPU, and transfers the result back to the XLA device.
Doing too many such operations during the training step can lead to significant slowdowns.</p>
<p><em>Possible sources</em>:</p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">item()</span></code> operation explicitly asks to evaluate the result. Don’t use it unless it’s necessary.</p></li>
</ul>
<p><em>Solution</em>:</p>
<ul>
<li><p>For most ops we can lower them to XLA to fix it. Checkout <a class="reference external" href="#metrics-report">metrics report section</a> to find out the missing ops and open a feature request on <a class="reference external" href="https://github.com/pytorch/xla/issues">GitHub</a>.</p></li>
<li><p>Even when a PyTorch tensor is known as a scalar, avoid using <code class="docutils literal notranslate"><span class="pre">tensor.item()</span></code>. Keep it as a tensor and use tensor operations on it.</p></li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">torch.where</span></code> to substitute control flow when applicable.
E.g. The control flow with <code class="docutils literal notranslate"><span class="pre">item()</span></code> used in <a class="reference external" href="https://github.com/pytorch/pytorch/blob/de19eeee99a2a282fc441f637b23d8e50c75ecd1/torch/nn/utils/clip_grad.py#L33">clip_grad*norm*</a> is problematic and impacts performance, so we have <a class="reference external" href="https://github.com/pytorch/xla/blob/master/torch_patches/X10-clip_grad.diff">patched</a> <code class="docutils literal notranslate"><span class="pre">clip_grad_norm_</span></code> by calling <code class="docutils literal notranslate"><span class="pre">torch.where</span></code> instead, which gives us a dramatic performance improvement.
.. code-block:: python</p>
<blockquote>
<div><p>…
else:</p>
<blockquote>
<div><p>device = parameters[0].device
total_norm = torch.zeros([], device=device if parameters else None)
for p in parameters:</p>
<blockquote>
<div><p>param_norm = p.grad.data.norm(norm_type) ** norm_type
total_norm.add_(param_norm)</p>
</div></blockquote>
<p>total_norm = (total_norm ** (1. / norm_type))</p>
</div></blockquote>
<p>clip_coef = torch.tensor(max_norm, device=device) / (total_norm + 1e-6)
for p in parameters:</p>
<blockquote>
<div><p>p.grad.data.mul_(torch.where(clip_coef &lt; 1, clip_coef, torch.tensor(1., device=device)))</p>
</div></blockquote>
</div></blockquote>
</li>
</ul>
</li>
<li><p><strong>Iterators in ``torch_xla.distributed.data_parallel`` may drop the last few batches in the input iterator.</strong></p>
<p>This is to make sure we do the same amount of work on all XLA devices.</p>
<p><em>Solution</em>:</p>
<ul class="simple">
<li><p>When dataset is small, and there are too few steps, this may result in a no-op epoch. Therefore, it is better to use
small batch sizes in those cases.</p></li>
</ul>
</li>
</ol>
</div>
<div class="section" id="xla-tensor-quirks">
<h2>XLA Tensor Quirks<a class="headerlink" href="#xla-tensor-quirks" title="Permalink to this heading">¶</a></h2>
<ol class="arabic simple">
<li><p><strong>XLA tensor internals are opaque.</strong> XLA tensors always appear to be
contiguous and without storage. Networks should not try to check the strides
of XLA tensors.</p></li>
<li><p><strong>XLA tensors should be moved to the CPU before saving them.</strong> Saving
XLA tensors directly causes them to be loaded back on the device(s) they were
saved from. If a device is unavailable at load time then the load will fail.
Moving XLA tensors to the CPU before saving them lets you decide which
device(s) to put the loaded tensors on. This is necessary if you want to
load the tensors on a machine without XLA devices. Care should be taken
moving the XLA tensors to the CPU before saving them, however, as moving
tensors across device types does not preserve view relationships. Instead,
views should be reconstructed as necessary after the tensors are loaded.</p></li>
<li><p><strong>Copying an XLA Tensor with Python’s copy.copy returns a deep copy, not a
shallow copy.</strong> Use a view of an XLA tensor to get a shallow copy of it.</p></li>
<li><p><strong>Handling shared weights.</strong> Modules can share weights by setting the
Parameters of one module to another. This “tying” of module weights should
be done <strong>AFTER</strong> the modules are moved to an XLA device. Otherwise two
independent copies of the shared tensor will be made on the XLA device.</p></li>
</ol>
</div>
<div class="section" id="more-debugging-tools">
<h2>More Debugging Tools<a class="headerlink" href="#more-debugging-tools" title="Permalink to this heading">¶</a></h2>
<p>We don’t expect users to use tools in this section to debug their models. But we might ask for
them when you submit a bug report since they provide additional information that metrics report
doesn’t have.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">print(torch_xla._XLAC._get_xla_tensors_text([res]))</span></code> where <code class="docutils literal notranslate"><span class="pre">res</span></code> is the result tensor prints out the IR.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">print(torch_xla._XLAC._get_xla_tensors_hlo([res]))</span></code> where <code class="docutils literal notranslate"><span class="pre">res</span></code> is the result tensor prints out the generated XLA HLO.</p></li>
</ul>
<p>Note these functions must be called prior to <code class="docutils literal notranslate"><span class="pre">mark_step()</span></code>, otherwise the tensor will already be materialized.</p>
<div class="section" id="environment-variables">
<h3>Environment Variables<a class="headerlink" href="#environment-variables" title="Permalink to this heading">¶</a></h3>
<p>There are also a number of environment variables which control the behavior of the <em>PyTorch/XLA</em>
software stack.</p>
<p>Setting such variables will cause different degrees of performance degradation, so they should
only be enabled for debugging.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">XLA_IR_DEBUG</span></code>: Enables the <em>Python</em> stack trace to be captured where creating IR nodes,
hence allowing to understand which <em>PyTorch</em> operation was responsible for generating the IR.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">XLA_HLO_DEBUG</span></code>: Enables the <em>Python</em> stack frame captured when _XLA_IR<em>DEBUG</em> is active,
to be propagated to the <em>XLA</em> <em>HLO</em> metadata.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">XLA_SAVE_TENSORS_FILE</span></code>: The path to a file which will be used to dump the IR graphs during
execution. Note that the file can become really big if the option is left enabled and the
<em>PyTorch</em> program let run for long time. The graphs are appended to the file, so to have a clean
sheet from run to run, the file should be explicitly removed.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">XLA_SAVE_TENSORS_FMT</span></code>: The format of the graphs stored within the _XLA_SAVE_TENSORS<em>FILE</em>
file. Can be <code class="docutils literal notranslate"><span class="pre">text</span></code> (the default), <code class="docutils literal notranslate"><span class="pre">dot</span></code> (the <em>Graphviz</em> format) or <code class="docutils literal notranslate"><span class="pre">hlo</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">XLA_FLAGS=--xla_dump_to</span></code>: If set to <code class="docutils literal notranslate"><span class="pre">=/tmp/dir_name</span></code>, XLA compiler will dump the unoptimized and optimzed HLO per compilation.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">XLA_METRICS_FILE</span></code>: If set, the path to a local file where the internal metrics will be
saved at every step. Metrics will be appended to the file, if already existing.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">XLA_SAVE_HLO_FILE</span></code>: If set, the path to a local file where, in case of compilation/execution
error, the offending HLO graph will be saved.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">XLA_SYNC_WAIT</span></code>: Forces the XLA tensor sync operation to wait for its completion, before
moving to the next step.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">XLA_USE_EAGER_DEBUG_MODE</span></code>: Forces the XLA tensor to execute eagerly, meaning compile and execute the torch operations one
by one. This is useful to bypass the long compilation time but overall step time will be a lot slower and memory usage will be higher
since all compiler optimizaiton will be skipped.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">TF_CPP_LOG_THREAD_ID</span></code>: If set to 1, the TF logs will show the thread ID
helping with debugging multithreaded processes.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">TF_CPP_VMODULE</span></code>: Environment variable used for TF VLOGs and takes the
form of <code class="docutils literal notranslate"><span class="pre">TF_CPP_VMODULE=name=value,...</span></code>. Note that for VLOGs you must set
<code class="docutils literal notranslate"><span class="pre">TF_CPP_MIN_LOG_LEVEL=0</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">TF_CPP_MIN_LOG_LEVEL</span></code>: Level to print messages for. <code class="docutils literal notranslate"><span class="pre">TF_CPP_MIN_LOG_LEVEL=0</span></code> will turn
on INFO logging, <code class="docutils literal notranslate"><span class="pre">TF_CPP_MIN_LOG_LEVEL=1</span></code> WARNING and so on. Our PyTorch/XLA <code class="docutils literal notranslate"><span class="pre">TF_VLOG</span></code> uses
<code class="docutils literal notranslate"><span class="pre">tensorflow::INFO</span></code> level by default so to see VLOGs set <code class="docutils literal notranslate"><span class="pre">TF_CPP_MIN_LOG_LEVEL=0</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">XLA_DUMP_HLO_GRAPH</span></code>: If set to <code class="docutils literal notranslate"><span class="pre">=1</span></code> in case of a compilation or execution error the
offending HLO graph will be dumped as part of the runtime error raised by <code class="docutils literal notranslate"><span class="pre">xla_util.cc</span></code>.</p></li>
</ul>
</div>
<div class="section" id="common-debugging-environment-variables-combinations">
<h3>Common Debugging Environment Variables Combinations<a class="headerlink" href="#common-debugging-environment-variables-combinations" title="Permalink to this heading">¶</a></h3>
<ul>
<li><p>Record the graph execution in the IR format</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">XLA_IR_DEBUG</span><span class="o">=</span><span class="mi">1</span> <span class="n">XLA_HLO_DEBUG</span><span class="o">=</span><span class="mi">1</span> <span class="n">XLA_SAVE_TENSORS_FMT</span><span class="o">=</span><span class="s2">&quot;text&quot;</span> <span class="n">XLA_SAVE_TENSORS_FILE</span><span class="o">=</span><span class="s2">&quot;/tmp/save1.ir&quot;</span>
</pre></div>
</div>
</li>
<li><p>Record the graph execution in the HLO format</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">XLA_IR_DEBUG</span><span class="o">=</span><span class="mi">1</span> <span class="n">XLA_HLO_DEBUG</span><span class="o">=</span><span class="mi">1</span> <span class="n">XLA_SAVE_TENSORS_FMT</span><span class="o">=</span><span class="s2">&quot;hlo&quot;</span> <span class="n">XLA_SAVE_TENSORS_FILE</span><span class="o">=</span><span class="s2">&quot;/tmp/save1.hlo&quot;</span>
</pre></div>
</div>
</li>
<li><p>Show debugging VLOG for runtime and graph compilation/execution</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">TF_CPP_MIN_LOG_LEVEL</span><span class="o">=</span><span class="mi">0</span> <span class="n">TF_CPP_VMODULE</span><span class="o">=</span><span class="s2">&quot;xla_graph_executor=5,pjrt_computation_client=3&quot;</span>
</pre></div>
</div>
</li>
</ul>
</div>
<div class="section" id="reproducing-pytorch-xla-ci-cd-unit-test-failures">
<h3>Reproducing PyTorch/XLA CI/CD unit test failures.<a class="headerlink" href="#reproducing-pytorch-xla-ci-cd-unit-test-failures" title="Permalink to this heading">¶</a></h3>
<p>You may see some test failures for a PR such as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">To</span> <span class="n">execute</span> <span class="n">this</span> <span class="n">test</span><span class="p">,</span> <span class="n">run</span> <span class="n">the</span> <span class="n">following</span> <span class="kn">from</span> <span class="nn">the</span> <span class="n">base</span> <span class="n">repo</span> <span class="nb">dir</span><span class="p">:</span>
    <span class="n">PYTORCH_TEST_WITH_SLOW</span><span class="o">=</span><span class="mi">1</span> <span class="n">python</span> <span class="o">../</span><span class="n">test</span><span class="o">/</span><span class="n">test_torch</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">k</span> <span class="n">test_put_xla_uint8</span>
</pre></div>
</div>
<p>Running this directly in the command line does not work. You need to set the environment variable <code class="docutils literal notranslate"><span class="pre">TORCH_TEST_DEVICES</span></code> to your local <code class="docutils literal notranslate"><span class="pre">pytorch/xla/test/pytorch_test_base.py</span></code>. For example:</p>
<p><code class="docutils literal notranslate"><span class="pre">TORCH_TEST_DEVICES=/path/to/pytorch/xla/test/pytorch_test_base.py</span> <span class="pre">PYTORCH_TEST_WITH_SLOW=1</span> <span class="pre">python</span> <span class="pre">../test/test_torch.py</span> <span class="pre">-k</span> <span class="pre">test_put_xla_uint8</span></code> should work.</p>
</div>
</div>
</div>
<div class="section" id="pjrt-runtime">
<h1>PJRT Runtime<a class="headerlink" href="#pjrt-runtime" title="Permalink to this heading">¶</a></h1>
<p>PyTorch/XLA has migrated from the TensorFlow-based XRT runtime to the <a class="reference external" href="https://github.com/openxla/xla/tree/main/xla/pjrt">PJRT
runtime</a>
used by <a class="reference external" href="https://github.com/google/jax">JAX</a>.</p>
<p>If you encounter a bug with PJRT, please file an issue on GitHub with the
<code class="docutils literal notranslate"><span class="pre">runtime</span></code> tag.</p>
<p><em>New features in PyTorch/XLA r2.1</em>:</p>
<ul class="simple">
<li><p>PJRT is stable in PyTorch/XLA r2.1!</p></li>
<li><p>Public runtime APIs have moved from <code class="docutils literal notranslate"><span class="pre">torch_xla.experimental.pjrt</span></code> to
<code class="docutils literal notranslate"><span class="pre">torch_xla.runtime</span></code>.</p>
<ul>
<li><p>The <code class="docutils literal notranslate"><span class="pre">pjrt://</span></code> init method has been renamed to <code class="docutils literal notranslate"><span class="pre">xla://</span></code>, and it is registered
by <code class="docutils literal notranslate"><span class="pre">torch_xla.distributed.xla_backend</span></code>.</p></li>
<li><p>The previous <code class="docutils literal notranslate"><span class="pre">torch_xla.experimental.*</span></code> names are still available in this
release for compatibility.</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">torchrun</span></code> is now supported when using <code class="docutils literal notranslate"><span class="pre">init_method='xla://'</span></code>.</p></li>
<li><p>New plugins for XPU and Neuron via the PJRT C API.</p></li>
</ul>
<p><em>New features in PyTorch/XLA r2.0</em>:</p>
<ul class="simple">
<li><p>PJRT will be configured by default if you don’t pass in any other runtime
configuration. If you continue to set XRT configuration (<code class="docutils literal notranslate"><span class="pre">XRT_TPU_CONFIG</span></code>),
this change has no impact</p></li>
<li><p>New TPU runtime implementation in <code class="docutils literal notranslate"><span class="pre">libtpu</span></code> improves performance by up to 30%.</p></li>
<li><p>New <code class="docutils literal notranslate"><span class="pre">xm.rendezvous</span></code> implementation that scales to thousands of TPU cores</p></li>
<li><p>[experimental] <code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code> support for TPU v2 and v3, including
<code class="docutils literal notranslate"><span class="pre">pjrt://</span></code> <code class="docutils literal notranslate"><span class="pre">init_method</span></code></p></li>
</ul>
<div class="section" id="tl-dr">
<h2>TL;DR<a class="headerlink" href="#tl-dr" title="Permalink to this heading">¶</a></h2>
<ul class="simple">
<li><p>To use the PJRT preview runtime, set the <code class="docutils literal notranslate"><span class="pre">PJRT_DEVICE</span></code> environment variable to
<code class="docutils literal notranslate"><span class="pre">CPU</span></code>, <code class="docutils literal notranslate"><span class="pre">TPU</span></code>, or <code class="docutils literal notranslate"><span class="pre">CUDA</span></code></p></li>
<li><p>In XRT, all distributed workloads are multiprocess, with one process per
device. On TPU v2 and v3 in PJRT, workloads are multiprocess and multithreaded
(4 processes with 2 threads each), so your workload should be thread-safe. See
<a class="reference external" href="#multithreading-on-tpu-v2v3">Multithreading on TPU v2/v3</a> and the
<a class="reference external" href="https://github.com/pytorch/xla/blob/master/API_GUIDE.md#running-on-multiple-xla-devices-with-multi-processing">Multiprocessing section of the API
guide</a>
for more information. Key differences to keep in mind:</p>
<ul>
<li><p>To initialize a model in a thread-safe way, either broadcast the parameters
across replicas after initialization
(<code class="docutils literal notranslate"><span class="pre">torch_xla.experimental.pjrt.broadcast_master_param</span></code>) or load each
replica’s parameters from a common checkpoint.</p></li>
<li><p>For other random number generation, use <code class="docutils literal notranslate"><span class="pre">torch.Generator</span></code> where possible.
The global <code class="docutils literal notranslate"><span class="pre">torch</span></code> RNG is <em>not</em> thread-safe, even if you set the same
<code class="docutils literal notranslate"><span class="pre">torch.manual_seed</span></code> across replicas.</p></li>
<li><p>To use <code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code>, import <code class="docutils literal notranslate"><span class="pre">torch_xla.experimental.pjrt_backend</span></code> and
use the <code class="docutils literal notranslate"><span class="pre">xla://</span></code> <code class="docutils literal notranslate"><span class="pre">init_method</span></code>.</p></li>
<li><p>These steps are optional for GPU and TPU v4.</p></li>
</ul>
</li>
</ul>
<p>Sample diff from XRT to PJRT:</p>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="w"> </span>import os

<span class="w"> </span>import torch
<span class="w"> </span>import torch.nn as nn
<span class="w"> </span>from torch.nn.parallel import DistributedDataParallel as DDP
<span class="w"> </span>import torch.optim as optim
<span class="w"> </span>import torch.distributed as dist
<span class="w"> </span>import torch_xla.core.xla_model as xm
<span class="w"> </span>import torch_xla.distributed.parallel_loader as pl
<span class="w"> </span>import torch_xla.distributed.xla_backend
<span class="w"> </span>import torch_xla.distributed.xla_multiprocessing as xmp
<span class="gi">+import torch_xla.runtime as xr</span>


<span class="w"> </span>def _mp_fn(index):
<span class="w"> </span>  device = xm.xla_device()
<span class="gd">-  dist.init_process_group(&#39;xla&#39;, rank=xm.get_ordinal(), world_size=xm.xrt_world_size())</span>
<span class="gi">+  dist.init_process_group(&#39;xla&#39;, init_method=&#39;xla://&#39;)</span>

<span class="w"> </span>  torch.manual_seed(42)
<span class="w"> </span>  model = nn.Linear(128, 10).to(device)

<span class="gi">+  # Optional for TPU v4 and GPU</span>
<span class="gi">+  xm.broadcast_master_param(model)</span>
<span class="w"> </span>  model = DDP(model, gradient_as_bucket_view=True)

<span class="w"> </span>  loss_fn = nn.MSELoss()
<span class="w"> </span>  optimizer = optim.SGD(model.parameters(), lr=.001)

<span class="w"> </span>  for i in range(10):
<span class="w"> </span>    data, target = torch.randn((128, 128), device=device), torch.randn((128, 10), device=device)

<span class="w"> </span>    optimizer.zero_grad()
<span class="w"> </span>    output = model(data)
<span class="w"> </span>    loss = loss_fn(output, target)
<span class="w"> </span>    loss.backward()

<span class="w"> </span>    optimizer.step()
<span class="w"> </span>    xm.mark_step()

<span class="w"> </span>  # Print mean parameters so we can confirm they&#39;re the same across replicas
<span class="w"> </span>  print([p.mean() for p in model.parameters()])

<span class="w"> </span>if __name__ == &#39;__main__&#39;:
<span class="gd">-  os.environ[&#39;XRT_TPU_CONFIG&#39;] = &#39;localservice;0;localhost:51011&#39;</span>
<span class="gd">-  os.environ[&#39;MASTER_ADDR&#39;] = &#39;localhost&#39;</span>
<span class="gd">-  os.environ[&#39;MASTER_PORT&#39;] = &#39;12355&#39;</span>

<span class="gi">+  # Recommended: set PJRT_DEVICE to your local device type</span>
<span class="gi">+  os.environ[&#39;PJRT_DEVICE&#39;] = &#39;TPU&#39;</span>

<span class="w"> </span>  xmp.spawn(_mp_fn)
</pre></div>
</div>
</div>
<div class="section" id="benefits">
<h2>Benefits<a class="headerlink" href="#benefits" title="Permalink to this heading">¶</a></h2>
<ul class="simple">
<li><p>Simple runtime configuration: just set <code class="docutils literal notranslate"><span class="pre">PJRT_DEVICE</span></code> to <code class="docutils literal notranslate"><span class="pre">TPU</span></code>, <code class="docutils literal notranslate"><span class="pre">CPU</span></code>, or <code class="docutils literal notranslate"><span class="pre">CUDA</span></code>
and start using XLA! Or, let PJRT select a device automatically based on your
environment.</p></li>
<li><p>Improved performance: reduced overhead from gRPC means faster end-to-end
execution. On TorchBench 2.0, we observed a &gt;35% improvement in training time
on TPU v4.</p></li>
<li><p>Easy pod execution: just copy your code to each TPU worker, and execute them
all at the same time with <code class="docutils literal notranslate"><span class="pre">gcloud</span> <span class="pre">compute</span> <span class="pre">tpus</span> <span class="pre">tpuvm</span> <span class="pre">ssh</span> <span class="pre">--worker=all</span></code>.</p></li>
<li><p>Better scaling: removes <a class="reference external" href="https://github.com/pytorch/xla/pull/3920">XRT’s limitation on parameter
sizes</a> and supports up to 2048 TPU
chips.</p></li>
</ul>
</div>
<div class="section" id="quickstart">
<h2>Quickstart<a class="headerlink" href="#quickstart" title="Permalink to this heading">¶</a></h2>
<p>To start using PJRT with PyTorch/XLA, all you need to do is set the
<code class="docutils literal notranslate"><span class="pre">PJRT_DEVICE</span></code> environment variable. If you’re working on a TPU v2 or v3, keep
reading to learn about the differences between TPU v2 and v3 and v4.</p>
<div class="section" id="cpu">
<h3>CPU<a class="headerlink" href="#cpu" title="Permalink to this heading">¶</a></h3>
<p>On any machine with PyTorch/XLA installed, you can run our MNIST example on CPU
like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">PJRT_DEVICE</span><span class="o">=</span><span class="n">CPU</span> <span class="n">python3</span> <span class="n">xla</span><span class="o">/</span><span class="n">test</span><span class="o">/</span><span class="n">test_train_mp_mnist</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">fake_data</span>
</pre></div>
</div>
</div>
<div class="section" id="tpu">
<h3>TPU<a class="headerlink" href="#tpu" title="Permalink to this heading">¶</a></h3>
<p>To create a new TPU with PyTorch/XLA r2.0 installed:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>gcloud alpha compute tpus tpu-vm create $USER-pjrt --accelerator-type=v4-8 --version=tpu-vm-v4-pt-2.0 --zone=us-central2-b --project=$PROJECT
</pre></div>
</div>
<p>On a v4-8, you can run our ResNet50 example like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">git</span> <span class="n">clone</span> <span class="o">--</span><span class="n">depth</span><span class="o">=</span><span class="mi">1</span> <span class="o">--</span><span class="n">branch</span> <span class="n">r2</span><span class="mf">.0</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">pytorch</span><span class="o">/</span><span class="n">xla</span><span class="o">.</span><span class="n">git</span>
<span class="n">PJRT_DEVICE</span><span class="o">=</span><span class="n">TPU</span> <span class="n">python3</span> <span class="n">xla</span><span class="o">/</span><span class="n">test</span><span class="o">/</span><span class="n">test_train_mp_imagenet</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">fake_data</span> <span class="o">--</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span> <span class="o">--</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span>
</pre></div>
</div>
<p>By default, PJRT will use all TPU chips. To use only one TPU chip, configure
<code class="docutils literal notranslate"><span class="pre">TPU_PROCESS_BOUNDS</span></code> and <code class="docutils literal notranslate"><span class="pre">TPU_VISIBLE_CHIPS</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">TPU_PROCESS_BOUNDS</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span> <span class="n">TPU_VISIBLE_CHIPS</span><span class="o">=</span><span class="mi">0</span> <span class="n">PJRT_DEVICE</span><span class="o">=</span><span class="n">TPU</span> <span class="n">python3</span> <span class="n">xla</span><span class="o">/</span><span class="n">test</span><span class="o">/</span><span class="n">test_train_mp_imagenet</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">fake_data</span> <span class="o">--</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span> <span class="o">--</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span>
</pre></div>
</div>
<div class="section" id="pods">
<h4>Pods<a class="headerlink" href="#pods" title="Permalink to this heading">¶</a></h4>
<p>On TPU Pods, use <code class="docutils literal notranslate"><span class="pre">gcloud</span></code> to run your command on each TPU in parallel:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>gcloud alpha compute tpus tpu-vm ssh $USER-pjrt --zone=us-central2-b --project=$PROJECT --worker=all --command=&quot;git clone --depth=1 --branch r1.13 https://github.com/pytorch/xla.git&quot;
gcloud alpha compute tpus tpu-vm ssh $USER-pjrt --zone=us-central2-b --project=$PROJECT --worker=all --command=&quot;PJRT_DEVICE=TPU python3 xla/test/test_train_mp_imagenet.py --fake_data --batch_size=256 --num_epochs=1&quot;
</pre></div>
</div>
</div>
<div class="section" id="docker">
<h4>Docker<a class="headerlink" href="#docker" title="Permalink to this heading">¶</a></h4>
<p>You can also use Docker to run your workload in a container with PyTorch/XLA
preinstalled:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>export DOCKER_IMAGE=gcr.io/...

# Optional: authenticate docker if your image is in a private GCP repository
gcloud compute tpus tpu-vm ssh $USER-pjrt --zone=us-central2-b --project=$PROJECT --worker=all --command &quot;sudo gcloud auth configure-docker&quot;

# Run your workload
gcloud compute tpus tpu-vm ssh $USER-pjrt --zone=us-central2-b --project=$PROJECT --worker=all --command &quot;sudo docker run --rm --privileged --net=host -e PJRT_DEVICE=TPU $DOCKER_IMAGE python pytorch/xla/test/test_train_mp_imagenet.py --fake_data&quot;
</pre></div>
</div>
<p>Note that <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">run</span></code> requires privileged access to the host (<code class="docutils literal notranslate"><span class="pre">--privileged</span></code>)
to expose the TPU device to the container. Docker on TPU pods is only supported
with host networking <code class="docutils literal notranslate"><span class="pre">--net=host</span></code> at this time. See the <a class="reference external" href="https://cloud.google.com/tpu/docs/run-in-container">Cloud TPU documentation</a>
for more information.</p>
</div>
</div>
<div class="section" id="gpu">
<h3>GPU<a class="headerlink" href="#gpu" title="Permalink to this heading">¶</a></h3>
</div>
<div class="section" id="single-node-gpu-training">
<h3>Single-node GPU training<a class="headerlink" href="#single-node-gpu-training" title="Permalink to this heading">¶</a></h3>
<p>To use GPUs with PJRT, simply set <code class="docutils literal notranslate"><span class="pre">PJRT_DEVICE=CUDA</span></code> and configure
<code class="docutils literal notranslate"><span class="pre">GPU_NUM_DEVICES</span></code> to the number of devices on the host. For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">PJRT_DEVICE</span><span class="o">=</span><span class="n">CUDA</span> <span class="n">GPU_NUM_DEVICES</span><span class="o">=</span><span class="mi">4</span> <span class="n">python3</span> <span class="n">xla</span><span class="o">/</span><span class="n">test</span><span class="o">/</span><span class="n">test_train_mp_imagenet</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">fake_data</span> <span class="o">--</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span> <span class="o">--</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span>
</pre></div>
</div>
<p>You can also use <code class="docutils literal notranslate"><span class="pre">torchrun</span></code> to initiate the single-node multi-GPU training. For example,</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>PJRT_DEVICE=CUDA torchrun --nnodes 1 --nproc-per-node ${NUM_GPU_DEVICES} xla/test/test_train_mp_imagenet.py --fake_data --pjrt_distributed --batch_size=128 --num_epochs=1
</pre></div>
</div>
<p>In the above example, <code class="docutils literal notranslate"><span class="pre">--nnodes</span></code> means how many machines (physical machines or VMs) to be used (it is 1 since we do single-node training). <code class="docutils literal notranslate"><span class="pre">--nproc-per-node</span></code> means how many GPU devices to be used.</p>
</div>
<div class="section" id="multi-node-gpu-training">
<h3>Multi-node GPU training<a class="headerlink" href="#multi-node-gpu-training" title="Permalink to this heading">¶</a></h3>
<p><strong>Note that this feature only works for cuda 12+</strong>. Similar to how PyTorch uses multi-node training, you can run the command as below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>PJRT_DEVICE=CUDA torchrun \
--nnodes=${NUMBER_GPU_VM} \
--node_rank=${CURRENT_NODE_RANK} \
--nproc_per_node=${NUMBER_LOCAL_GPU_DEVICES} \
--rdzv_endpoint=&lt;internal_ip_address:port&gt; multinode_training.py
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--nnodes</span></code>: how many GPU machines to be used.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--node_rank</span></code>: the index of the current GPU machines. The value can be 0, 1, …, ${NUMBER_GPU_VM}-1.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--nproc_per_node</span></code>: the number of GPU devices to be used on the current machine.</p></li>
<li><p><cite>–rdzv_endpoint</cite>: the endpoint of the GPU machine with node_rank==0, in the form <cite>host:port`</cite>. The``host<code class="docutils literal notranslate"><span class="pre">will</span> <span class="pre">be</span> <span class="pre">the</span> <span class="pre">internal</span> <span class="pre">IP</span> <span class="pre">address.</span> <span class="pre">The</span></code>port` can be any available port on the machine. For single-node training/inference, this parameter can be omitted.</p></li>
</ul>
<p>For example, if you want to train on 2 GPU machines: machine_0 and machine_1, on the first GPU machine machine_0, run</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># PJRT_DEVICE=CUDA torchrun \</span>
<span class="o">--</span><span class="n">nnodes</span><span class="o">=</span><span class="mi">2</span> \
<span class="o">--</span><span class="n">node_rank</span><span class="o">=</span><span class="mi">0</span> \
<span class="o">--</span><span class="n">nproc_per_node</span><span class="o">=</span><span class="mi">4</span> \
<span class="o">--</span><span class="n">rdzv_endpoint</span><span class="o">=</span><span class="s2">&quot;&lt;MACHINE_0_INTERNAL_IP_ADDRESS&gt;:12355&quot;</span> <span class="n">pytorch</span><span class="o">/</span><span class="n">xla</span><span class="o">/</span><span class="n">test</span><span class="o">/</span><span class="n">test_train_mp_imagenet</span><span class="o">.</span><span class="n">py</span>  <span class="o">--</span><span class="n">fake_data</span> <span class="o">--</span><span class="n">pjrt_distributed</span> <span class="o">--</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span> <span class="o">--</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span>
</pre></div>
</div>
<p>On the second GPU machine, run</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># PJRT_DEVICE=CUDA torchrun \</span>
<span class="o">--</span><span class="n">nnodes</span><span class="o">=</span><span class="mi">2</span> \
<span class="o">--</span><span class="n">node_rank</span><span class="o">=</span><span class="mi">1</span> \
<span class="o">--</span><span class="n">nproc_per_node</span><span class="o">=</span><span class="mi">4</span> \
<span class="o">--</span><span class="n">rdzv_endpoint</span><span class="o">=</span><span class="s2">&quot;&lt;MACHINE_0_INTERNAL_IP_ADDRESS&gt;:12355&quot;</span> <span class="n">pytorch</span><span class="o">/</span><span class="n">xla</span><span class="o">/</span><span class="n">test</span><span class="o">/</span><span class="n">test_train_mp_imagenet</span><span class="o">.</span><span class="n">py</span>  <span class="o">--</span><span class="n">fake_data</span> <span class="o">--</span><span class="n">pjrt_distributed</span> <span class="o">--</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span> <span class="o">--</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span>
</pre></div>
</div>
<p>the difference between the 2 commands above are <code class="docutils literal notranslate"><span class="pre">--node_rank</span></code> and potentially <code class="docutils literal notranslate"><span class="pre">--nproc_per_node</span></code> if you want to use different number of GPU devices on each machine. All the rest are identical. For more information about <code class="docutils literal notranslate"><span class="pre">torchrun</span></code>, please refer to this <a class="reference external" href="https://pytorch.org/docs/stable/elastic/run.html">page</a>.</p>
</div>
</div>
<div class="section" id="differences-from-xrt">
<h2>Differences from XRT<a class="headerlink" href="#differences-from-xrt" title="Permalink to this heading">¶</a></h2>
<p>Although in most cases we expect PJRT and XRT to work mostly interchangeably
from the end-user’s perspective (especially on TPU v4), there are some subtle
differences that are important to keep in mind. Importantly, XRT was designed
around the TPU Node architecture, so it will always spawn a client and a server
process, even on TPU VMs. Thus, every batch of inputs has additional latency
from serializing and deserializing data to send it over the network.</p>
<p>PJRT uses the local device directly with no intermediate server process. In the
default configuration, PJRT will create one process per TPU chip, or 4 processes
per TPU host. See the <a class="reference external" href="https://cloud.google.com/tpu/docs/system-architecture-tpu-vm">Cloud TPU
documentation</a> for
more information about TPU architecture.</p>
<ul class="simple">
<li><p>Performance gains are possible for workloads constrained overhead from .</p></li>
<li><p>Under XRT, the server process is the only process that interacts with the TPU
devices, and client processes don’t have direct access to the TPU devices.
When profiling a single-host TPU (e.g. v3-8 or v4-8), you would normally see 8
device traces (one for each TPU core). With PJRT, each process has one chip,
and a profile from that process will show only 2 TPU cores.</p>
<ul>
<li><p>For the same reason, profiling does not work on TPU Pods with XRT, because
the server process runs independently from the user’s model code. PJRT does
not have that constraint, so it is possible to profile 2 TPU cores per
process in a TPU Pod.</p></li>
</ul>
</li>
<li><p>PJRT only supports the TPU VM architecture and we have no plans to support the
TPU Node architecture with PJRT.</p></li>
<li><p>Runtime configuration is significantly simpler with PJRT. <code class="docutils literal notranslate"><span class="pre">xla_dist</span></code> is not
required to run TPU Pod workloads. Instead, copy your code to each TPU host
(<code class="docutils literal notranslate"><span class="pre">[gcloud</span> <span class="pre">compute</span> <span class="pre">tpus</span> <span class="pre">tpu-vm</span>
<span class="pre">scp](https://cloud.google.com/sdk/gcloud/reference/alpha/compute/tpus/tpu-vm/scp)</span></code>)
and run the code on each host in parallel (e.g. <code class="docutils literal notranslate"><span class="pre">[gcloud</span> <span class="pre">compute</span> <span class="pre">tpus</span> <span class="pre">tpu-vm</span>
<span class="pre">ssh</span> <span class="pre">--workers=all</span> <span class="pre">--command=&quot;PJRT_DEVICE=TPU</span> <span class="pre">python</span>
<span class="pre">run.py&quot;](https://cloud.google.com/sdk/gcloud/reference/alpha/compute/tpus/tpu-vm/ssh)</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">xm.rendezvous</span></code> has been reimplemented using XLA-native collective
communication to enhance stability on large TPU pods. See below for more
details.</p></li>
</ul>
<div class="section" id="id20">
<h3>Multithreading on TPU v2/v3<a class="headerlink" href="#id20" title="Permalink to this heading">¶</a></h3>
<p>On TPU v2 and v3, <strong>distributed workloads always run multithreaded</strong>, since each
TPU core exposes two TPU cores as devices and only one process may open a TPU
chip at a time. In its default configuration, <code class="docutils literal notranslate"><span class="pre">xmp.spawn</span></code> automatically spawns
as many processes as possible (4 per TPU host) and creates two threads per
process (one per TPU core).</p>
<p>Note: on TPU v4, each TPU chip is represented as one PyTorch device, so
distributed workloads will run across 4 processes, each with only one thread.
This is identical to XRT’s behavior.</p>
<p>In most cases, this will not require substantial changes to your existing code.
The main change you will have to make in most cases is to model initialization.
Because <code class="docutils literal notranslate"><span class="pre">torch</span></code>‘s global RNG is shared between threads, results will vary
between threads and runs even if you set <code class="docutils literal notranslate"><span class="pre">torch.manual_seed</span></code> to the same value
in every replica. To get consistent parameters between replicas, either use
<code class="docutils literal notranslate"><span class="pre">torch_xla.experimental.pjrt.broadcast_master_param</span></code> to broadcast one replica’s
parameters to all other replicas, or load each replica’s parameters from a
common checkpoint.</p>
</div>
<div class="section" id="changes-to-xm-rendezvous">
<h3>Changes to xm.rendezvous<a class="headerlink" href="#changes-to-xm-rendezvous" title="Permalink to this heading">¶</a></h3>
<p><em>New in PyTorch/XLA r2.0</em></p>
<p>With XRT, worker 0 runs a mesh master service, and all processes on all workers
connect to that service over gRPC. In practice, we found that running a single
mesh master process was unreliable on TPU pods with thousands of chips due to
the number of inbound connections to worker 0. A single client process timing
out could cause a failure and force the entire workload to restart.</p>
<p>Thus, we have reimplemented <code class="docutils literal notranslate"><span class="pre">xm.rendezvous</span></code> with native XLA collective
communication, which is much more stable and well-tested on large TPU pods. This
imposes two new constraints compared to the XRT implementation:</p>
<ul class="simple">
<li><p>Because the payload has to become part of the XLA graph, <code class="docutils literal notranslate"><span class="pre">xm.mark_step</span></code> is
called both before and after the data is transferred. Calling <code class="docutils literal notranslate"><span class="pre">xm.rendezvous</span></code>
in the middle of model code may force an unwanted compilation.</p></li>
<li><p>Because XLA does not permit collective operations to run on a subset of
workers, all workers must participate in the <code class="docutils literal notranslate"><span class="pre">rendezvous</span></code>.</p></li>
</ul>
<p>If you require the old behavior of <code class="docutils literal notranslate"><span class="pre">xm.rendezvous</span></code> (i.e. communicating data
without altering the XLA graph and/or synchronizing a subset of workers),
consider using
<cite>``torch.distributed.barrier`</cite> &lt;<a class="reference external" href="https://pytorch.org/docs/stable/distributed.html#torch.distributed.barrier">https://pytorch.org/docs/stable/distributed.html#torch.distributed.barrier</a>&gt;`_
or
<cite>``torch.distributed.all_gather_object`</cite> &lt;<a class="reference external" href="https://pytorch.org/docs/stable/distributed.html#torch.distributed.all_gather_object">https://pytorch.org/docs/stable/distributed.html#torch.distributed.all_gather_object</a>&gt;`_
with a <code class="docutils literal notranslate"><span class="pre">gloo</span></code> process group. If you are also using the <code class="docutils literal notranslate"><span class="pre">xla</span></code> <code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code>
backend, you can use <code class="docutils literal notranslate"><span class="pre">torch.new_group</span></code> to create a <code class="docutils literal notranslate"><span class="pre">gloo</span></code> subgroup. See <a class="reference external" href="https://pytorch.org/docs/stable/distributed.html#monitored-barrier">this
example</a>
from the PyTorch documentation. Keep in mind these constraints:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code> is not fully supported on TPU v2/v3. Only a subset of
operations with the <code class="docutils literal notranslate"><span class="pre">xla</span></code> backend are implemented, and <code class="docutils literal notranslate"><span class="pre">gloo</span></code> will likely not
work as expected in a multithreaded context.</p></li>
<li><p>In our experiments, <code class="docutils literal notranslate"><span class="pre">gloo</span></code> does not scale well to thousands of TPU chips, so
expect this alternative to be less reliable than using <code class="docutils literal notranslate"><span class="pre">xm.rendezvous</span></code> with
PJRT at large scales.</p></li>
</ul>
</div>
<div class="section" id="pjrt-and-torch-distributed">
<h3>PJRT and torch.distributed<a class="headerlink" href="#pjrt-and-torch-distributed" title="Permalink to this heading">¶</a></h3>
<p><em>New in PyTorch/XLA r2.0</em></p>
<p>When using PJRT with <code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code> and
<code class="docutils literal notranslate"><span class="pre">[torch.nn.parallel.DistributedDataParallel](https://github.com/pytorch/xla/blob/master/docs/ddp.md)</span></code>
we strongly recommend using the new <code class="docutils literal notranslate"><span class="pre">xla://</span></code> <code class="docutils literal notranslate"><span class="pre">init_method</span></code>, which automatically
finds the replica IDs, world size, and master IP by querying the runtime. For
example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">import</span> <span class="nn">torch_xla.core.xla_model</span> <span class="k">as</span> <span class="nn">xm</span>
<span class="kn">import</span> <span class="nn">torch_xla.distributed.xla_multiprocessing</span> <span class="k">as</span> <span class="nn">xmp</span>
<span class="kn">from</span> <span class="nn">torch_xla.experimental</span> <span class="kn">import</span> <span class="n">pjrt</span>

<span class="c1"># Required for `xla://` init_method and `xla` backend</span>
<span class="kn">import</span> <span class="nn">torch_xla.distributed.xla_backend</span>

<span class="k">def</span> <span class="nf">_all_gather</span><span class="p">(</span><span class="n">index</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
  <span class="c1"># No need to pass in `rank` or `world_size`</span>
  <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="s1">&#39;xla&#39;</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;xla://&#39;</span><span class="p">)</span>

  <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">index</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>
  <span class="n">output</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">())]</span>
  <span class="n">dist</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

  <span class="n">xm</span><span class="o">.</span><span class="n">mark_step</span><span class="p">()</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
  <span class="n">xmp</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span><span class="n">_all_gather</span><span class="p">)</span>
</pre></div>
</div>
<p>Note: Although the <code class="docutils literal notranslate"><span class="pre">xla://</span></code> init_method is not required on TPU v4, it is still
recommended. If you use <code class="docutils literal notranslate"><span class="pre">env://</span></code>, <code class="docutils literal notranslate"><span class="pre">MASTER_ADDR</span></code> must be set to IP host that has
device 0, which is <em>not</em> always worker 0. The <code class="docutils literal notranslate"><span class="pre">xla://</span></code> init_method finds this
IP automatically.</p>
<p>Note: For TPU v2/v3, you still need to import
<code class="docutils literal notranslate"><span class="pre">torch_xla.experimental.pjrt_backend</span></code>, as TPU v2/v3 support in
<code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code> is still experimental.</p>
<p>For more information about using <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> on PyTorch/XLA, see
<cite>``ddp.md`</cite> &lt;./ddp.md&gt;`_ on TPU V4. For an example that uses DDP and PJRT together,
run the following <a class="reference external" href="../test/test_train_mp_imagenet.py">example script</a> on a TPU:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">PJRT_DEVICE</span><span class="o">=</span><span class="n">TPU</span> <span class="n">python</span> <span class="n">xla</span><span class="o">/</span><span class="n">test</span><span class="o">/</span><span class="n">test_train_mp_mnist</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">ddp</span> <span class="o">--</span><span class="n">pjrt_distributed</span> <span class="o">--</span><span class="n">fake_data</span> <span class="o">--</span><span class="n">num_epochs</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="performance">
<h2>Performance<a class="headerlink" href="#performance" title="Permalink to this heading">¶</a></h2>
<p>TorchBench shows improvements in average training time across tasks with PJRT
compared to XRT, with an average improvement of over 35% on TPU v4-8. The
benefits vary significantly by task and model type, ranging from 0% to 175%.
The following chart shows the breakdown by task:</p>
<a class="reference external image-reference" href="assets/torchbench_pjrt_vs_xrt.svg"><img alt="PJRT vs XRT" src="assets/torchbench_pjrt_vs_xrt.svg" /></a>
<div class="section" id="new-tpu-runtime">
<h3>New TPU runtime<a class="headerlink" href="#new-tpu-runtime" title="Permalink to this heading">¶</a></h3>
<p><em>New in PyTorch/XLA r2.0</em></p>
<p>The PyTorch/XLA r2.0 release introduces support for the <a class="reference external" href="https://github.com/openxla/community/blob/main/rfcs/20230123-pjrt-plugin.md#rfc-openxla-pjrt-plugin">PJRT Plugin
API</a>,
used to access the new TFRT-based TPU runtime in <code class="docutils literal notranslate"><span class="pre">libtpu</span></code>. This is now the
default runtime when <code class="docutils literal notranslate"><span class="pre">PJRT_DEVICE=TPU</span></code> is set. The legacy StreamExecutor-based
TPU runtime used in 1.13 will still be available with <code class="docutils literal notranslate"><span class="pre">PJRT_DEVICE=TPU_LEGACY</span></code>
in the 2.0 release, but it will be removed in a future version. If you encounter
an issue that only happens on <code class="docutils literal notranslate"><span class="pre">TPU</span></code> and not <code class="docutils literal notranslate"><span class="pre">TPU_LEGACY</span></code>, please file an issue
on GitHub.</p>
<p>In most cases, we expect performance to be similar between the two runtimes, but
in some cases, the new runtime may be up to 30% faster. The following chart
shows the breakdown by task:</p>
<a class="reference external image-reference" href="assets/torchbench_tfrt_vs_se.svg"><img alt="TFRT vs StreamExecutor" src="assets/torchbench_tfrt_vs_se.svg" /></a>
<p>Note: the improvements shown in this chart are also included in the PJRT vs XRT
comparison.</p>
</div>
</div>
<div class="section" id="torchdynamo-torch-compile-integration-in-pytorch-xla">
<h2>TorchDynamo(torch.compile) integration in PyTorch XLA<a class="headerlink" href="#torchdynamo-torch-compile-integration-in-pytorch-xla" title="Permalink to this heading">¶</a></h2>
<p><a class="reference external" href="https://pytorch.org/docs/stable/torch.compiler.html">TorchDynamo</a> is a Python-level JIT compiler designed to make unmodified PyTorch programs faster. It provides a clean API for compiler backends to hook in and its biggest feature is to dynamically modify Python bytecode right before it is executed. In the pytorch/xla 2.0 release, PyTorch/XLA provided an experimental backend for the TorchDynamo for both inference and training.</p>
<p>The way that XLA bridge works is that Dynamo will provide a TorchFX graph when it recognizes a model pattern and PyTorch/XLA will use existing Lazy Tensor technology to compile the FX graph and return the compiled function.</p>
<div class="section" id="integration">
<h3>Integration<a class="headerlink" href="#integration" title="Permalink to this heading">¶</a></h3>
<p>Support for PyTorch/XLA and Dynamo currently exists by adding the <code class="docutils literal notranslate"><span class="pre">backend='openxla'</span></code> argument to <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code>. For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch_xla.core.xla_model</span> <span class="k">as</span> <span class="nn">xm</span>

<span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
  <span class="n">a_xla</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>
  <span class="n">b_xla</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>
  <span class="k">return</span> <span class="n">a_xla</span> <span class="o">+</span> <span class="n">b_xla</span>

<span class="n">compiled_code</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">add</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="s1">&#39;openxla&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">compiled_code</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="section" id="inference">
<h3>Inference<a class="headerlink" href="#inference" title="Permalink to this heading">¶</a></h3>
<p>Here is a small code example of running resnet18 with <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torch_xla.core.xla_model</span> <span class="k">as</span> <span class="nn">xm</span>

<span class="k">def</span> <span class="nf">eval_model</span><span class="p">(</span><span class="n">loader</span><span class="p">):</span>
  <span class="n">device</span> <span class="o">=</span> <span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">()</span>
  <span class="n">xla_resnet18</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
  <span class="n">xla_resnet18</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
  <span class="n">dynamo_resnet18</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">xla_resnet18</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="s1">&#39;openxla&#39;</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
      <span class="n">output</span> <span class="o">=</span> <span class="n">dynamo_resnet18</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
<p>With the <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> you will see that PyTorch/XLA only traces the resent18 model once during the init time and executes the compiled binary every time <code class="docutils literal notranslate"><span class="pre">dynamo_resnet18</span></code> is invoked, instead of tracing the model every time. Here is a inference speed analysis to compare Dynamo and Lazy using torch bench on Cloud TPU v4-8</p>
<p>resnet18 | 2.59
resnet50 | 2.64
resnext50_32x4d | 1.91
alexnet | 1.28
mobilenet_v2 | 18.62
mnasnet1_0 | 2.68
vgg16 | 1.33
BERT_pytorch | 7.49
squeezenet1_1 | 2.29
timm_vision_transformer | 3.52
geomean | 3.04</p>
</div>
<div class="section" id="training">
<h3>Training<a class="headerlink" href="#training" title="Permalink to this heading">¶</a></h3>
<p>PyTorch/XLA also supports Dynamo for training, but it is  experimental and we are working with the PyTorch Compiler team to iterate on the implementation. Here is an example of training a resnet18 with <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torch_xla.core.xla_model</span> <span class="k">as</span> <span class="nn">xm</span>

<span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
  <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
  <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
  <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
  <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
  <span class="k">return</span> <span class="n">pred</span>

<span class="k">def</span> <span class="nf">train_model_main</span><span class="p">(</span><span class="n">loader</span><span class="p">):</span>
  <span class="n">device</span> <span class="o">=</span> <span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">()</span>
  <span class="n">xla_resnet18</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
  <span class="n">xla_resnet18</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
  <span class="n">dynamo_train_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
        <span class="n">train_model</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="s1">&#39;openxla&#39;</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span>
    <span class="n">xla_optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">dynamo_train_model</span><span class="p">(</span><span class="n">xla_resnet18</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">xla_optimizer</span><span class="p">)</span>
</pre></div>
</div>
<p>We expect to extract and execute 3 graphs per training step instead of 1 graph per training step if you use the Lazy tensor. Here is a training speed analysis to compare Dynamo and Lazy using a torch bench on Cloud TPU v4-8.</p>
<p>resnet50 | 1.33
resnet18 | 1.33
BERT_pytorch | 3.07
resnext50_32x4d | 1.43
alexnet | 1.12
mobilenet_v2 | 1.4
mnasnet1_0 | 1.19
vgg16 | 0.81
timm_vision_transformer | 1.87
squeezenet1_1 | 1.41
geomean | 1.41</p>
<blockquote>
<div><p><strong>NOTE:</strong> We run each model’s fwd and bwd for a single step and then collect the e2e time. In the real world we will run multiple steps at each training job which can easily hide the tracing cost from execution(since it is async). Lazy Tensor will have much better performance in that scenario.</p>
</div></blockquote>
</div>
<div class="section" id="feature-gaps">
<h3>Feature gaps<a class="headerlink" href="#feature-gaps" title="Permalink to this heading">¶</a></h3>
<p>There is one gap we want to call out that are preventing us from using the TorchDynamo on larger scale models.</p>
<ol class="arabic simple">
<li><p>TorchDynamo will trace forward and backward into separate graphs. For PyTorch/XLA it is important to let the XLA compiler see the whole step as one graph to best optimize the speed. There is also a fixed overhead to launch every device execution which make executing multiple graphs per training step less ideal.</p></li>
</ol>
<p>This gap compared to Lazy Tensor makes it less efficient in real world training use cases, especially the tracing cost can be overlapped with the execution in training.</p>
</div>
<div class="section" id="take-away">
<h3>Take away<a class="headerlink" href="#take-away" title="Permalink to this heading">¶</a></h3>
<p>TorchDynamo provides a really promising way for the compiler backend to hide the complexity from the user and easily retrieve the modeling code in a graph format. Compared with PyTorch/XLA’s traditional Lazy Tensor way of extracting the graph, TorchDynamo can skip the graph tracing for every iteration, hence providing a much better inference response time.</p>
<p>Most models supported by PyTorch/XLA, have seen significant speedup when running inference with the new dynamo-xla bridge. Our community is working hard to expand the set of supported models. Regarding the training feature gaps mentioned above, the PyTorch/XLA community is super excited to improve the training gap in our upcoming development work. The team continues to heavily invest in TorchDynamo and work with the upstream to mature the training story.</p>
</div>
</div>
<div class="section" id="fully-sharded-data-parallel-fsdp-in-pytorch-xla">
<h2>Fully Sharded Data Parallel (FSDP) in PyTorch XLA<a class="headerlink" href="#fully-sharded-data-parallel-fsdp-in-pytorch-xla" title="Permalink to this heading">¶</a></h2>
<p>Fully Sharded Data Parallel (FSDP) in PyTorch XLA is a utility for sharding Module parameters across data-parallel workers.</p>
<p>Example usage:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch_xla.core.xla_model</span> <span class="k">as</span> <span class="nn">xm</span>
<span class="kn">from</span> <span class="nn">torch_xla.distributed.fsdp</span> <span class="kn">import</span> <span class="n">XlaFullyShardedDataParallel</span> <span class="k">as</span> <span class="n">FSDP</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">FSDP</span><span class="p">(</span><span class="n">my_module</span><span class="p">)</span>
<span class="n">optim</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">optim</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<p>It is also possible to shard individual layers separately and have an outer wrapper handle any leftover parameters.</p>
<p>Notes:</p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">XlaFullyShardedDataParallel</span></code> class supports both the ZeRO-2 optimizer (sharding gradients and optimizer states) and the ZeRO-3 optimizer (sharding parameters, gradients, and optimizer states) in <a class="reference external" href="https://arxiv.org/abs/1910.02054">https://arxiv.org/abs/1910.02054</a>.</p>
<ul>
<li><p>The ZeRO-3 optimizer should be implemented via nested FSDP with <code class="docutils literal notranslate"><span class="pre">reshard_after_forward=True</span></code>. See <code class="docutils literal notranslate"><span class="pre">test/test_train_mp_mnist_fsdp_with_ckpt.py</span></code> and <code class="docutils literal notranslate"><span class="pre">test/test_train_mp_imagenet_fsdp.py</span></code> for an example.</p></li>
<li><p>For large models that cannot fit into a single TPU memory or the host CPU memory, one should interleave submodule construction with inner FSDP wrapping. See <cite>``FSDPViTModel`</cite> &lt;<a class="reference external" href="https://github.com/ronghanghu/vit_10b_fsdp_example/blob/master/run_vit_training.py">https://github.com/ronghanghu/vit_10b_fsdp_example/blob/master/run_vit_training.py</a>&gt;`_ for an example.</p></li>
</ul>
</li>
<li><p>a simple wrapper <code class="docutils literal notranslate"><span class="pre">checkpoint_module</span></code> is provided (based on <code class="docutils literal notranslate"><span class="pre">torch_xla.utils.checkpoint.checkpoint</span></code> from <a class="reference external" href="https://github.com/pytorch/xla/pull/3524">https://github.com/pytorch/xla/pull/3524</a>) to perform <a class="reference external" href="https://spell.ml/blog/gradient-checkpointing-pytorch-YGypLBAAACEAefHs">gradient checkpointing</a> over a given <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> instance. See <code class="docutils literal notranslate"><span class="pre">test/test_train_mp_mnist_fsdp_with_ckpt.py</span></code> and <code class="docutils literal notranslate"><span class="pre">test/test_train_mp_imagenet_fsdp.py</span></code> for an example.</p></li>
<li><p>Auto-wrapping submodules: instead of manually nested FSDP wrapping, one can also specify an <code class="docutils literal notranslate"><span class="pre">auto_wrap_policy</span></code> argument to automatically wrap the submodules with inner FSDP. <code class="docutils literal notranslate"><span class="pre">size_based_auto_wrap_policy</span></code> in <code class="docutils literal notranslate"><span class="pre">torch_xla.distributed.fsdp.wrap</span></code> is an example of <code class="docutils literal notranslate"><span class="pre">auto_wrap_policy</span></code> callable, this policy wraps layers with the number of parameters larger than 100M. <code class="docutils literal notranslate"><span class="pre">transformer_auto_wrap_policy</span></code> in <code class="docutils literal notranslate"><span class="pre">torch_xla.distributed.fsdp.wrap</span></code> is an example of <code class="docutils literal notranslate"><span class="pre">auto_wrap_policy</span></code> callable for transformer-like model architectures.</p></li>
</ul>
<p>For example, to automatically wrap all <code class="docutils literal notranslate"><span class="pre">torch.nn.Conv2d</span></code> submodules with inner FSDP, one can use:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch_xla.distributed.fsdp.wrap</span> <span class="kn">import</span> <span class="n">transformer_auto_wrap_policy</span>
<span class="n">auto_wrap_policy</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">transformer_auto_wrap_policy</span><span class="p">,</span> <span class="n">transformer_layer_cls</span><span class="o">=</span><span class="p">{</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">})</span>
</pre></div>
</div>
<p>Additionally, one can also specify an <code class="docutils literal notranslate"><span class="pre">auto_wrapper_callable</span></code> argument to use a custom callable wrapper for the submodules (the default wrapper is just the <code class="docutils literal notranslate"><span class="pre">XlaFullyShardedDataParallel</span></code> class itself). For example, one can use the following to apply gradient checkpointing (i.e. activation checkpointing/rematerialization) to each auto-wrapped submodule.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch_xla.distributed.fsdp</span> <span class="kn">import</span> <span class="n">checkpoint_module</span>
<span class="n">auto_wrapper_callable</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">m</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">XlaFullyShardedDataParallel</span><span class="p">(</span>
    <span class="n">checkpoint_module</span><span class="p">(</span><span class="n">m</span><span class="p">),</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>
</div>
<ul>
<li><p>When stepping the optimizer, directly call <code class="docutils literal notranslate"><span class="pre">optimizer.step</span></code> and do not call <code class="docutils literal notranslate"><span class="pre">xm.optimizer_step</span></code>. The latter reduces the gradient across ranks, which is not needed for FSDP (where the parameters are already sharded).</p></li>
<li><p>When saving model and optimizer checkpoints during training, each training process needs to save its own checkpoint of the (sharded) model and optimizer state dicts (use <code class="docutils literal notranslate"><span class="pre">master_only=False</span></code> and set different paths for each rank in <code class="docutils literal notranslate"><span class="pre">xm.save</span></code>). When resuming, it needs to load the checkpoint for the corresponding rank.</p></li>
<li><p>Please also save <code class="docutils literal notranslate"><span class="pre">model.get_shard_metadata()</span></code> along with <code class="docutils literal notranslate"><span class="pre">model.state_dict()</span></code> as follows and use <code class="docutils literal notranslate"><span class="pre">consolidate_sharded_model_checkpoints</span></code> to stitch the sharded model checkpoints together into a full model state dict. See <code class="docutils literal notranslate"><span class="pre">test/test_train_mp_mnist_fsdp_with_ckpt.py</span></code> for an example.
.. code-block:: python3</p>
<blockquote>
<div><dl class="simple">
<dt>ckpt = {</dt><dd><p>‘model’: model.state_dict(),
‘shard_metadata’: model.get_shard_metadata(),
‘optimizer’: optimizer.state_dict(),</p>
</dd>
</dl>
<p>}
ckpt_path = f’/tmp/rank-{xm.get_ordinal()}-of-{xm.xrt_world_size()}.pth’
xm.save(ckpt, ckpt_path, master_only=False)</p>
</div></blockquote>
</li>
<li><p>The checkpoint consolidation script can also be launched from the command line as follows.
.. code-block:: bash</p>
<blockquote>
<div><p># consolidate the saved checkpoints via command line tool
python3 -m torch_xla.distributed.fsdp.consolidate_sharded_ckpts –ckpt_prefix /path/to/your_sharded_checkpoint_files –ckpt_suffix “_rank-<em>-of-</em>.pth”</p>
</div></blockquote>
</li>
</ul>
<p>The implementation of this class is largely inspired by and mostly follows the structure of <code class="docutils literal notranslate"><span class="pre">fairscale.nn.FullyShardedDataParallel</span></code> in <a class="reference external" href="https://fairscale.readthedocs.io/en/stable/api/nn/fsdp.html">https://fairscale.readthedocs.io/en/stable/api/nn/fsdp.html</a>. One of the biggest differences from <code class="docutils literal notranslate"><span class="pre">fairscale.nn.FullyShardedDataParallel</span></code> is that in XLA we don’t have explicit parameter storage, so here we resort to a different approach to free full parameters for ZeRO-3.</p>
<hr class="docutils" />
<div class="section" id="example-training-scripts-on-mnist-and-imagenet">
<h3>Example training scripts on MNIST and ImageNet<a class="headerlink" href="#example-training-scripts-on-mnist-and-imagenet" title="Permalink to this heading">¶</a></h3>
<ul class="simple">
<li><p>MNIST: <cite>``test/test_train_mp_mnist_fsdp_with_ckpt.py`</cite> &lt;<a class="reference external" href="https://github.com/pytorch/xla/blob/master/test/test_train_mp_mnist_fsdp_with_ckpt.py">https://github.com/pytorch/xla/blob/master/test/test_train_mp_mnist_fsdp_with_ckpt.py</a>&gt;`_ (it also tests checkpoint consolidation)</p></li>
<li><p>ImageNet: <cite>``test/test_train_mp_imagenet_fsdp.py`</cite> &lt;<a class="reference external" href="https://github.com/pytorch/xla/blob/master/test/test_train_mp_imagenet_fsdp.py">https://github.com/pytorch/xla/blob/master/test/test_train_mp_imagenet_fsdp.py</a>&gt;`_</p></li>
</ul>
<div class="section" id="installation">
<h4>Installation<a class="headerlink" href="#installation" title="Permalink to this heading">¶</a></h4>
<p>FSDP is available on PyTorch/XLA 1.12 release and newer nightly. Please refer to <a class="reference external" href="https://github.com/pytorch/xla#-available-images-and-wheels">https://github.com/pytorch/xla#-available-images-and-wheels</a> for installation guide.</p>
</div>
<div class="section" id="clone-pytorch-xla-repo">
<h4>Clone PyTorch/XLA repo<a class="headerlink" href="#clone-pytorch-xla-repo" title="Permalink to this heading">¶</a></h4>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>--recursive<span class="w"> </span>https://github.com/pytorch/pytorch
<span class="nb">cd</span><span class="w"> </span>pytorch/
git<span class="w"> </span>clone<span class="w"> </span>--recursive<span class="w"> </span>https://github.com/pytorch/xla.git
<span class="nb">cd</span><span class="w"> </span>~/
</pre></div>
</div>
</div>
<div class="section" id="train-mnist-on-v3-8-tpu">
<h4>Train MNIST on v3-8 TPU<a class="headerlink" href="#train-mnist-on-v3-8-tpu" title="Permalink to this heading">¶</a></h4>
<p>It gets around 98.9 accuracy for 2 epochs:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>~/pytorch/xla/test/test_train_mp_mnist_fsdp_with_ckpt.py<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--batch_size<span class="w"> </span><span class="m">16</span><span class="w"> </span>--drop_last<span class="w"> </span>--num_epochs<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--use_nested_fsdp<span class="w"> </span>--use_gradient_checkpointing
</pre></div>
</div>
<p>This script automatically tests checkpoint consolidation at the end. You can also manually consolidate the sharded checkpoints via</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># consolidate the saved checkpoints via command line tool</span>
python3<span class="w"> </span>-m<span class="w"> </span>torch_xla.distributed.fsdp.consolidate_sharded_ckpts<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--ckpt_prefix<span class="w"> </span>/tmp/mnist-fsdp/final_ckpt<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--ckpt_suffix<span class="w"> </span><span class="s2">&quot;_rank-*-of-*.pth&quot;</span>
</pre></div>
</div>
</div>
<div class="section" id="train-imagenet-with-resnet-50-on-v3-8-tpu">
<h4>Train ImageNet with ResNet-50 on v3-8 TPU<a class="headerlink" href="#train-imagenet-with-resnet-50-on-v3-8-tpu" title="Permalink to this heading">¶</a></h4>
<p>It gets around 75.9 accuracy for 100 epochs; download <a class="reference external" href="https://github.com/pytorch/examples/tree/master/imagenet#requirements">ImageNet-1k</a> to <code class="docutils literal notranslate"><span class="pre">/datasets/imagenet-1k</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>~/pytorch/xla/test/test_train_mp_imagenet_fsdp.py<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--datadir<span class="w"> </span>/datasets/imagenet-1k<span class="w"> </span>--drop_last<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--model<span class="w"> </span>resnet50<span class="w"> </span>--test_set_batch_size<span class="w"> </span><span class="m">64</span><span class="w"> </span>--eval_interval<span class="w"> </span><span class="m">10</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--lr<span class="w"> </span><span class="m">0</span>.4<span class="w"> </span>--batch_size<span class="w"> </span><span class="m">128</span><span class="w"> </span>--num_warmup_epochs<span class="w"> </span><span class="m">5</span><span class="w"> </span>--lr_scheduler_divide_every_n_epochs<span class="w"> </span><span class="m">30</span><span class="w"> </span>--lr_scheduler_divisor<span class="w"> </span><span class="m">10</span><span class="w"> </span>--num_epochs<span class="w"> </span><span class="m">100</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--use_nested_fsdp
</pre></div>
</div>
<p>You can also add <code class="docutils literal notranslate"><span class="pre">--use_gradient_checkpointing</span></code> (which needs to be used along with <code class="docutils literal notranslate"><span class="pre">--use_nested_fsdp</span></code> or <code class="docutils literal notranslate"><span class="pre">--auto_wrap_policy</span></code>) to apply gradient checkpointing on the residual blocks.</p>
</div>
</div>
<hr class="docutils" />
<div class="section" id="example-training-scripts-on-tpu-pod-with-10-billion-parameters">
<h3>Example training scripts on TPU pod (with 10 billion parameters)<a class="headerlink" href="#example-training-scripts-on-tpu-pod-with-10-billion-parameters" title="Permalink to this heading">¶</a></h3>
<p>To train large models that cannot fit into a single TPU, one should apply auto-wrap or manually wrap the submodules with inner FSDP when building the entire model to implement the ZeRO-3 algorithm.</p>
<p>Please see <a class="reference external" href="https://github.com/ronghanghu/vit_10b_fsdp_example">https://github.com/ronghanghu/vit_10b_fsdp_example</a> for an example of sharded training of a Vision Transformer (ViT) model using this XLA FSDP PR.</p>
</div>
</div>
</div>
<div class="section" id="how-to-do-distributeddataparallel">
<h1>How to do <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code><a class="headerlink" href="#how-to-do-distributeddataparallel" title="Permalink to this heading">¶</a></h1>
<p>This document shows how to use torch.nn.parallel.DistributedDataParallel in xla,
and further describes its difference against the native xla data parallel
approach.</p>
<div class="section" id="background-motivation">
<h2>Background / Motivation<a class="headerlink" href="#background-motivation" title="Permalink to this heading">¶</a></h2>
<p>Customers have long requested the ability to use PyTorch’s
DistributedDataParallel API with xla. And here we enable it as an experimental
feature.</p>
</div>
<div class="section" id="how-to-use-distributeddataparallel">
<h2>How to use DistributedDataParallel<a class="headerlink" href="#how-to-use-distributeddataparallel" title="Permalink to this heading">¶</a></h2>
<p>For those who switched from the PyTorch eager mode to XLA, here are all the
changes you need to do to convert your eager DDP model into XLA model. We assume
that you already know how to use XLA <a class="reference external" href="../API_GUIDE.md#running-on-a-single-xla-device">on a single
device</a>.</p>
<ol class="arabic simple">
<li><p>Import xla specific distributed packages:</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch_xla.core.xla_model</span> <span class="k">as</span> <span class="nn">xm</span>
<span class="kn">import</span> <span class="nn">torch_xla.distributed.xla_backend</span>
</pre></div>
</div>
<ol class="arabic simple">
<li><p>Init xla process group similar to other process groups such as nccl and gloo.</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="s2">&quot;xla&quot;</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic simple">
<li><p>Use xla specific APIs to get rank and world_size if you need to.</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">new_rank</span> <span class="o">=</span> <span class="n">xm</span><span class="o">.</span><span class="n">get_ordinal</span><span class="p">()</span>
<span class="n">world_size</span> <span class="o">=</span> <span class="n">xm</span><span class="o">.</span><span class="n">xrt_world_size</span><span class="p">()</span>
</pre></div>
</div>
<ol class="arabic simple">
<li><p>Pass <code class="docutils literal notranslate"><span class="pre">gradient_as_bucket_view=True</span></code> to the DDP wrapper.</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ddp_model</span> <span class="o">=</span> <span class="n">DDP</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">gradient_as_bucket_view</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic simple">
<li><p>Finally launch your model with xla specific launcher.</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">xmp</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span><span class="n">demo_fn</span><span class="p">)</span>
</pre></div>
</div>
<p>Here we have put everything together (the example is actually taken from the
<a class="reference external" href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html">DDP tutorial</a>).
The way you code it is pretty similar to the eager experience. Just with xla
specific touches on a single device plus the above five changes to your script.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">tempfile</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>

<span class="kn">from</span> <span class="nn">torch.nn.parallel</span> <span class="kn">import</span> <span class="n">DistributedDataParallel</span> <span class="k">as</span> <span class="n">DDP</span>

<span class="c1"># additional imports for xla</span>
<span class="kn">import</span> <span class="nn">torch_xla.core.xla_model</span> <span class="k">as</span> <span class="nn">xm</span>
<span class="kn">import</span> <span class="nn">torch_xla.distributed.xla_backend</span>
<span class="kn">import</span> <span class="nn">torch_xla.distributed.xla_multiprocessing</span> <span class="k">as</span> <span class="nn">xmp</span>

<span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">):</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;MASTER_ADDR&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;localhost&#39;</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;MASTER_PORT&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;12355&#39;</span>

    <span class="c1"># initialize the xla process group</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="s2">&quot;xla&quot;</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">cleanup</span><span class="p">():</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">destroy_process_group</span><span class="p">()</span>

<span class="k">class</span> <span class="nc">ToyModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ToyModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1000000</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1000000</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">net2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">net1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>

<span class="k">def</span> <span class="nf">demo_basic</span><span class="p">(</span><span class="n">rank</span><span class="p">):</span>
    <span class="c1"># xla specific APIs to get rank, world_size.</span>
    <span class="n">new_rank</span> <span class="o">=</span> <span class="n">xm</span><span class="o">.</span><span class="n">get_ordinal</span><span class="p">()</span>
    <span class="k">assert</span> <span class="n">new_rank</span> <span class="o">==</span> <span class="n">rank</span>
    <span class="n">world_size</span> <span class="o">=</span> <span class="n">xm</span><span class="o">.</span><span class="n">xrt_world_size</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Running basic DDP example on rank </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">setup</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">)</span>

    <span class="c1"># create model and move it to XLA device</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">()</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">ToyModel</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="c1"># currently, graident_as_bucket_view is needed to make DDP work for xla</span>
    <span class="n">ddp_model</span> <span class="o">=</span> <span class="n">DDP</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">gradient_as_bucket_view</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">ddp_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">ddp_model</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">loss_fn</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="c1"># xla specific API to execute the graph</span>
    <span class="n">xm</span><span class="o">.</span><span class="n">mark_step</span><span class="p">()</span>

    <span class="n">cleanup</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">run_demo</span><span class="p">(</span><span class="n">demo_fn</span><span class="p">):</span>
    <span class="c1"># xla specific launcher</span>
    <span class="n">xmp</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span><span class="n">demo_fn</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">run_demo</span><span class="p">(</span><span class="n">demo_basic</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="benchmarking">
<h2>Benchmarking<a class="headerlink" href="#benchmarking" title="Permalink to this heading">¶</a></h2>
<div class="section" id="resnet50-with-fake-data">
<h3>Resnet50 with fake data<a class="headerlink" href="#resnet50-with-fake-data" title="Permalink to this heading">¶</a></h3>
<p>The following results are collected with the command: <code class="docutils literal notranslate"><span class="pre">python</span>
<span class="pre">test/test_train_mp_imagenet.py</span> <span class="pre">--fake_data</span> <span class="pre">--model=resnet50</span> <span class="pre">--num_epochs=1</span></code> on a
TPU VM V3-8 environment with ToT PyTorch and PyTorch/XLA. And the statistical
metrics are produced by using the script in this <a class="reference external" href="https://github.com/pytorch/xla/pull/4107">pull
request</a>. The unit for the rate is
images per second.</p>
<table>
  <tr>
   <td>Type
   </td>
   <td>Mean
   </td>
   <td>Median
   </td>
   <td>90th %
   </td>
   <td>Std Dev
   </td>
   <td>CV
   </td>
  </tr>
  <tr>
   <td>xm.optimizer_step
   </td>
   <td>418.54
   </td>
   <td>419.22
   </td>
   <td>430.40
   </td>
   <td>9.76
   </td>
   <td>0.02
   </td>
  </tr>
  <tr>
   <td>DDP
   </td>
   <td>395.97
   </td>
   <td>395.54
   </td>
   <td>407.13
   </td>
   <td>7.60
   </td>
   <td>0.02
   </td>
  </tr>
</table><p>The performance difference between our native approach for distributed data
parallel and DistributedDataParallel wrapper is: 1 - 395.97 / 418.54 = 5.39%.
This result seems reasonable given the DDP wrapper introduces extra overheads on
tracing the DDP runtime.</p>
</div>
<div class="section" id="mnist-with-fake-data">
<h3>MNIST with fake data<a class="headerlink" href="#mnist-with-fake-data" title="Permalink to this heading">¶</a></h3>
<p>The following results are collected with the command: <code class="docutils literal notranslate"><span class="pre">python</span>
<span class="pre">test/test_train_mp_mnist.py</span> <span class="pre">--fake_data</span></code> on a TPU VM V3-8 environment with ToT
PyTorch and PyTorch/XLA. And the statistical metrics are produced by using the
script in this <a class="reference external" href="https://github.com/pytorch/xla/pull/4107">pull request</a>. The
unit for the rate is images per second.</p>
<table>
  <tr>
   <td>Type
   </td>
   <td>Mean
   </td>
   <td>Median
   </td>
   <td>90th %
   </td>
   <td>Std Dev
   </td>
   <td>CV
   </td>
  </tr>
  <tr>
   <td>xm.optimizer_step
   </td>
   <td>17864.19
   </td>
   <td>20108.96
   </td>
   <td>24351.74
   </td>
   <td>5866.83
   </td>
   <td>0.33
   </td>
  </tr>
  <tr>
   <td>DDP
   </td>
   <td>10701.39
   </td>
   <td>11770.00
   </td>
   <td>14313.78
   </td>
   <td>3102.92
   </td>
   <td>0.29
   </td>
  </tr>
</table><p>The performance difference between our native approach for distributed data
parallel and DistributedDataParallel wrapper is: 1 - 14313.78 / 24351.74 =
41.22%. Here we compare 90th % instead since the dataset is small and first a
few rounds are heavily impacted by data loading. This slowdown is huge but makes
sense given the model is small. The additional DDP runtime tracing overhead is
hard to amortize.</p>
</div>
<div class="section" id="mnist-with-real-data">
<h3>MNIST with real data<a class="headerlink" href="#mnist-with-real-data" title="Permalink to this heading">¶</a></h3>
<p>The following results are collected with the command: <code class="docutils literal notranslate"><span class="pre">python</span>
<span class="pre">test/test_train_mp_mnist.py</span> <span class="pre">--logdir</span> <span class="pre">mnist/</span></code> on a TPU VM V3-8 environment with
ToT PyTorch and PyTorch/XLA.</p>
<a class="reference external image-reference" href="assets/ddp_md_mnist_with_real_data.png"><img alt="learning_curves" src="assets/ddp_md_mnist_with_real_data.png" /></a>
<p>And we can observe that the DDP wrapper converges slower than the native XLA
approach even though it still achieves a high accuracy rate at 97.48% at the
end. (The native approach achieves 99%.)</p>
</div>
</div>
<div class="section" id="disclaimer">
<h2>Disclaimer<a class="headerlink" href="#disclaimer" title="Permalink to this heading">¶</a></h2>
<p>This feature is still experimental and under active development. Use it in
cautions and feel free to file any bugs to the <a class="reference external" href="https://github.com/pytorch/xla/">xla github
repo</a>. For those who are interested in the
native xla data parallel approach, here is the
<a class="reference external" href="../API_GUIDE.md#running-on-multiple-xla-devices-with-multi-processing">tutorial</a>.</p>
<p>Here are some of the known issues that are under investigation:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">gradient_as_bucket_view=True</span></code> needs to be enforced.</p></li>
<li><p>There are some issues while being used with <code class="docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code>. <code class="docutils literal notranslate"><span class="pre">​​test_train_mp_mnist.py</span></code> with real data crashes before exiting.</p></li>
</ul>
</div>
</div>
<div class="section" id="how-to-run-with-pytorch-xla-gpu">
<h1>How to run with PyTorch/XLA:GPU<a class="headerlink" href="#how-to-run-with-pytorch-xla-gpu" title="Permalink to this heading">¶</a></h1>
<p>PyTorch/XLA enables PyTorch users to utilize the XLA compiler which supports accelerators including TPU, GPU, and CPU. This doc will go over the basic steps to run PyTorch/XLA on a nvidia GPU instances.</p>
<div class="section" id="create-a-gpu-instance">
<h2>Create a GPU instance<a class="headerlink" href="#create-a-gpu-instance" title="Permalink to this heading">¶</a></h2>
<p>You can either use a local machine with GPU attached or a GPU VM on the cloud. For example in Google Cloud you can follow this <a class="reference external" href="https://cloud.google.com/compute/docs/gpus/create-vm-with-gpus">doc</a> to create the GPU VM.</p>
</div>
<div class="section" id="environment-setup">
<h2>Environment Setup<a class="headerlink" href="#environment-setup" title="Permalink to this heading">¶</a></h2>
<p>Make sure you have cuda driver installed on the host.</p>
<div class="section" id="id23">
<h3>Docker<a class="headerlink" href="#id23" title="Permalink to this heading">¶</a></h3>
<p>Pytorch/XLA currently publish prebuilt docker images and wheels with cuda11.8/12.1 and python 3.8. We recommend users to create a docker container with corresponding config. For a full list of docker images and wheels, please refer to <a class="reference external" href="https://github.com/pytorch/xla#available-docker-images-and-wheels">this doc</a>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>sudo docker pull us-central1-docker.pkg.dev/tpu-pytorch-releases/docker/xla:nightly_3.8_cuda_12.1

# Installing the NVIDIA Container Toolkit per https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html
# For example
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
  &amp;&amp; curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
    sed &#39;s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g&#39; | \
    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit

# Configuring the NVIDIA Container Toolkit
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker

sudo docker run --shm-size=16g --net=host --gpus all -it -d us-central1-docker.pkg.dev/tpu-pytorch-releases/docker/xla:nightly_3.8_cuda_12.1 bin/bash
sudo docker exec -it $(sudo docker ps | awk &#39;NR==2 { print $1 }&#39;) /bin/bash
</pre></div>
</div>
<p>Note that you need to restart the docker to make gpu devices visible in the docker container. After logging into the docker, you can use <code class="docutils literal notranslate"><span class="pre">nvidia-smi</span></code> to verify the device is setup correctly.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">pytorch</span><span class="p">)</span> <span class="n">root</span><span class="o">@</span><span class="mi">20</span><span class="n">ab2c7a2d06</span><span class="p">:</span><span class="o">/</span><span class="c1"># nvidia-smi</span>
<span class="n">Thu</span> <span class="n">Dec</span>  <span class="mi">8</span> <span class="mi">06</span><span class="p">:</span><span class="mi">24</span><span class="p">:</span><span class="mi">29</span> <span class="mi">2022</span>
<span class="o">+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">NVIDIA</span><span class="o">-</span><span class="n">SMI</span> <span class="mf">510.47.03</span>    <span class="n">Driver</span> <span class="n">Version</span><span class="p">:</span> <span class="mf">510.47.03</span>    <span class="n">CUDA</span> <span class="n">Version</span><span class="p">:</span> <span class="mf">11.6</span>     <span class="o">|</span>
<span class="o">|-------------------------------+----------------------+----------------------+</span>
<span class="o">|</span> <span class="n">GPU</span>  <span class="n">Name</span>        <span class="n">Persistence</span><span class="o">-</span><span class="n">M</span><span class="o">|</span> <span class="n">Bus</span><span class="o">-</span><span class="n">Id</span>        <span class="n">Disp</span><span class="o">.</span><span class="n">A</span> <span class="o">|</span> <span class="n">Volatile</span> <span class="n">Uncorr</span><span class="o">.</span> <span class="n">ECC</span> <span class="o">|</span>
<span class="o">|</span> <span class="n">Fan</span>  <span class="n">Temp</span>  <span class="n">Perf</span>  <span class="n">Pwr</span><span class="p">:</span><span class="n">Usage</span><span class="o">/</span><span class="n">Cap</span><span class="o">|</span>         <span class="n">Memory</span><span class="o">-</span><span class="n">Usage</span> <span class="o">|</span> <span class="n">GPU</span><span class="o">-</span><span class="n">Util</span>  <span class="n">Compute</span> <span class="n">M</span><span class="o">.</span> <span class="o">|</span>
<span class="o">|</span>                               <span class="o">|</span>                      <span class="o">|</span>               <span class="n">MIG</span> <span class="n">M</span><span class="o">.</span> <span class="o">|</span>
<span class="o">|===============================+======================+======================|</span>
<span class="o">|</span>   <span class="mi">0</span>  <span class="n">Tesla</span> <span class="n">V100</span><span class="o">-</span><span class="n">SXM2</span><span class="o">...</span>  <span class="n">Off</span>  <span class="o">|</span> <span class="mi">00000000</span><span class="p">:</span><span class="mi">00</span><span class="p">:</span><span class="mf">04.0</span> <span class="n">Off</span> <span class="o">|</span>                    <span class="mi">0</span> <span class="o">|</span>
<span class="o">|</span> <span class="n">N</span><span class="o">/</span><span class="n">A</span>   <span class="mi">36</span><span class="n">C</span>    <span class="n">P0</span>    <span class="mi">38</span><span class="n">W</span> <span class="o">/</span> <span class="mi">300</span><span class="n">W</span> <span class="o">|</span>      <span class="mi">0</span><span class="n">MiB</span> <span class="o">/</span> <span class="mi">16384</span><span class="n">MiB</span> <span class="o">|</span>      <span class="mi">1</span><span class="o">%</span>      <span class="n">Default</span> <span class="o">|</span>
<span class="o">|</span>                               <span class="o">|</span>                      <span class="o">|</span>                  <span class="n">N</span><span class="o">/</span><span class="n">A</span> <span class="o">|</span>
<span class="o">+-------------------------------+----------------------+----------------------+</span>

<span class="o">+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">Processes</span><span class="p">:</span>                                                                  <span class="o">|</span>
<span class="o">|</span>  <span class="n">GPU</span>   <span class="n">GI</span>   <span class="n">CI</span>        <span class="n">PID</span>   <span class="n">Type</span>   <span class="n">Process</span> <span class="n">name</span>                  <span class="n">GPU</span> <span class="n">Memory</span> <span class="o">|</span>
<span class="o">|</span>        <span class="n">ID</span>   <span class="n">ID</span>                                                   <span class="n">Usage</span>      <span class="o">|</span>
<span class="o">|=============================================================================|</span>
<span class="o">|</span>  <span class="n">No</span> <span class="n">running</span> <span class="n">processes</span> <span class="n">found</span>                                                 <span class="o">|</span>
<span class="o">+-----------------------------------------------------------------------------+</span>
</pre></div>
</div>
</div>
<div class="section" id="check-environment-variable">
<h3>Check environment variable<a class="headerlink" href="#check-environment-variable" title="Permalink to this heading">¶</a></h3>
<p>Make sure <code class="docutils literal notranslate"><span class="pre">PATH</span></code> and <code class="docutils literal notranslate"><span class="pre">LD_LIBRARY_PATH</span></code> environment variables account for cuda. Please do a <code class="docutils literal notranslate"><span class="pre">echo</span> <span class="pre">$PATH</span></code> and <code class="docutils literal notranslate"><span class="pre">echo</span> <span class="pre">$LD_LIBRARY_PATH</span></code> to verify. If not, please follow <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#mandatory-actions">link</a> to do so. Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">echo</span> <span class="s2">&quot;export PATH=\$PATH:/usr/local/cuda-12.1/bin&quot;</span> <span class="o">&gt;&gt;</span> <span class="o">~/.</span><span class="n">bashrc</span>
<span class="n">echo</span> <span class="s2">&quot;export LD_LIBRARY_PATH=\$LD_LIBRARY_PATH:/usr/local/cuda-12.1/lib64&quot;</span> <span class="o">&gt;&gt;</span> <span class="o">~/.</span><span class="n">bashrc</span>
<span class="n">source</span> <span class="o">~/.</span><span class="n">bashrc</span>
</pre></div>
</div>
</div>
<div class="section" id="wheel">
<h3>Wheel<a class="headerlink" href="#wheel" title="Permalink to this heading">¶</a></h3>
<blockquote>
<div><p><a href="#id24"><span class="problematic" id="id25">**</span></a><em>NOTE:</em>**  The wheel file is compatible only with x86_64 linux based architecutre. To check the architecture of your linux system, execute the following command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">uname</span> <span class="o">-</span><span class="n">a</span>
</pre></div>
</div>
</div></blockquote>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip3</span> <span class="n">install</span> <span class="n">torch</span><span class="o">==</span><span class="mf">2.3.0</span>
<span class="c1"># GPU whl for python 3.10 + cuda 12.1</span>
<span class="n">pip3</span> <span class="n">install</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">storage</span><span class="o">.</span><span class="n">googleapis</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">pytorch</span><span class="o">-</span><span class="n">xla</span><span class="o">-</span><span class="n">releases</span><span class="o">/</span><span class="n">wheels</span><span class="o">/</span><span class="n">cuda</span><span class="o">/</span><span class="mf">12.1</span><span class="o">/</span><span class="n">torch_xla</span><span class="o">-</span><span class="mf">2.3.0</span><span class="o">-</span><span class="n">cp310</span><span class="o">-</span><span class="n">cp310</span><span class="o">-</span><span class="n">manylinux_2_28_x86_64</span><span class="o">.</span><span class="n">whl</span>
</pre></div>
</div>
<p>Wheels for other Python version and CUDA version can be found <a class="reference external" href="https://github.com/pytorch/xla?tab=readme-ov-file#available-docker-images-and-wheels">here</a>.</p>
</div>
</div>
<div class="section" id="run-some-simple-models">
<h2>Run some simple models<a class="headerlink" href="#run-some-simple-models" title="Permalink to this heading">¶</a></h2>
<p>In order to run below examples, you need to clone the pytorch/xla repository.</p>
<div class="section" id="mp-imagenet-example">
<h3>MP_ImageNet Example<a class="headerlink" href="#mp-imagenet-example" title="Permalink to this heading">¶</a></h3>
<p>This example uses ImageNet. It is included in what we already cloned in our Docker container.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">pytorch</span><span class="p">)</span> <span class="n">root</span><span class="o">@</span><span class="mi">20</span><span class="n">ab2c7a2d06</span><span class="p">:</span><span class="o">/</span><span class="c1"># export GPU_NUM_DEVICES=1 PJRT_DEVICE=CUDA</span>
<span class="p">(</span><span class="n">pytorch</span><span class="p">)</span> <span class="n">root</span><span class="o">@</span><span class="mi">20</span><span class="n">ab2c7a2d06</span><span class="p">:</span><span class="o">/</span><span class="c1"># git clone --recursive https://github.com/pytorch/xla.git</span>
<span class="p">(</span><span class="n">pytorch</span><span class="p">)</span> <span class="n">root</span><span class="o">@</span><span class="mi">20</span><span class="n">ab2c7a2d06</span><span class="p">:</span><span class="o">/</span><span class="c1"># python xla/test/test_train_mp_imagenet.py --fake_data</span>
<span class="o">==&gt;</span> <span class="n">Preparing</span> <span class="n">data</span><span class="o">..</span>
<span class="n">Epoch</span> <span class="mi">1</span> <span class="n">train</span> <span class="n">begin</span> <span class="mi">06</span><span class="p">:</span><span class="mi">12</span><span class="p">:</span><span class="mi">38</span>
<span class="o">|</span> <span class="n">Training</span> <span class="n">Device</span><span class="o">=</span><span class="n">xla</span><span class="p">:</span><span class="mi">0</span><span class="o">/</span><span class="mi">0</span> <span class="n">Epoch</span><span class="o">=</span><span class="mi">1</span> <span class="n">Step</span><span class="o">=</span><span class="mi">0</span> <span class="n">Loss</span><span class="o">=</span><span class="mf">6.89059</span> <span class="n">Rate</span><span class="o">=</span><span class="mf">2.82</span> <span class="n">GlobalRate</span><span class="o">=</span><span class="mf">2.82</span> <span class="n">Time</span><span class="o">=</span><span class="mi">06</span><span class="p">:</span><span class="mi">13</span><span class="p">:</span><span class="mi">23</span>
<span class="o">|</span> <span class="n">Training</span> <span class="n">Device</span><span class="o">=</span><span class="n">xla</span><span class="p">:</span><span class="mi">0</span><span class="o">/</span><span class="mi">0</span> <span class="n">Epoch</span><span class="o">=</span><span class="mi">1</span> <span class="n">Step</span><span class="o">=</span><span class="mi">20</span> <span class="n">Loss</span><span class="o">=</span><span class="mf">6.79297</span> <span class="n">Rate</span><span class="o">=</span><span class="mf">117.16</span> <span class="n">GlobalRate</span><span class="o">=</span><span class="mf">45.84</span> <span class="n">Time</span><span class="o">=</span><span class="mi">06</span><span class="p">:</span><span class="mi">13</span><span class="p">:</span><span class="mi">36</span>
<span class="o">|</span> <span class="n">Training</span> <span class="n">Device</span><span class="o">=</span><span class="n">xla</span><span class="p">:</span><span class="mi">0</span><span class="o">/</span><span class="mi">0</span> <span class="n">Epoch</span><span class="o">=</span><span class="mi">1</span> <span class="n">Step</span><span class="o">=</span><span class="mi">40</span> <span class="n">Loss</span><span class="o">=</span><span class="mf">6.43628</span> <span class="n">Rate</span><span class="o">=</span><span class="mf">281.16</span> <span class="n">GlobalRate</span><span class="o">=</span><span class="mf">80.49</span> <span class="n">Time</span><span class="o">=</span><span class="mi">06</span><span class="p">:</span><span class="mi">13</span><span class="p">:</span><span class="mi">43</span>
<span class="o">|</span> <span class="n">Training</span> <span class="n">Device</span><span class="o">=</span><span class="n">xla</span><span class="p">:</span><span class="mi">0</span><span class="o">/</span><span class="mi">0</span> <span class="n">Epoch</span><span class="o">=</span><span class="mi">1</span> <span class="n">Step</span><span class="o">=</span><span class="mi">60</span> <span class="n">Loss</span><span class="o">=</span><span class="mf">5.83108</span> <span class="n">Rate</span><span class="o">=</span><span class="mf">346.88</span> <span class="n">GlobalRate</span><span class="o">=</span><span class="mf">108.82</span> <span class="n">Time</span><span class="o">=</span><span class="mi">06</span><span class="p">:</span><span class="mi">13</span><span class="p">:</span><span class="mi">49</span>
<span class="o">|</span> <span class="n">Training</span> <span class="n">Device</span><span class="o">=</span><span class="n">xla</span><span class="p">:</span><span class="mi">0</span><span class="o">/</span><span class="mi">0</span> <span class="n">Epoch</span><span class="o">=</span><span class="mi">1</span> <span class="n">Step</span><span class="o">=</span><span class="mi">80</span> <span class="n">Loss</span><span class="o">=</span><span class="mf">4.99023</span> <span class="n">Rate</span><span class="o">=</span><span class="mf">373.62</span> <span class="n">GlobalRate</span><span class="o">=</span><span class="mf">132.43</span> <span class="n">Time</span><span class="o">=</span><span class="mi">06</span><span class="p">:</span><span class="mi">13</span><span class="p">:</span><span class="mi">56</span>
<span class="o">|</span> <span class="n">Training</span> <span class="n">Device</span><span class="o">=</span><span class="n">xla</span><span class="p">:</span><span class="mi">0</span><span class="o">/</span><span class="mi">0</span> <span class="n">Epoch</span><span class="o">=</span><span class="mi">1</span> <span class="n">Step</span><span class="o">=</span><span class="mi">100</span> <span class="n">Loss</span><span class="o">=</span><span class="mf">3.92699</span> <span class="n">Rate</span><span class="o">=</span><span class="mf">384.33</span> <span class="n">GlobalRate</span><span class="o">=</span><span class="mf">152.40</span> <span class="n">Time</span><span class="o">=</span><span class="mi">06</span><span class="p">:</span><span class="mi">14</span><span class="p">:</span><span class="mi">02</span>
<span class="o">|</span> <span class="n">Training</span> <span class="n">Device</span><span class="o">=</span><span class="n">xla</span><span class="p">:</span><span class="mi">0</span><span class="o">/</span><span class="mi">0</span> <span class="n">Epoch</span><span class="o">=</span><span class="mi">1</span> <span class="n">Step</span><span class="o">=</span><span class="mi">120</span> <span class="n">Loss</span><span class="o">=</span><span class="mf">2.68816</span> <span class="n">Rate</span><span class="o">=</span><span class="mf">388.35</span> <span class="n">GlobalRate</span><span class="o">=</span><span class="mf">169.49</span> <span class="n">Time</span><span class="o">=</span><span class="mi">06</span><span class="p">:</span><span class="mi">14</span><span class="p">:</span><span class="mi">09</span>
</pre></div>
</div>
</div>
<div class="section" id="resnet-example">
<h3>ResNet Example<a class="headerlink" href="#resnet-example" title="Permalink to this heading">¶</a></h3>
<p>This example uses ResNet.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">pytorch</span><span class="p">)</span> <span class="n">root</span><span class="o">@</span><span class="mi">20</span><span class="n">ab2c7a2d06</span><span class="p">:</span><span class="o">/</span><span class="c1"># python3 /xla/examples/train_resnet_base.py</span>
<span class="mi">1</span><span class="p">:</span><span class="mi">35</span><span class="n">PM</span> <span class="n">UTC</span> <span class="n">on</span> <span class="n">Jun</span> <span class="mi">08</span><span class="p">,</span> <span class="mi">2024</span>
<span class="n">epoch</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="n">step</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">6.887794017791748</span><span class="p">,</span> <span class="n">rate</span><span class="p">:</span> <span class="mf">8.746502586051985</span>
<span class="n">epoch</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="n">step</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">6.877807140350342</span><span class="p">,</span> <span class="n">rate</span><span class="p">:</span> <span class="mf">238.4789458412044</span>
<span class="n">epoch</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="n">step</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">6.867819786071777</span><span class="p">,</span> <span class="n">rate</span><span class="p">:</span> <span class="mf">329.86095958663503</span>
<span class="n">epoch</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="n">step</span><span class="p">:</span> <span class="mi">30</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">6.857839584350586</span><span class="p">,</span> <span class="n">rate</span><span class="p">:</span> <span class="mf">367.3038003653586</span>
<span class="n">epoch</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="n">step</span><span class="p">:</span> <span class="mi">40</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">6.847847938537598</span><span class="p">,</span> <span class="n">rate</span><span class="p">:</span> <span class="mf">381.53141087190835</span>
<span class="n">epoch</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="n">step</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">6.837860584259033</span><span class="p">,</span> <span class="n">rate</span><span class="p">:</span> <span class="mf">387.80462249591113</span>
<span class="o">...</span>
<span class="n">epoch</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="n">step</span><span class="p">:</span> <span class="mi">260</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">6.628140926361084</span><span class="p">,</span> <span class="n">rate</span><span class="p">:</span> <span class="mf">391.135639565343</span>
<span class="n">epoch</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="n">step</span><span class="p">:</span> <span class="mi">270</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">6.618192195892334</span><span class="p">,</span> <span class="n">rate</span><span class="p">:</span> <span class="mf">391.6901797745233</span>
<span class="n">epoch</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="n">step</span><span class="p">:</span> <span class="mi">280</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">6.608224391937256</span><span class="p">,</span> <span class="n">rate</span><span class="p">:</span> <span class="mf">391.1602680460045</span>
<span class="n">epoch</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="n">step</span><span class="p">:</span> <span class="mi">290</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">6.598264217376709</span><span class="p">,</span> <span class="n">rate</span><span class="p">:</span> <span class="mf">391.6731498290759</span>
<span class="n">Epoch</span> <span class="mi">1</span> <span class="n">train</span> <span class="n">end</span>  <span class="mi">1</span><span class="p">:</span><span class="mi">36</span><span class="n">PM</span> <span class="n">UTC</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="amp-automatic-mixed-precision">
<h2>AMP (AUTOMATIC MIXED PRECISION)<a class="headerlink" href="#amp-automatic-mixed-precision" title="Permalink to this heading">¶</a></h2>
<p>AMP is very useful on GPU training and PyTorch/XLA reuse Cuda’s AMP rule. You can checkout our <a class="reference external" href="https://github.com/pytorch/xla/blob/master/test/test_train_mp_mnist_amp.py">mnist example</a> and <a class="reference external" href="https://github.com/pytorch/xla/blob/master/test/test_train_mp_imagenet_amp.py">imagenet example</a>. Note that we also used a modified version of <a class="reference external" href="https://github.com/pytorch/xla/tree/master/torch_xla/amp/syncfree">optimizers</a> to avoid the additional sync between device and host.</p>
</div>
<div class="section" id="develop-pytorch-xla-on-a-gpu-instance-build-pytorch-xla-from-source-with-gpu-support">
<h2>Develop PyTorch/XLA on a GPU instance (build PyTorch/XLA from source with GPU support)<a class="headerlink" href="#develop-pytorch-xla-on-a-gpu-instance-build-pytorch-xla-from-source-with-gpu-support" title="Permalink to this heading">¶</a></h2>
<ol class="arabic simple">
<li><p>Inside a GPU VM, create a docker container from a development docker image. For example:</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>sudo docker pull us-central1-docker.pkg.dev/tpu-pytorch-releases/docker/development:3.8_cuda_12.1

# Installing the NVIDIA Container Toolkit per https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html
# For example
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
  &amp;&amp; curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
    sed &#39;s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g&#39; | \
    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit

# Configuring the NVIDIA Container Toolkit
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker

sudo docker run --shm-size=16g --net=host --gpus all -it -d us-central1-docker.pkg.dev/tpu-pytorch-releases/docker/development:3.8_cuda_12.1
sudo docker exec -it $(sudo docker ps | awk &#39;NR==2 { print $1 }&#39;) /bin/bash
</pre></div>
</div>
<ol class="arabic simple">
<li><p>Build PyTorch and PyTorch/XLA from source.</p></li>
</ol>
<p>Make sure <code class="docutils literal notranslate"><span class="pre">PATH</span></code> and <code class="docutils literal notranslate"><span class="pre">LD_LIBRARY_PATH</span></code> environment variables account for cuda. See the <a class="reference external" href="https://github.com/pytorch/xla/blob/master/docs/gpu.md#check-environment-variable">above</a> for more info.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">git</span> <span class="n">clone</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">pytorch</span><span class="o">/</span><span class="n">pytorch</span><span class="o">.</span><span class="n">git</span>
<span class="n">cd</span> <span class="n">pytorch</span>
<span class="n">USE_CUDA</span><span class="o">=</span><span class="mi">1</span> <span class="n">python</span> <span class="n">setup</span><span class="o">.</span><span class="n">py</span> <span class="n">install</span>

<span class="n">git</span> <span class="n">clone</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">pytorch</span><span class="o">/</span><span class="n">xla</span><span class="o">.</span><span class="n">git</span>
<span class="n">cd</span> <span class="n">xla</span>
<span class="n">XLA_CUDA</span><span class="o">=</span><span class="mi">1</span> <span class="n">python</span> <span class="n">setup</span><span class="o">.</span><span class="n">py</span> <span class="n">install</span>
</pre></div>
</div>
<ol class="arabic simple">
<li><p>Verify if PyTorch and PyTorch/XLA have been installed successfully.</p></li>
</ol>
<p>If you can run the tests in the section
<a class="reference external" href="#run-some-simple-models">Run some simple models</a> successfully, then PyTorch and
PyTorch/XLA should have been installed successfully.</p>
</div>
</div>
<div class="section" id="pytorch-xla-spmd-user-guide">
<h1>PyTorch/XLA SPMD User Guide<a class="headerlink" href="#pytorch-xla-spmd-user-guide" title="Permalink to this heading">¶</a></h1>
<p>In this user guide, we discuss how <a class="reference external" href="https://arxiv.org/abs/2105.04663">GSPMD</a> is integrated in PyTorch/XLA, and provide a design overview to illustrate how the SPMD sharding annotation API and its constructs work.</p>
<div class="section" id="what-is-pytorch-xla-spmd">
<h2>What is PyTorch/XLA SPMD?<a class="headerlink" href="#what-is-pytorch-xla-spmd" title="Permalink to this heading">¶</a></h2>
<p><a class="reference external" href="https://arxiv.org/abs/2105.04663">GSPMD</a> is an automatic parallelization system for common ML workloads. The XLA compiler will transform the single device program into a partitioned one with proper collectives, based on the user provided sharding hints. This feature allows developers to write PyTorch programs as if they are on a single large device without any custom sharded computation ops and/or collective communications to scale.</p>
<a class="reference external image-reference" href="assets/spmd_mode.png"><img alt="alt_text" src="assets/spmd_mode.png" /></a>
<p><a href="#id29"><span class="problematic" id="id30">*</span></a><span class="raw-html-m2r"><span style="text-decoration:underline;">Figure 1. Comparison of two different execution strategies, (a) for non-SPMD and (b) for SPMD.</span></span>*</p>
</div>
<div class="section" id="how-to-use-pytorch-xla-spmd">
<h2>How to use PyTorch/XLA SPMD?<a class="headerlink" href="#how-to-use-pytorch-xla-spmd" title="Permalink to this heading">¶</a></h2>
<p>Here is an simple example of using SPMD</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch_xla.core.xla_model</span> <span class="k">as</span> <span class="nn">xm</span>
<span class="kn">import</span> <span class="nn">torch_xla.runtime</span> <span class="k">as</span> <span class="nn">xr</span>
<span class="kn">import</span> <span class="nn">torch_xla.distributed.spmd</span> <span class="k">as</span> <span class="nn">xs</span>
<span class="kn">from</span> <span class="nn">torch_xla.distributed.spmd</span> <span class="kn">import</span> <span class="n">Mesh</span>


<span class="c1"># Enable XLA SPMD execution mode.</span>
<span class="n">xr</span><span class="o">.</span><span class="n">use_spmd</span><span class="p">()</span>


<span class="c1"># Device mesh, this and partition spec as well as the input tensor shape define the individual shard shape.</span>
<span class="n">num_devices</span> <span class="o">=</span> <span class="n">xr</span><span class="o">.</span><span class="n">global_runtime_device_count</span><span class="p">()</span>
<span class="n">mesh_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_devices</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">device_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_devices</span><span class="p">))</span>
<span class="n">mesh</span> <span class="o">=</span> <span class="n">Mesh</span><span class="p">(</span><span class="n">device_ids</span><span class="p">,</span> <span class="n">mesh_shape</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="s1">&#39;model&#39;</span><span class="p">))</span>


<span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>


<span class="c1"># Mesh partitioning, each device holds 1/8-th of the input</span>
<span class="n">partition_spec</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="s1">&#39;model&#39;</span><span class="p">)</span>
<span class="n">xs</span><span class="o">.</span><span class="n">mark_sharding</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">mesh</span><span class="p">,</span> <span class="n">partition_spec</span><span class="p">)</span>
</pre></div>
</div>
<p>Let’s explain these concepts one by one</p>
<div class="section" id="spmd-mode">
<h3>SPMD Mode<a class="headerlink" href="#spmd-mode" title="Permalink to this heading">¶</a></h3>
<p>In order to use SPMD, you need to enable it via <code class="docutils literal notranslate"><span class="pre">xr.use_spmd()</span></code>. In SPMD mode there is only one logical device. Distributed computation and collective is handled by the <code class="docutils literal notranslate"><span class="pre">mark_sharding</span></code>. Note that user can not mix SPMD with other distributed libraries.</p>
</div>
<div class="section" id="mesh">
<h3>Mesh<a class="headerlink" href="#mesh" title="Permalink to this heading">¶</a></h3>
<p>For a given cluster of devices, a physical mesh is a representation of the interconnect topology.</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">mesh_shape</span></code> is a tuple that will be multiplied to the total number of physical devices.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">device_ids</span></code> is almost always <code class="docutils literal notranslate"><span class="pre">np.array(range(num_devices))</span></code>.</p></li>
<li><p>Users are also encouraged to give each mesh dimension a name. In the above example, the first mesh dimension is the <code class="docutils literal notranslate"><span class="pre">data</span></code> dimension and the second mesh dimension is the <code class="docutils literal notranslate"><span class="pre">model</span></code> dimension.</p></li>
</ol>
<p>You can also check more mesh info via</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">mesh</span><span class="o">.</span><span class="n">shape</span><span class="p">()</span>
<span class="go">OrderedDict([(&#39;data&#39;, 4), (&#39;model&#39;, 1)])</span>
</pre></div>
</div>
</div>
<div class="section" id="partition-spec">
<h3>Partition Spec<a class="headerlink" href="#partition-spec" title="Permalink to this heading">¶</a></h3>
<p>partition_spec has the same rank as the input tensor. Each dimension describes how the corresponding input tensor dimension is sharded across the device mesh. In the above example tensor <code class="docutils literal notranslate"><span class="pre">t</span></code>’s fist dimension is being sharded at <code class="docutils literal notranslate"><span class="pre">data</span></code> dimension and the second dimension is being sharded at <code class="docutils literal notranslate"><span class="pre">model</span></code> dimension.</p>
<p>User can also shard tensor that has different dimensions from the mesh shape.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">t1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">t2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># First dimension is being replicated.</span>
<span class="n">xs</span><span class="o">.</span><span class="n">mark_sharding</span><span class="p">(</span><span class="n">t1</span><span class="p">,</span> <span class="n">mesh</span><span class="p">,</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="s1">&#39;model&#39;</span><span class="p">))</span>

<span class="c1"># First dimension is being sharded at data dimension.</span>
<span class="c1"># model dimension is used for replication when omitted.</span>
<span class="n">xs</span><span class="o">.</span><span class="n">mark_sharding</span><span class="p">(</span><span class="n">t2</span><span class="p">,</span> <span class="n">mesh</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">,))</span>

<span class="c1"># First dimension is sharded across both mesh axes.</span>
<span class="n">xs</span><span class="o">.</span><span class="n">mark_sharding</span><span class="p">(</span> <span class="n">t2</span><span class="p">,</span> <span class="n">mesh</span><span class="p">,</span> <span class="p">((</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="s1">&#39;model&#39;</span><span class="p">),))</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id31">
<h2>Further Reading<a class="headerlink" href="#id31" title="Permalink to this heading">¶</a></h2>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://github.com/pytorch/xla/blob/master/examples/data_parallel/train_resnet_spmd_data_parallel.py">Example</a> to use SPMD to express data parallism.</p></li>
<li><p><a class="reference external" href="https://github.com/pytorch/xla/blob/master/examples/fsdp/train_decoder_only_fsdp_v2.py">Example</a> to use SPMD to express FSDP(Fully Sharded Data Parallel).</p></li>
<li><p><a class="reference external" href="https://github.com/pytorch/xla/blob/master/docs/spmd_advanced.md">SPMD advanced topics</a></p></li>
<li><p><a class="reference external" href="https://github.com/pytorch/xla/blob/master/docs/spmd_distributed_checkpoint.md">Spmd Distributed Checkpoint</a></p></li>
</ol>
</div>
</div>
<div class="section" id="fully-sharded-data-parallel-via-spmd">
<h1>Fully Sharded Data Parallel via SPMD<a class="headerlink" href="#fully-sharded-data-parallel-via-spmd" title="Permalink to this heading">¶</a></h1>
<p>Fully Sharded Data Parallel via SPMD or FSDPv2 is an utility that re-expresses the famous FSDP algorithm in SPMD. <a class="reference external" href="https://github.com/pytorch/xla/blob/master/torch_xla/experimental/spmd_fully_sharded_data_parallel.py">This</a> is
an experimental feature that aiming to offer a familiar interface for users to enjoy all the benefits that SPMD brings into
the table. The design doc is <a class="reference external" href="https://github.com/pytorch/xla/issues/6379">here</a>.</p>
<p>Please review the <a class="reference external" href="./spmd.md">SPMD user guide</a> before proceeding.</p>
<p>Example usage:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch_xla.core.xla_model</span> <span class="k">as</span> <span class="nn">xm</span>
<span class="kn">import</span> <span class="nn">torch_xla.distributed.spmd</span> <span class="k">as</span> <span class="nn">xs</span>
<span class="kn">from</span> <span class="nn">torch_xla.experimental.spmd_fully_sharded_data_parallel</span> <span class="kn">import</span> <span class="n">SpmdFullyShardedDataParallel</span> <span class="k">as</span> <span class="n">FSDPv2</span>

<span class="c1"># Define the mesh following common SPMD practice</span>
<span class="n">num_devices</span> <span class="o">=</span> <span class="n">xr</span><span class="o">.</span><span class="n">global_runtime_device_count</span><span class="p">()</span>
<span class="n">mesh_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_devices</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">device_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_devices</span><span class="p">))</span>
<span class="c1"># To be noted, the mesh must have an axis named &#39;fsdp&#39;, which the weights and activations will be sharded on.</span>
<span class="n">mesh</span> <span class="o">=</span> <span class="n">xs</span><span class="o">.</span><span class="n">Mesh</span><span class="p">(</span><span class="n">device_ids</span><span class="p">,</span> <span class="n">mesh_shape</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;fsdp&#39;</span><span class="p">,</span> <span class="s1">&#39;model&#39;</span><span class="p">))</span>

<span class="c1"># Shard the input, and assume x is a 2D tensor.</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">xs</span><span class="o">.</span><span class="n">mark_sharding</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mesh</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;fsdp&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>

<span class="c1"># As normal FSDP, but an extra mesh is needed.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">FSDPv2</span><span class="p">(</span><span class="n">my_module</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>
<span class="n">optim</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">optim</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<p>It is also possible to shard individual layers separately and have an outer wrapper handle any leftover parameters. Here is an example to autowrap each <code class="docutils literal notranslate"><span class="pre">DecoderLayer</span></code>.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch_xla.distributed.fsdp.wrap</span> <span class="kn">import</span> <span class="n">transformer_auto_wrap_policy</span>

<span class="c1"># Apply FSDP sharding on each DecoderLayer layer.</span>
<span class="n">auto_wrap_policy</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span>
    <span class="n">transformer_auto_wrap_policy</span><span class="p">,</span>
    <span class="n">transformer_layer_cls</span><span class="o">=</span><span class="p">{</span>
        <span class="n">decoder_only_model</span><span class="o">.</span><span class="n">DecoderLayer</span>
    <span class="p">},</span>
<span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">FSDPv2</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span> <span class="n">mesh</span><span class="o">=</span><span class="n">mesh</span><span class="p">,</span> <span class="n">auto_wrap_policy</span><span class="o">=</span><span class="n">auto_wrap_policy</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="sharding-output">
<h2>Sharding output<a class="headerlink" href="#sharding-output" title="Permalink to this heading">¶</a></h2>
<p>To ensure the XLA compiler correctly implements the FSDP algorithm, we need to shard both weights and activations. This means sharding the output of the forward method. Since the forward function output can vary, we offer shard_output to shard activations in cases where your module output doesn’t fall into one of these categories:</p>
<ol class="arabic simple">
<li><p>A single tensor</p></li>
<li><p>A tuple of tensors where the 0th element is the activation.</p></li>
</ol>
<p>Example usage:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">shard_output</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">mesh</span><span class="p">):</span>
    <span class="n">xs</span><span class="o">.</span><span class="n">mark_sharding</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">logits</span><span class="p">,</span> <span class="n">mesh</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;fsdp&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">FSDPv2</span><span class="p">(</span><span class="n">my_module</span><span class="p">,</span> <span class="n">mesh</span><span class="p">,</span> <span class="n">shard_output</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="id35">
<h2>Gradient checkpointing<a class="headerlink" href="#id35" title="Permalink to this heading">¶</a></h2>
<p>Currently, gradient checkpointing needs to be applied to the module before the FSDP wrapper. Otherwise, recursively loop into children modules will end up with infinite loop. We will fix this issue in the future releases.</p>
<p>Example usage:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch_xla.distributed.fsdp</span> <span class="kn">import</span> <span class="n">checkpoint_module</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">FSDPv2</span><span class="p">(</span><span class="n">checkpoint_module</span><span class="p">(</span><span class="n">my_module</span><span class="p">),</span> <span class="n">mesh</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="huggingface-llama-2-example">
<h2>HuggingFace Llama 2 Example<a class="headerlink" href="#huggingface-llama-2-example" title="Permalink to this heading">¶</a></h2>
<p>We have a fork of HF Llama 2 to demonstrate a potential integration <a class="reference external" href="https://github.com/huggingface/transformers/compare/main...pytorch-tpu:transformers:llama2-spmd-fsdp">here</a>.</p>
</div>
</div>


             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright .

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">PyTorch on XLA Devices</a><ul>
<li><a class="reference internal" href="#creating-an-xla-tensor">Creating an XLA Tensor</a></li>
<li><a class="reference internal" href="#xla-tensors-are-pytorch-tensors">XLA Tensors are PyTorch Tensors</a></li>
<li><a class="reference internal" href="#running-models-on-xla-devices">Running Models on XLA Devices</a><ul>
<li><a class="reference internal" href="#running-on-a-single-xla-device">Running on a Single XLA Device</a></li>
<li><a class="reference internal" href="#running-on-multiple-xla-devices-with-multi-processing">Running on Multiple XLA Devices with Multi-processing</a></li>
<li><a class="reference internal" href="#running-on-tpu-pods">Running on TPU Pods</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id3">XLA Tensor Deep Dive</a><ul>
<li><a class="reference internal" href="#xla-tensors-are-lazy">XLA Tensors are Lazy</a></li>
<li><a class="reference internal" href="#xla-tensors-and-bfloat16">XLA Tensors and bFloat16</a></li>
<li><a class="reference internal" href="#memory-layout">Memory Layout</a></li>
<li><a class="reference internal" href="#moving-xla-tensors-to-and-from-the-cpu">Moving XLA Tensors to and from the CPU</a></li>
<li><a class="reference internal" href="#saving-and-loading-xla-tensors">Saving and Loading XLA Tensors</a></li>
</ul>
</li>
<li><a class="reference internal" href="#compilation-caching">Compilation Caching</a></li>
<li><a class="reference internal" href="#further-reading">Further Reading</a></li>
</ul>
</li>
<li><a class="reference internal" href="#pytorch-xla-api">PyTorch/XLA API</a><ul>
<li><a class="reference internal" href="#module-torch_xla">torch_xla</a><ul>
<li><a class="reference internal" href="#torch_xla.device"><code class="docutils literal notranslate"><span class="pre">device()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.devices"><code class="docutils literal notranslate"><span class="pre">devices()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.device_count"><code class="docutils literal notranslate"><span class="pre">device_count()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.sync"><code class="docutils literal notranslate"><span class="pre">sync()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.step"><code class="docutils literal notranslate"><span class="pre">step()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.manual_seed"><code class="docutils literal notranslate"><span class="pre">manual_seed()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#module-torch_xla.runtime">runtime</a><ul>
<li><a class="reference internal" href="#torch_xla.runtime.device_type"><code class="docutils literal notranslate"><span class="pre">device_type()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.runtime.local_process_count"><code class="docutils literal notranslate"><span class="pre">local_process_count()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.runtime.local_device_count"><code class="docutils literal notranslate"><span class="pre">local_device_count()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.runtime.addressable_device_count"><code class="docutils literal notranslate"><span class="pre">addressable_device_count()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.runtime.global_device_count"><code class="docutils literal notranslate"><span class="pre">global_device_count()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.runtime.global_runtime_device_count"><code class="docutils literal notranslate"><span class="pre">global_runtime_device_count()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.runtime.world_size"><code class="docutils literal notranslate"><span class="pre">world_size()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.runtime.global_ordinal"><code class="docutils literal notranslate"><span class="pre">global_ordinal()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.runtime.local_ordinal"><code class="docutils literal notranslate"><span class="pre">local_ordinal()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.runtime.get_master_ip"><code class="docutils literal notranslate"><span class="pre">get_master_ip()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.runtime.use_spmd"><code class="docutils literal notranslate"><span class="pre">use_spmd()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.runtime.is_spmd"><code class="docutils literal notranslate"><span class="pre">is_spmd()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.runtime.initialize_cache"><code class="docutils literal notranslate"><span class="pre">initialize_cache()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#module-torch_xla.core.xla_model">xla_model</a><ul>
<li><a class="reference internal" href="#torch_xla.core.xla_model.xla_device"><code class="docutils literal notranslate"><span class="pre">xla_device()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.core.xla_model.xla_device_hw"><code class="docutils literal notranslate"><span class="pre">xla_device_hw()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.core.xla_model.is_master_ordinal"><code class="docutils literal notranslate"><span class="pre">is_master_ordinal()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.core.xla_model.all_reduce"><code class="docutils literal notranslate"><span class="pre">all_reduce()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.core.xla_model.all_gather"><code class="docutils literal notranslate"><span class="pre">all_gather()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.core.xla_model.all_to_all"><code class="docutils literal notranslate"><span class="pre">all_to_all()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.core.xla_model.add_step_closure"><code class="docutils literal notranslate"><span class="pre">add_step_closure()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.core.xla_model.wait_device_ops"><code class="docutils literal notranslate"><span class="pre">wait_device_ops()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.core.xla_model.optimizer_step"><code class="docutils literal notranslate"><span class="pre">optimizer_step()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.core.xla_model.save"><code class="docutils literal notranslate"><span class="pre">save()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.core.xla_model.rendezvous"><code class="docutils literal notranslate"><span class="pre">rendezvous()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.core.xla_model.mesh_reduce"><code class="docutils literal notranslate"><span class="pre">mesh_reduce()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.core.xla_model.set_rng_state"><code class="docutils literal notranslate"><span class="pre">set_rng_state()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.core.xla_model.get_rng_state"><code class="docutils literal notranslate"><span class="pre">get_rng_state()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.core.xla_model.get_memory_info"><code class="docutils literal notranslate"><span class="pre">get_memory_info()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.core.xla_model.get_stablehlo"><code class="docutils literal notranslate"><span class="pre">get_stablehlo()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.core.xla_model.get_stablehlo_bytecode"><code class="docutils literal notranslate"><span class="pre">get_stablehlo_bytecode()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#module-torch_xla.distributed.parallel_loader">distributed</a><ul>
<li><a class="reference internal" href="#torch_xla.distributed.parallel_loader.ParallelLoader"><code class="docutils literal notranslate"><span class="pre">ParallelLoader</span></code></a><ul>
<li><a class="reference internal" href="#torch_xla.distributed.parallel_loader.ParallelLoader.per_device_loader"><code class="docutils literal notranslate"><span class="pre">ParallelLoader.per_device_loader()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torch_xla.distributed.xla_multiprocessing.spawn"><code class="docutils literal notranslate"><span class="pre">spawn()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#module-torch_xla.distributed.spmd">spmd</a><ul>
<li><a class="reference internal" href="#torch_xla.distributed.spmd.mark_sharding"><code class="docutils literal notranslate"><span class="pre">mark_sharding()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.distributed.spmd.clear_sharding"><code class="docutils literal notranslate"><span class="pre">clear_sharding()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.distributed.spmd.set_global_mesh"><code class="docutils literal notranslate"><span class="pre">set_global_mesh()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.distributed.spmd.get_global_mesh"><code class="docutils literal notranslate"><span class="pre">get_global_mesh()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.distributed.spmd.Mesh"><code class="docutils literal notranslate"><span class="pre">Mesh</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.distributed.spmd.HybridMesh"><code class="docutils literal notranslate"><span class="pre">HybridMesh</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.distributed.spmd.ShardingSpec"><code class="docutils literal notranslate"><span class="pre">ShardingSpec</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#module-torch_xla.experimental">experimental</a><ul>
<li><a class="reference internal" href="#torch_xla.experimental.eager_mode"><code class="docutils literal notranslate"><span class="pre">eager_mode()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.experimental.compile"><code class="docutils literal notranslate"><span class="pre">compile()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#module-torch_xla.debug.metrics">debug</a><ul>
<li><a class="reference internal" href="#torch_xla.debug.metrics.metrics_report"><code class="docutils literal notranslate"><span class="pre">metrics_report()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.debug.metrics.short_metrics_report"><code class="docutils literal notranslate"><span class="pre">short_metrics_report()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.debug.metrics.counter_names"><code class="docutils literal notranslate"><span class="pre">counter_names()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.debug.metrics.counter_value"><code class="docutils literal notranslate"><span class="pre">counter_value()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.debug.metrics.metric_names"><code class="docutils literal notranslate"><span class="pre">metric_names()</span></code></a></li>
<li><a class="reference internal" href="#torch_xla.debug.metrics.metric_data"><code class="docutils literal notranslate"><span class="pre">metric_data()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#beginner-s-guide-to-pytorch-xla">Beginner’s Guide to PyTorch/XLA</a><ul>
<li><a class="reference internal" href="#basic-high-level-understanding-of-some-xla-details">Basic high-level understanding of some XLA details</a></li>
<li><a class="reference internal" href="#tpu-setup">TPU Setup</a></li>
<li><a class="reference internal" href="#converting-code-to-pytorch-xla">Converting code to PyTorch XLA</a></li>
<li><a class="reference internal" href="#example-1-stable-diffusion-inference-in-pytorch-lightning-on-a-single-tpu-device">Example 1. Stable Diffusion inference in PyTorch Lightning on a Single TPU Device</a></li>
<li><a class="reference internal" href="#example-2-hf-stable-diffusion-inference">Example 2. HF Stable Diffusion Inference</a></li>
<li><a class="reference internal" href="#running-on-a-single-tpu-device">Running on a Single TPU device</a></li>
<li><a class="reference internal" href="#profiling-and-performance-analysis">Profiling and performance analysis</a></li>
<li><a class="reference internal" href="#running-on-multiple-tpu-devices">Running on Multiple TPU Devices</a></li>
<li><a class="reference internal" href="#running-on-pods">Running on Pods</a></li>
</ul>
</li>
<li><a class="reference internal" href="#troubleshooting">Troubleshooting</a><ul>
<li><a class="reference internal" href="#sanity-check">Sanity Check</a><ul>
<li><a class="reference internal" href="#check-pytorch-xla-version">Check PyTorch/XLA Version</a></li>
<li><a class="reference internal" href="#perform-a-simple-calculation">Perform A Simple Calculation</a></li>
<li><a class="reference internal" href="#run-resnet-with-fake-data">Run Resnet With Fake Data</a></li>
</ul>
</li>
<li><a class="reference internal" href="#performance-debugging">Performance Debugging</a></li>
<li><a class="reference internal" href="#pytorch-xla-debugging-tool">PyTorch/XLA Debugging Tool</a><ul>
<li><a class="reference internal" href="#perform-a-auto-metrics-analysis">Perform A Auto-Metrics Analysis</a></li>
<li><a class="reference internal" href="#compilation-execution-analysis">Compilation &amp; Execution Analysis</a></li>
</ul>
</li>
<li><a class="reference internal" href="#get-a-metrics-report">Get A Metrics Report</a></li>
<li><a class="reference internal" href="#understand-the-metrics-report">Understand The Metrics Report</a></li>
<li><a class="reference internal" href="#clear-the-metrics-report">Clear The Metrics Report</a></li>
<li><a class="reference internal" href="#pytorch-xla-dynamo-debugging-tool">PyTorch/XLA + Dynamo Debugging Tool</a></li>
<li><a class="reference internal" href="#performance-profiling">Performance Profiling</a></li>
<li><a class="reference internal" href="#simple-benchmarking">Simple Benchmarking</a></li>
<li><a class="reference internal" href="#known-performance-caveats">Known Performance Caveats</a></li>
<li><a class="reference internal" href="#xla-tensor-quirks">XLA Tensor Quirks</a></li>
<li><a class="reference internal" href="#more-debugging-tools">More Debugging Tools</a><ul>
<li><a class="reference internal" href="#environment-variables">Environment Variables</a></li>
<li><a class="reference internal" href="#common-debugging-environment-variables-combinations">Common Debugging Environment Variables Combinations</a></li>
<li><a class="reference internal" href="#reproducing-pytorch-xla-ci-cd-unit-test-failures">Reproducing PyTorch/XLA CI/CD unit test failures.</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#pjrt-runtime">PJRT Runtime</a><ul>
<li><a class="reference internal" href="#tl-dr">TL;DR</a></li>
<li><a class="reference internal" href="#benefits">Benefits</a></li>
<li><a class="reference internal" href="#quickstart">Quickstart</a><ul>
<li><a class="reference internal" href="#cpu">CPU</a></li>
<li><a class="reference internal" href="#tpu">TPU</a><ul>
<li><a class="reference internal" href="#pods">Pods</a></li>
<li><a class="reference internal" href="#docker">Docker</a></li>
</ul>
</li>
<li><a class="reference internal" href="#gpu">GPU</a></li>
<li><a class="reference internal" href="#single-node-gpu-training">Single-node GPU training</a></li>
<li><a class="reference internal" href="#multi-node-gpu-training">Multi-node GPU training</a></li>
</ul>
</li>
<li><a class="reference internal" href="#differences-from-xrt">Differences from XRT</a><ul>
<li><a class="reference internal" href="#id20">Multithreading on TPU v2/v3</a></li>
<li><a class="reference internal" href="#changes-to-xm-rendezvous">Changes to xm.rendezvous</a></li>
<li><a class="reference internal" href="#pjrt-and-torch-distributed">PJRT and torch.distributed</a></li>
</ul>
</li>
<li><a class="reference internal" href="#performance">Performance</a><ul>
<li><a class="reference internal" href="#new-tpu-runtime">New TPU runtime</a></li>
</ul>
</li>
<li><a class="reference internal" href="#torchdynamo-torch-compile-integration-in-pytorch-xla">TorchDynamo(torch.compile) integration in PyTorch XLA</a><ul>
<li><a class="reference internal" href="#integration">Integration</a></li>
<li><a class="reference internal" href="#inference">Inference</a></li>
<li><a class="reference internal" href="#training">Training</a></li>
<li><a class="reference internal" href="#feature-gaps">Feature gaps</a></li>
<li><a class="reference internal" href="#take-away">Take away</a></li>
</ul>
</li>
<li><a class="reference internal" href="#fully-sharded-data-parallel-fsdp-in-pytorch-xla">Fully Sharded Data Parallel (FSDP) in PyTorch XLA</a><ul>
<li><a class="reference internal" href="#example-training-scripts-on-mnist-and-imagenet">Example training scripts on MNIST and ImageNet</a><ul>
<li><a class="reference internal" href="#installation">Installation</a></li>
<li><a class="reference internal" href="#clone-pytorch-xla-repo">Clone PyTorch/XLA repo</a></li>
<li><a class="reference internal" href="#train-mnist-on-v3-8-tpu">Train MNIST on v3-8 TPU</a></li>
<li><a class="reference internal" href="#train-imagenet-with-resnet-50-on-v3-8-tpu">Train ImageNet with ResNet-50 on v3-8 TPU</a></li>
</ul>
</li>
<li><a class="reference internal" href="#example-training-scripts-on-tpu-pod-with-10-billion-parameters">Example training scripts on TPU pod (with 10 billion parameters)</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#how-to-do-distributeddataparallel">How to do <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code></a><ul>
<li><a class="reference internal" href="#background-motivation">Background / Motivation</a></li>
<li><a class="reference internal" href="#how-to-use-distributeddataparallel">How to use DistributedDataParallel</a></li>
<li><a class="reference internal" href="#benchmarking">Benchmarking</a><ul>
<li><a class="reference internal" href="#resnet50-with-fake-data">Resnet50 with fake data</a></li>
<li><a class="reference internal" href="#mnist-with-fake-data">MNIST with fake data</a></li>
<li><a class="reference internal" href="#mnist-with-real-data">MNIST with real data</a></li>
</ul>
</li>
<li><a class="reference internal" href="#disclaimer">Disclaimer</a></li>
</ul>
</li>
<li><a class="reference internal" href="#how-to-run-with-pytorch-xla-gpu">How to run with PyTorch/XLA:GPU</a><ul>
<li><a class="reference internal" href="#create-a-gpu-instance">Create a GPU instance</a></li>
<li><a class="reference internal" href="#environment-setup">Environment Setup</a><ul>
<li><a class="reference internal" href="#id23">Docker</a></li>
<li><a class="reference internal" href="#check-environment-variable">Check environment variable</a></li>
<li><a class="reference internal" href="#wheel">Wheel</a></li>
</ul>
</li>
<li><a class="reference internal" href="#run-some-simple-models">Run some simple models</a><ul>
<li><a class="reference internal" href="#mp-imagenet-example">MP_ImageNet Example</a></li>
<li><a class="reference internal" href="#resnet-example">ResNet Example</a></li>
</ul>
</li>
<li><a class="reference internal" href="#amp-automatic-mixed-precision">AMP (AUTOMATIC MIXED PRECISION)</a></li>
<li><a class="reference internal" href="#develop-pytorch-xla-on-a-gpu-instance-build-pytorch-xla-from-source-with-gpu-support">Develop PyTorch/XLA on a GPU instance (build PyTorch/XLA from source with GPU support)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#pytorch-xla-spmd-user-guide">PyTorch/XLA SPMD User Guide</a><ul>
<li><a class="reference internal" href="#what-is-pytorch-xla-spmd">What is PyTorch/XLA SPMD?</a></li>
<li><a class="reference internal" href="#how-to-use-pytorch-xla-spmd">How to use PyTorch/XLA SPMD?</a><ul>
<li><a class="reference internal" href="#spmd-mode">SPMD Mode</a></li>
<li><a class="reference internal" href="#mesh">Mesh</a></li>
<li><a class="reference internal" href="#partition-spec">Partition Spec</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id31">Further Reading</a></li>
</ul>
</li>
<li><a class="reference internal" href="#fully-sharded-data-parallel-via-spmd">Fully Sharded Data Parallel via SPMD</a><ul>
<li><a class="reference internal" href="#sharding-output">Sharding output</a></li>
<li><a class="reference internal" href="#id35">Gradient checkpointing</a></li>
<li><a class="reference internal" href="#huggingface-llama-2-example">HuggingFace Llama 2 Example</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
         <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
         <script src="_static/jquery.js"></script>
         <script src="_static/underscore.js"></script>
         <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="_static/doctools.js"></script>
         <script src="_static/sphinx_highlight.js"></script>
     

  

  <script type="text/javascript" src="_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>Learn</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">Get Started</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">Tutorials</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>Ecosystem</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">Tools</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">Community</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">Forums</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">Developer Resources</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">Contributor Awards - 2023</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">About PyTorch Edge</a>
             </li>
             
             <li>
               <a href="https://pytorch.org/executorch-overview">ExecuTorch</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Docs</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch Domains</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>Blog & News</a>
          </li>
            
           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch Blog</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">Community Blog</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">Videos</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>
          </ul>
          
          <li class="resources-mobile-menu-title">
            <a>About</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">Governing Board</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>